[["index.html", "Ciencia de Datos - UMA Capítulo 1 Introducción", " Ciencia de Datos - UMA Domingo López, Ángel Mora 2022-05-15 Capítulo 1 Introducción En este libro queremos recopilar material que se ha estado utilizando en las asignaturas de: Laboratorio de Computación Científica - E.T.S.I. Informática - Universidad de Málaga Introducción a Ciencia de Datos I - Máster Universitario en Ingeniería Informática - Universidad de Málaga Docentes: Domingo López-Rodíguez - dominlopez@uma.es https://dominlopez.netlify.com Ángel Mora Bonilla - amora@uma.es - https://amorabonilla.github.io Contenido: Resúmenes de material Ejercicios Proyectos "],["análisis-exploratorio-de-datos.html", "Capítulo 2 Análisis Exploratorio de Datos 2.1 Importación de datos 2.2 dplyr para Manipulación de Datos 2.3 Ordenar filas con arrange() 2.4 Añadir o renombrar variables con mutate() 2.5 Proyecto Starwarks", " Capítulo 2 Análisis Exploratorio de Datos https://dplyr.tidyverse.org/ https://dplyr.tidyverse.org/articles/dplyr.html https://r4ds.had.co.nz/transform.html Descargar a local desde directorio Datasets de CV, un dataset descargado del repositorio UCI: breast-cancer.data, y breast-cancer.names1.csv (nombres de las columnas) Usar el paquete dplyr, otro de los paquetes centrales del denominado tidyverse. En tidyverse - dplyr Instalar tidyverse Cargar paquete tidyverse 2.1 Importación de datos Importa en R el fichero breast-cancer.data. Colocar los nombres al dataset que están en breast-cancer.names1.csv. 2.2 dplyr para Manipulación de Datos dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges: 5 funciones clave de dplyr: Escoger observaciones (filas) según sus valores (filter()) - filtrar, consultar las filas según condiciones, etc. Reordenar las filas (arrange()). Seleccionar variables por su nombre (select()). Añadir nuevas variables como función de variables ya existentes (mutate()). Encontrar valores representativos de cada variable (summarise()). Estas se combinan con: group_by() which allows you to perform any operation “by group”. Todos estos verbos funcionan de la misma manera: El primer argumento es un dataframe. Los demás argumentos describen qué hacer con el dataframe, usando los nombres de las variables (columnas) sin necesidad de utilizar comillas. El resultado es un nuevo dataframe. 2.2.1 Backends In addition to data frames/tibbles, dplyr makes working with other computational backends accessible and efficient. Below is a list of alternative backends: dtplyr: for large, in-memory datasets. Translates your dplyr code to high performance data.table code. dbplyr: for data stored in a relational database. Translates your dplyr code to SQL. sparklyr: for very large datasets stored in Apache Spark. 2.2.2 Filtrado de filas con filter() filter() extrae un subconjunto de las observaciones (filas), basándose en los valores de una o más columnas. Argumentos: - nombre del dataframe - expresiones (lógicas) para filtrar el dataframe extraer las filas del dataset para los pacientes que el tumor ha sido recurrente extraer las filas del dataset que tienen cancer en el pecho izquierdo, no radiadas y de edad 40 a 49 extraer las filas del dataset con edad menor de 35 años y que tienen cancer en el pecho izquierdo 2.3 Ordenar filas con arrange() Para ordenar con arrange() indicar columnas por las que ordenar ascendentemente, en caso de querer descendente: desc(variable). Ejemplo: num_casos distintos 286 0 deg_malig n media lasuma 1 71 1 71 2 130 2 260 3 85 3 255 reordenar el dataset según el tamaño del tumor reordenar el dataset según el tamaño del tumor y el grado de que tiene el tumor reordenar el dataset según el tamaño del tumor (pero descendiente) y el grado de maligno que tiene el tumor - ayuda - desc(columna) extraer los 10 pacientes con pre-menopausia, con mayor tamaño de tumor y menor número de nodos invasores y ordenados por edad. 2.3.1 Selección de columnas select() seleccionar únicamente aquellas variables en las que estamos interesados - ayuda - select(dataset, col1, col2, col3) Seleccionar del dataset las columnas clase, tamaño de tumor y grado de tumor Podemos seleccionar intervalos de columnas (de col1 a col4): ‘deseleccionar’ columnas - NO SELECCIONAR COLUMNAS Otros argumentos interesantes select(): everything(), es mover algunas columnas o variables al principio del dataframe. starts_with('abc'): encuentra todas las columnas cuyo nombre comienza por “abc”. ends_with('xyz'): encuentra todas las columnas cuyo nombre termina en “xyz”. contains('ijk'): para seleccionar las columnas cuyo nombre contenga la cadena de caracteres “ijk”. Y otras funciones más complejas (que filtran las columnas por expresiones regulares), que se pueden ver al hacer ?select. 2.4 Añadir o renombrar variables con mutate() El verbo mutate() se usa para añadir nuevas columnas al final del dataframe. añadimos columna dist_grado_peor que es 4 menos el grado de malignidad. añadir una columna media_tumor que tenga el valor medio de los valores almacenados en variable tumor_size 2.4.1 rename() función rename(), que, internamente, se comporta como select(), pero guardando todas las variables que no se mencionan explícitamente: 2.4.2 Uso del operador %&gt;% %&gt;% utiliza la salida del término que hay a la izquierda del símbolo %&gt;% como primer argumento de la función que está a la derecha de dicho símbolo. x %&gt;% f(y) es igual que hacer f(x, y) 2.4.3 Otros verbos auxiliares 2.4.3.1 summarize() num_casos distintos 286 0 n() contar n_distinct() valores únicos mean(), min(), max() first(), last(), nth() 2.4.3.2 group_by() summarize se suele usar con group by deg_malig n media 1 71 1 2 130 2 3 85 3 2.4.3.3 slice() Usando slice() extraer los 10 pacientes con pre-menopausia, con mayor tamaño de tumor y menor número de nodos invasores y ordenados por edad. Recurrencia age menopause tumor_size inv_nodes node_caps 1 no-recurrence-events 30-39 premeno 30-34 0-2 no 2 no-recurrence-events 40-49 premeno 20-24 0-2 no 3 no-recurrence-events 40-49 premeno 20-24 0-2 no 4 no-recurrence-events 60-69 ge40 15-19 0-2 no 5 no-recurrence-events 40-49 premeno 0-4 0-2 no 6 no-recurrence-events 60-69 ge40 15-19 0-2 no 7 no-recurrence-events 50-59 premeno 25-29 0-2 no 8 no-recurrence-events 60-69 ge40 20-24 0-2 no 9 no-recurrence-events 40-49 premeno 50-54 0-2 no 10 no-recurrence-events 40-49 premeno 20-24 0-2 no deg_malig breast breast_quad irradiat 1 3 left left_low no 2 2 right right_up no 3 2 left left_low no 4 2 right left_up no 5 2 right right_low no 6 2 left left_low no 7 2 left left_low no 8 1 left left_low no 9 2 left left_low no 10 2 right left_up no 2.4.3.4 transmute() Si sólo se desea guardar las nuevas variables, se puede usar la función transmute(): 2.4.4 Operaciones con dos tablas https://dplyr.tidyverse.org/articles/two-table.html 2.5 Proyecto Starwarks Usando el dataset starwarks que está en paquete dplyr: Obtener los 10 humanos más viejos, masculinos, con planeta natal Tatooine. Encontrar a aquellos personajes de ojos azules y rubios/as de especie humana, procedentes de Tatooine, ordenados por edad de menor a mayor. Calcular su altura media. Encontrar aquellos personajes de especie Human o Naboo y calcular una variable con los valores pesado o ligero (si su massa es mayor que 79 es pesado). Mostrar las variables name, height mass y heavyorlight y ordenar por mass de mayor a menor. Calcular el indice de masa corporal de todos los personajes (eliminando los que tienen masa o altura NA). A continuación mostrar el nombre, altura, masa y IMC de cada personaje, con orden de IMC descendente. Obtener los personajes cuya única nave fuese un X-wing y ordenarlos de forma descendente según su masa Obtener los personajes de masa superior a la media de las masas, obviando valores nulos, y ordenarlos de forma decreciente. Obtener las alturas medias de los personajes con el campo “gender” igual a “female”, “male” y “hermaphrodite”, ignorando NA. Filtrar por las especies que sean “Droid”, ordenados por altura descendiente y masa. Reemplazar las masas y alturas con valor NA por 1 y mostrar solo la media de todas esas masas y la mediana de esas alturas. Sacar aquellas filas con las cadenas “Jedi” ó “Clones” en la columna films. Agrupar por homeworld y sacar la media de la columnas height y mass (por separado). Reemplazar los valores NA en la columna mass por 0. Filtrar los datos de aquellos personajes que hayan aparecido solo en la película “Return of the Jedi”y que tengan un mundo natal, ordenados por peso. Para ello transforma los valores NA en 0. Seleccionar los humanos que midan más de 170 cm y que hayan salido en Attack of the Clones, agrupandolos por homeworld obviando los NA y hallar la media de sus pesos sustituyendo los NA por la mediana y mostrarlos en orden descendiente. Encontrar para cada homeworld cuantas especies diferentes lo habitan y ordenalos de mayor a menor variedad. Controlar que no se tiene en cuenta NA como especie Filtrar a los personajes mayores de 25, y luego ordenarlos por el número de películas en el que aparecen (dato que no viene directamente y tenemos que obtenerlo). Encontrar cuantas especies diferentes habitan cada homeworld y ordenarlos de mayor a menor variedad, controlando que NA no es una especie. De todos los personajes de Star Wars filtrar por los que tengan mass mayor o igual a 70, agruparlos por species y gender y calcular la media de height de estos (eliminando los valores NA previamente). Mostrar el resultado ordenado de mayor a menor altura Filtrar por aquellos personajes que tienen los ojos azules y un homeworld y birth_year asignados (diferentes de NA) Añadir una columna ficticia en la que se indica la edad que tendrían si no hubiesen muerto y actualmente estemos en 2019, es decir, restar a 2019 su año de nacimiento Agrupar según el país dónde viven Obtener como resultado la media de los valores height, mass y la columna edad previamente calculada Ordenar por la columna mass de forma descendentemente Sustituir los valores NA del peso y la altura por la media de todos los pesos y alturas respectivamente. Filtrar los humanos cuyo peso sea mayor o igual a 70, agrupados por homeworld, calcular la mediana de la altura y el número de humanos de cada homeworld. Ordenar por número de humanos que hay en cada homeworld. Obtener todos los humanos, quitando los que su altura es NA Añadir una columna con la diferencia entre su altura y la altura media de los humanos Agrupémoslos por su homeworld y la columna nueva pasa a ser la media Obten las 3 homeworld que están más por debajo de la media "],["algebra-lineal.html", "Capítulo 3 Algebra Lineal 3.1 grandes sistemas 3.2 matrices dispersas", " Capítulo 3 Algebra Lineal ## sistemas de ecuaciones lineales 3.1 grandes sistemas 3.2 matrices dispersas "],["test-estadísticos-bayesiano.html", "Capítulo 4 Test estadísticos bayesiano", " Capítulo 4 Test estadísticos bayesiano "],["modelling-techniques.html", "Capítulo 5 Modelling techniques 5.1 Interpolation 5.2 Linear Regression 5.3 Generalized Regression 5.4 Improving the models 5.5 Model validation 5.6 Proyecto - regresion EconomistData 5.7 Proyecto - Analysing Boston dataset 5.8 Poyecto - evaluación de cursos - variables categóricas 5.9 Anexo: R avanzado 5.10 Proyecto - Predicción de riesgos", " Capítulo 5 Modelling techniques There are two types of basic numerical techniques: interpolation and approximation (adjustment). The goal is to get a function that fits the best of your ability to a set of data (a cloud of data) from observations, data collected from sensors, datasets, etc. The simplest fitting model is the linear model (LM) named in Machine Learning community as linear regression. Although regression is probably the most simple approach for supervised learning, linear regression is still a useful and widely used statistical learning method. Other more complex statistical learning approaches are generalizations of linear regression. 5.1 Interpolation The normal input is a data table \\[\\{(x_i,y_i)\\}\\] And the goal is to find an interpolating function \\[ \\phi(x)=a_0+a_{1}f_{1}(x)\\] (the easiest approach considers polynomial functions \\(f_1(x)=x\\)). Then, the model will be: \\[ \\phi(x)=a_0+a_{1}x\\] Existence and uniqueness of the solution Simply, if we consider that \\(\\phi(x)\\) verifies the conditions \\((x_i,y_i)\\), the following linear system is buit: \\[ \\left. \\begin{array}{ll} a_0+a_{1}x_{1}&amp;=y_{1}\\\\ a_0+a_{1}x_{2}&amp;=y_{2}\\\\ \\ldots&amp;\\ldots\\\\ a_0+a_{1}x_{m}&amp;=y_{m}\\\\ \\end{array}\\right\\}\\] Solving the system above, the coefficients \\(a_0\\) and \\(a_{1}\\) are obtained (if they exist) and we will have the Interpolation function: \\[ \\phi (x)=a_0+a_{1}x\\] \\(a_0\\) is named the intercept. \\(a_1\\) is named the slope. The condition for existence of the interpolation function \\(\\phi(x)\\) is that the system should be consistent, its uniqueness depends on the solution(s) of the system. 5.1.1 Polynomial interpolation Assuming we have \\(n+1\\) points \\[\\{ (x_{0}, y_{0}), (x_{1}, y_{1}), (x_{2}, y_{2}), \\ldots, (x_{n}, y_{n})\\} \\] polynomial interpolation consists of finding a polynomial function \\(\\phi(x)\\) passing through the given points. \\[ \\phi (x)=a_0+a_{1}x+a_{2}x^2+\\ldots+a_{n}x^n\\] Note that we are using that a basis of the polynomials of grade less than or equal to \\(n\\) is \\(B=\\{1, x,x^2,\\ldots,x^n\\}\\). The equations will be: \\[ \\left. \\begin{array}{lr} a_0+a_{1}x_{0}+a_{2}x_{0}^2+\\ldots&amp;=y_{0}\\\\ a_0+a_{1}x_{1}+a_{2}x_{1}^2+\\ldots&amp;=y_{1}\\\\ \\ldots&amp;\\ldots\\\\ a_0+a_{1}x_{n}+a_{2}x_{n}^2+\\ldots&amp;=y_{n}\\\\ \\\\ \\end{array}\\right\\}\\] 5.2 Linear Regression Minimizing the residual sum of squares With a large number of observations, interpolation methods are not adequate, hence an adjusting linear regression model \\(y=\\phi(x)\\) is used. Such a model reflects the effect of changing a variable \\(x\\) (independent variable) in variable \\(y\\) (dependent variable). It considers that there exist a linear relationship between the variables. A common way to estimate the parameters of a statistical model is to adjust a function which minimize the errors. The most used method is that of minimizing the residual sum of squares, RSS, which is defined by: \\[ RSS(a)={\\mid\\mid \\epsilon \\mid\\mid_2}^2= \\displaystyle\\sum_{i=1}^{N}{\\epsilon_i}^2=\\displaystyle\\sum_{i=1}^{N}(y_i-a^TX)^2 \\] We compute the adjusting function which minimizes the RSS. 5.2.1 Linear Regression in R The easier model is linear regression: \\[ \\phi(x) = a_0 + a_1 x\\] Goal: Prediction of future observations. Find relationships, functions between dataset variables. Description of the data structure. In mathematical terms, regression is to find a linear function that approaches the data cloud. 5.2.2 lm() function Linear model: y is the dependent variable, the output. x is the independent variable, the predictor dataset a dataset with the attributes x and y Call: lm(formula = CPI ~ HDI) Coefficients: (Intercept) HDI -1.540 8.497 The linear model is \\[\\phi(x)=-1.540 + 8.497x\\] Which is the meaning of the coefficient \\(8.497\\)? - If \\(x\\) is increased in 1, then \\(Y\\) is increased in 1 Which is the meaning of the coefficient -1.540? - Value expected of \\(y\\), if \\(x\\) has the value 0 5.2.3 Evaluate the model Does the data fit the model found? Trying a more complicated model?: Polynomial regression. If the point cloud is \\(n\\) in size, interpolation can be achieved with a polynomial grade of \\(n-1\\). Very high grade polynomials introduce errors: oscillations, cost of computation, etc. For each model found, calculate in R the errors made, the accuracy, etc. Call: lm(formula = CPI ~ HDI) Residuals: Min 1Q Median 3Q Max -2.9180 -1.1872 -0.2029 1.0744 3.4453 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.5400 0.4453 -3.458 0.000686 HDI 8.4975 0.6539 12.994 &lt; 2e-16 — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 1.506 on 171 degrees of freedom Multiple R-squared: 0.4968, Adjusted R-squared: 0.4939 F-statistic: 168.9 on 1 and 171 DF, p-value: &lt; 2.2e-16 5.2.4 Accuracy of the model There is some measures of the accuracy of the model. 5.2.4.1 RSE We define the Residual Standard Error as follows: \\[RSE=\\sqrt{\\frac{1}{n-2} \\,\\, RSS}\\] RSE is an estimate of the standard deviation of \\(\\epsilon_i\\). RSE is considered a measure of the lack of fit of the model to the data. If the predictions using the model are very good, we can conclude that the model fits the data very well. On the other hand, if the model doesn’t fit the data well then the RSE may be quite large. 5.2.4.2 R-squared R-squared (called the coefficient of determination) or fraction of variance explained is \\[R^2=1-\\frac{RSS}{TSS}\\] where \\[TSS=\\sum_{i=1}^n (y_i-\\overline{y})\\] RSE is an absolute measure of lack of fit of the model. \\(R^2\\) takes the form of a proportion measure (the proportion of variance explained), that is, the proportion of the variance in the outcome variable that can be accounted for by the predictor. \\(R^2\\) can be estimated with the correlation \\(r\\) between the input (\\(X\\) variable) and the output (\\(Y\\) variable) as follows: \\(R^2=r^2\\). If \\(R^2\\) is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the linear model is wrong, or the inherent error is high, or both. Note: The Pearson correlation is equivalent to running a linear regression model that uses only one predictor variable - the squared correlation \\(r^2\\) is identical to the \\(R^2\\) value for a linear regression with only a single predictor. 5.2.4.3 R-squared and Adjusted R-squared R language returns Multiple R-squared and Adjusted R-squared. If you add more predictors into the model, the \\(R^2\\) value will increase (or at least it will be the same). In a regression model with \\(K\\) predictors, fit to a data set containing \\(N\\) observations, the adjusted \\(R^2\\) is: \\[ \\mbox{adj. } R^2 = 1 - \\left(\\frac{\\mbox{SS}_{res}}{\\mbox{SS}_{tot}} \\times \\frac{N-1}{N-K-1} \\right) \\] The adjusted \\(R^2\\) value will only increase when the new variables improve the model performance. 5.3 Generalized Regression The linear model \\(\\phi(x) = a_0 + a_1 x\\) could be generalized in several ways. We can also apply linear regression to more than 1 variable \\(x=\\{x_1, x_2\\}\\). For example, consider modeling temperature as a function of location \\(\\{x_1,x_2\\}\\). \\[ \\phi(x) = a_0 + a_1 x_1 + a_2x_2\\] or even, \\[ \\phi_2(x) =a_0 + a_1x_1 + a_2x_2 + a_3x_1^2\\] Note: it is possible to model non-linear relationships. In general, a simple approach (we could use non-polynomial functions) is considered using polynomial basis functions, where the model has the form \\[\\phi(x)=a_0+a_1x+a_2x^2+ \\ldots+ a_nx^n\\] 5.3.1 Multiple Linear Regression Given a set of variables \\(X_1, X_2, \\dots, X_m\\), multiple linear regression computes the next model: \\[\\phi(x)=a_o+a_1X_1+a_2X_2+\\dots, a_mX_m+\\epsilon\\] In this model a coefficient \\(a_j\\) is interpreted as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed. The ideal scenario is when the predictors are uncorrelated and in this case each coefficient can be estimated and tested separately. Correlations amongst predictors cause problems because the variance of all coefficients tends to increase, sometimes dramatically. Normally the independence is a utopia. Normally, predictors usually change together. Call: lm(formula = HDI.Rank ~ CPI + HDI) Coefficients: (Intercept) CPI HDI 293.991 -2.935 -283.876 (Intercept) CPI HDI 293.991367 -2.935028 -283.876464 1 2 3 4 5 6 -4.605992 -5.108072 8.665987 -2.157349 -13.936740 2.895255 5.3.1.1 About plot(model) The first plot (residuals vs. fitted values) is a simple scatterplot between residuals and predicted values. R shows some outliers. The second plot (normal Q-Q) is a normal probability plot. It will give a straight line if the errors are distributed normally. Outliers deviate from the straight line. The third plot (Scale-Location), like the the first, should look random. No patterns. We have a little V-shaped pattern. The last plot (Cook’s distance) tells us which points have the greatest influence on the regression (leverage points). We see that points 20,36, 116 have great influence on the model. Detection of outliers: Remove these points and repeat. 5.3.2 Evaluating the Regression Coefficients \\[\\phi(x)=a_o+a_1X_1+a_2X_2+\\dots, a_pX_p\\] 5.3.2.1 F-statistic We establish the null hypothesis: \\[H_O: a_o = a_1 = a_2= \\dots= a_p= 0\\] versus the alternative \\[H_a: \\mbox{ at least one } a_i \\mbox{ is non-zero}\\] The hyphotesis test is performed by means of F-statistic: \\[F=\\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\\] where \\[TSS = \\sum (y_i − \\overline{y} )^2\\] When there is no relationship between the real valor and predictors, the F-statistic must to take on a value close to 1. If \\(H_a\\) is true the F-value must to be greater than 1. If F-value is closer to 1, the answer to reject the hyphotesis depends on values of \\(n\\) and \\(p\\). Using F-value, p-value is computed. 5.3.2.2 p-value Using p-value we can determine whether or not to reject \\(H_0\\). p-value is essentially 0: extremely strong evidence that at least one of the media is associated with output variable. R language returns p-value for each coefficient: These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. Call: lm(formula = HDI.Rank ~ CPI + HDI) Residuals: Min 1Q Median 3Q Max -19.9326 -6.5788 -0.5189 5.9769 21.6061 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 293.9914 2.5014 117.529 &lt; 2e-16 CPI -2.9350 0.4153 -7.068 3.89e-11 HDI -283.8765 5.0064 -56.703 &lt; 2e-16 *** — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 8.178 on 170 degrees of freedom Multiple R-squared: 0.9782, Adjusted R-squared: 0.9779 F-statistic: 3806 on 2 and 170 DF, p-value: &lt; 2.2e-16 Pr(&gt;|t|) of the coefficients is close to 0 (significatives) - see *** in each row of the table. F-value is 3806 (must to be greater than 1). p-value is close to 0. degrees of freedom: The number of independent pieces of information used in the estimation of a parameter. From the algebaic point of view is the theal number of equations used from the data. (see Grados de libertad) Multiple R-squared is used for evaluating how well your model fits the data. 97% of the variability in HDI.Rank is explained by CPI+HDI. (Multiple R squared) measures the amount of variation in the response variable that can be explained by the predictor variables. Adjusted Rsquared adds penalties for the number of predictors in the model. Therefore it shows a balance between the most parsimonious model, and the best fitting model ( ratio between the number of observations and the predictors). If you have a large difference between your multiple and your adjusted Rsquared that indicates you may have overfit your model. 5.4 Improving the models Trying a more complicated model Polynomial regression. Polynomial ortogonal regression. Non linear regression. etc. Using the previous example: Call: lm(formula = CPI ~ HDI) Coefficients: (Intercept) HDI -1.540 8.497 Computing errors: We develop a function that receives the model, the input variable, and computes the norm 2 of the vector of errors (eucliean distance): \\[\\mid\\mid \\vec{e} \\mid\\mid= \\sqrt{\\sum (CPI_{i}-\\phi(HDI_{i}))^2} \\] - \\(\\phi(HDI_{i}))\\) will be in the function y_aprox [1] 19.69212 Is this result coherent with residuals? [1] 2.734588e-14 We obtain the same values (taking in account rouning). In any case the norm 2 of the errors in all the points of the dataset is 19.6921162 (computed using mi.verror$norma.error). In the following we will improve the model. Before, we will examine the information computed in the building of the model (write f1$ and R will show all the possible parameters): CPI HDI 1 1.5 0.398 2 3.1 0.739 3 2.9 0.698 4 2.0 0.486 5 3.0 0.797 6 2.6 0.716 1 2 3 4 5 6 1.841949 4.739580 4.391184 2.589725 5.232432 4.544139 [1] 2.743037e-14 5.4.1 Updating the models 5.4.2 Searching the best regression Given a set of variables \\(A, B, X, Y, Z, U\\) we can test the following models, in addition to the combinations of these with the linear and polynomial models previously seen. \\(+\\) para combinaciones de variables \\(A+B\\) \\(.\\) combinación de una de las variables con todas las demás Examples: 5.4.3 Orthogonal polynomials When we approach a function using classical approach - we minimize the norm 2 of the errors - we need to solve this linear equation system: For this proposal we use in R language ``I(x^k)`` to add a term with k degree. In orthogonal approach the system will be: For instance using Legendre family: In R language we will use ``poly(x,k)`` to use orthogonal polynomials of k degree. Trying a polynomial grade of \\(n\\) for \\(\\phi(x)\\). Call: lm(formula = CPI ~ HDI + I(HDI^2)) Coefficients: (Intercept) HDI I(HDI^2) 9.552 -30.051 30.785 Call: lm(formula = CPI ~ HDI + I(HDI^2) + I(HDI^3)) Coefficients: (Intercept) HDI I(HDI^2) I(HDI^3) -7.116 59.605 -119.817 80.060 Call: lm(formula = CPI ~ HDI + I(HDI^2) + I(HDI^3) + I(HDI^4)) Coefficients: (Intercept) HDI I(HDI^2) I(HDI^3) I(HDI^4) -0.08463 7.38593 18.62201 -75.89626 63.32330 Call: lm(formula = CPI ~ poly(HDI, 2)) Coefficients: (Intercept) poly(HDI, 2)1 poly(HDI, 2)2 4.052 19.568 11.859 Call: lm(formula = CPI ~ poly(HDI, 3)) Coefficients: (Intercept) poly(HDI, 3)1 poly(HDI, 3)2 poly(HDI, 3)3 4.052 19.568 11.859 5.159 Call: lm(formula = CPI ~ poly(HDI, 4)) Coefficients: (Intercept) poly(HDI, 4)1 poly(HDI, 4)2 poly(HDI, 4)3 poly(HDI, 4)4 4.0520 19.5681 11.8590 5.1587 0.6239 For each model found, calculate in R the errors made, the accuracy, etc. The parameter $ Pr (&gt;F)$ is the probability that rejecting null hypothesis (the most complex model does not fit better than the simplest model) could be an error. 5.5 Model validation It is important to evaluate how the model adjusts the point cloud. There are many ways to make this assessment. Usually statisticians examine diagnostic plots after constructing the model. Most evaluation models focus on residuals If we call \\(\\phi(x)\\) to the regression linear function, at each point the error (residual) function is \\[e (x_i)=\\mid \\phi(x_i)-y_i\\mid \\] 5.5.1 Residuals We assume these errors: They must have a zero average. If this is not the case, the bias must be measured. More points may need to be included in the data cloud. Errors must be uniformly distributed. We must wait for the residues to be uniformly distributed without patterns that detect anomalies. A first way would be to face the waste with the adjustment and the points should be around \\(y=0\\). Facing residues with the adjustment and points should be around y = 0. 5.5.2 Other study of residuals Another possibility would be to confront the residues with respect to the X-axis values. Points should be around \\(y=0\\). Draw the point cloud next to the adjustment. Useful commands: predict, fitted. Estimators with residuals or draw the residuals 1 2 3 4 5 6 2.468377 4.156869 3.575200 2.218723 5.156484 3.817822 1 2 3 4 5 6 2.468377 4.156869 3.575200 2.218723 5.156484 3.817822 1 2 3 4 5 6 -0.9683773 -1.0568692 -0.6752004 -0.2187232 -2.1564845 -1.2178223 [1] -3.145643e-17 5.6 Proyecto - regresion EconomistData Improving regression with EconomistData dataSet. (Intercept) CPI HDI 293.991367 -2.935028 -283.876464 Call: lm(formula = HDI.Rank ~ CPI + HDI) Coefficients: (Intercept) CPI HDI 293.991 -2.935 -283.876 1 2 3 4 5 6 -4.60599167 -5.10807191 8.66598738 -2.15734861 -13.93673984 2.89525520 7 8 9 10 11 12 -2.44188161 -0.86747447 2.76622608 -0.69690560 -8.21829193 1.87144180 13 14 15 16 17 18 1.01589083 -7.33669193 -2.46410654 3.02896845 11.92180933 10.43680829 19 20 21 22 23 24 -2.51782785 21.60610822 4.98504226 1.88724332 -10.43701937 -10.22317210 25 26 27 28 29 30 -13.70985035 -0.36041659 0.17465976 -4.69679020 16.39312099 -11.16467728 31 32 33 34 35 36 -12.00982993 -0.33860867 12.59786618 4.54001917 -1.02878983 -19.93264142 37 38 39 40 41 42 0.77185089 0.30085876 -3.98371883 -10.28558786 -10.37611145 -6.04445802 43 44 45 46 47 48 -8.52410046 3.66733567 1.88059785 7.79734086 7.23059067 1.32426390 49 50 51 52 53 54 10.33665832 12.32046646 0.02684822 -10.58090997 -4.17033750 -9.01963378 55 56 57 58 59 60 5.97694164 -2.49937369 12.14645508 3.50934743 1.12369775 -4.60293950 61 62 63 64 65 66 6.03241096 -10.59463476 7.87830014 -12.17430366 -11.32591264 10.04000583 67 68 69 70 71 72 -1.82840115 12.06249696 -1.41606338 -10.84704151 -0.70956622 4.38764699 73 74 75 76 77 78 13.96549663 2.63386986 5.95289808 -7.21882433 -7.88590197 -10.43672650 79 80 81 82 83 84 1.07241621 -2.73844536 12.36203288 -6.57882450 -0.04118425 14.24613472 85 86 87 88 89 90 -8.50502517 -1.74412350 12.75621810 -0.78303728 -10.14369399 -5.86908897 91 92 93 94 95 96 4.02564135 -9.20391934 -8.37519745 -9.96329461 2.07726908 2.07442105 97 98 99 100 101 102 -0.63569608 -4.34075557 9.98854683 -8.86163679 -5.36998964 0.64873945 103 104 105 106 107 108 4.63934387 -9.60140437 9.75604064 9.30454080 -9.38249946 11.20383176 109 110 111 112 113 114 -10.65856881 -3.47649222 16.34554816 -0.51888391 -6.54203159 -3.34876745 115 116 117 118 119 120 9.54944141 -16.91023903 -0.64800176 1.11939457 9.22967666 1.41994197 121 122 123 124 125 126 -8.28864876 -2.24787220 8.24354415 1.79816613 8.45614978 -8.05714531 127 128 129 130 131 132 -5.43163410 0.04217940 -11.71774620 -6.62056839 8.46677827 13.79651559 133 134 135 136 137 138 11.57122268 11.76225118 -6.49236455 -0.18048754 -7.85640169 -8.46672378 139 140 141 142 143 144 -11.27130400 4.84791253 -10.49828222 -4.72790498 0.71020644 16.76178084 145 146 147 148 149 150 -3.55065523 8.85286350 -4.47372418 11.84971387 3.29073539 -0.07127899 151 152 153 154 155 156 -0.82266968 12.04963221 12.07221208 -0.89984945 12.59147817 0.57155095 157 158 159 160 161 162 -1.46103690 4.95625185 -6.85316332 9.30751297 8.76540081 7.44393283 163 164 165 166 167 168 0.66160421 -4.29485930 -3.87368502 -10.82508278 -3.17089656 7.66949195 169 170 171 172 173 16.43301085 -6.76561190 -2.67688090 1.46760353 -7.79675397 In the following, we check some models. 5.7 Proyecto - Analysing Boston dataset This dataset stores the Housing Values in Suburbs of Boston. 5.7.1 Fit a simple linear regression \\[medv=\\phi(lstat)=a_0+a_1\\, lstat\\] lstat: lower status of the population (percent). medv: median value of owner-occupied homes in $1000s. The model founded is: \\[medv=\\phi(lstat)=34.55-0.95\\, lstat\\] 5.7.2 Analyzing the model For more detailed information: For more information we can use names with the linear model: Confidence interval for the coeffient estimates: Visualizing the dataset+model: ## Visualizing the linear model # Computing residuals residuals: rstudent:return the studentized residuals Some plots regarding residuals, etc… On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues function which.max(): function identifies the index of the largest element of a which.max() vector. In this case, it tells us which observation has the largest leverage statistic. 5.7.3 Multiple regression Fitting a model trying the regression with two input variables. lstat: lower status of the population (percent). medv: median value of owner-occupied homes in $1000s. age: age proportion of owner-occupied units built prior to 1940. \\[medv=\\phi(lstat,age)=a_0+a_1\\, lstat+ +a_2\\, age\\] Data set contains 13 variables, how to select the best variables?: Facing the output variable against the rest of variables Analyzing the summary of the model. 5.7.4 Interaction terms 5.7.5 Non-linear transformations of the predictors The p-value associated with the quadratic term suggests that lm.fit5 is an improved model. 5.7.6 Using anova Fist, we compare the model: linear (lm.fit) and quadratic (lm.fit5). If the models are represented, we could extract some important ideas: We can see a non-linearity in the relationship between medv and lstat. Now, we will use anova() (Analysis of Variance Table) performs a hypothesis test comparing the models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Interpretation: The null hypothesis says that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. See the F-statistic and the p-value associated. This provides very clear evidence that the model containing the predictors \\(lstat\\) and \\(lstat^2\\) is better than the model with the predictor \\(lstat\\). 5.8 Poyecto - evaluación de cursos - variables categóricas Regresión múltiple: \\[y=f(x_1,x_2,\\dots)\\] \\(y\\) es una variable de salida numérica \\(x_1\\) es variable 1 de entrada numérica o categórica \\(x_1\\) es variable 2 de entrada numérica o categórica, … 5.8.1 EDA - exploratory data analysis Vamos a explorar las relaciones en un .csv con relaciones entre las puntuaciones de la evaluación de la enseñanza dadas por los estudiantes y las puntuaciones estéticas, de edad, etc. de los profesores. Recordatorio de fases de EDA: Observar los valores Preprocesamiento Estimar estadísticos Visualizaciones ID score bty_avg age gender 1 1 4.7 5 36 female 2 2 4.1 5 36 female 3 3 3.9 5 36 female 4 4 4.8 5 36 female 5 5 4.6 3 59 male 6 6 4.3 3 59 male ID score bty_avg age Min. : 1.0 Min. :2.300 Min. :1.667 Min. :29.00 1st Qu.:116.5 1st Qu.:3.800 1st Qu.:3.167 1st Qu.:42.00 Median :232.0 Median :4.300 Median :4.333 Median :48.00 Mean :232.0 Mean :4.175 Mean :4.418 Mean :48.37 3rd Qu.:347.5 3rd Qu.:4.600 3rd Qu.:5.500 3rd Qu.:57.00 Max. :463.0 Max. :5.000 Max. :8.167 Max. :73.00 gender Length:463 Class :character Mode :character Nota: Nuevas funciones de tidyverse en última sección. Rows: 463 Columns: 5 $ ID 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… $ score 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4.… $ bty_avg 5.000, 5.000, 5.000, 5.000, 3.000, 3.000, 3.000, 3.333, 3.333,… $ age 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 40… $ gender “female”, “female”, “female”, “female”, “male”, “male”, “male”… Table 5.1: Data summary Name Piped data Number of rows 463 Number of columns 5 _______________________ Column type frequency: character 1 numeric 4 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace gender 0 1 4 6 0 2 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ID 0 1 232.00 133.80 1.00 116.50 232.00 347.5 463.00 ▇▇▇▇▇ score 0 1 4.17 0.54 2.30 3.80 4.30 4.6 5.00 ▁▁▅▇▇ bty_avg 0 1 4.42 1.53 1.67 3.17 4.33 5.5 8.17 ▃▇▇▃▂ age 0 1 48.37 9.80 29.00 42.00 48.00 57.0 73.00 ▅▆▇▆▁ ID score bty_avg age gender 1 284 4.0 1.667 34 female 2 336 3.1 1.667 60 male 3 406 5.0 2.833 57 male 4 101 4.4 4.333 48 male 5 111 3.5 4.333 57 female mean_bty_avg mean_score median_bty_avg median_score 1 4.417844 4.17473 4.333 4.3 Table 5.1: Data summary Name Piped data Number of rows 463 Number of columns 2 _______________________ Column type frequency: numeric 2 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist score 0 1 4.17 0.54 2.30 3.80 4.30 4.6 5.00 ▁▁▅▇▇ bty_avg 0 1 4.42 1.53 1.67 3.17 4.33 5.5 8.17 ▃▇▇▃▂ \\(p0\\): 0-percentile: valor al cual el 0% de las observaciones son más pequeñas que él (valor mínimo por tanto) \\(p25\\): 25-percentile: valor al cual el 25% de las observaciones son más pequeñas que él … 5.8.1.1 Correlación Se establece entre variables numéricas El coeficiente de correlación es estimador de la relación lineal entre dos variables numéricas. Su valor oscila entre -1 y 1: -1: perfecta relación negativa 0: no relación +1 perfecta relación positiva correlation 1 0.1871424 relación debilmente positiva 5.8.1.2 Visualización Objetivo: visualizar relaciones entre puntuaciones, edad según el género. 5.8.2 Modelo lineal Construimos un primer modelo lineal univariable (puntuación frente a media de las edades): Call: lm(formula = score ~ bty_avg, data = evals5) Coefficients: (Intercept) bty_avg 3.88034 0.06664 Call: lm(formula = score ~ bty_avg, data = evals5) Residuals: Min 1Q Median 3Q Max -1.9246 -0.3690 0.1420 0.3977 0.9309 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.88034 0.07614 50.96 &lt; 2e-16 bty_avg 0.06664 0.01629 4.09 5.08e-05 — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 0.5348 on 461 degrees of freedom Multiple R-squared: 0.03502, Adjusted R-squared: 0.03293 F-statistic: 16.73 on 1 and 461 DF, p-value: 5.083e-05 5.8.3 Interacción de modelos Se usa el operador de construcción de modelos :. \\[y=f_1(x_1,x_2)\\] En nuestro Call: lm(formula = score ~ age * gender, data = evals5) Coefficients: (Intercept) age gendermale age:gendermale 4.88299 -0.01752 -0.44604 0.01353 Call: lm(formula = score ~ age * gender, data = evals5) Residuals: Min 1Q Median 3Q Max -1.86453 -0.34815 0.09863 0.40661 0.96327 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.882989 0.205210 23.795 &lt; 2e-16 age -0.017523 0.004472 -3.919 0.000103 gendermale -0.446044 0.265407 -1.681 0.093520 . age:gendermale 0.013531 0.005531 2.446 0.014803 * — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 0.5314 on 459 degrees of freedom Multiple R-squared: 0.05138, Adjusted R-squared: 0.04518 F-statistic: 8.288 on 3 and 459 DF, p-value: 2.227e-05 Explicación: intercept y age son las variables para el género femenino (female alfabéticamente antes que male) gendermale y age:gendermale son los desplazamientos del intercept y de slope de los profesores (male) respecto a los valores de intercept y de slope de las profesoras. Regresión obtenida: \\[y=b_0+b_1 \\cdot age + b_2 \\cdot is\\_male(x) + b_3 \\cdot is\\_male(x)\\] - \\(b_0\\) es (Intercept) - \\(b_1\\) es age - \\(b_2\\) es gendermale - \\(b_3\\) es age:gendermale La interacción estudia si el efecto asociado de una variable depende del valor de otra variable. en nuestro caso, la respuesta es que si existe: existe una da diferencia en las pendientes de la edad de los instructores masculinos con respecto a los femeninos 5.8.4 Multiple Regression Table 5.2: Data summary Name Piped data Number of rows 463 Number of columns 3 _______________________ Column type frequency: character 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace gender 0 1 4 6 0 2 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist score 0 1 4.17 0.54 2.3 3.8 4.3 4.6 5 ▁▁▅▇▇ age 0 1 48.37 9.80 29.0 42.0 48.0 57.0 73 ▅▆▇▆▁ correlation 1 -0.107032 modelo con la edad Call: lm(formula = score ~ age, data = evals5) Coefficients: (Intercept) age 4.461932 -0.005938 Call: lm(formula = score ~ age + bty_avg, data = evals5) Coefficients: (Intercept) age bty_avg 4.054732 -0.003059 0.060656 Call: lm(formula = score ~ age, data = evals5) Residuals: Min 1Q Median 3Q Max -1.9185 -0.3531 0.1172 0.4172 0.8825 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.461932 0.126778 35.195 &lt;2e-16 ** age -0.005938 0.002569 -2.311 0.0213 — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 0.5413 on 461 degrees of freedom Multiple R-squared: 0.01146, Adjusted R-squared: 0.009311 F-statistic: 5.342 on 1 and 461 DF, p-value: 0.02125 Call: lm(formula = score ~ age + bty_avg, data = evals5) Residuals: Min 1Q Median 3Q Max -1.9427 -0.3474 0.1293 0.3957 0.9478 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.054732 0.169865 23.870 &lt; 2e-16 age -0.003059 0.002664 -1.148 0.251396 bty_avg 0.060656 0.017098 3.548 0.000429 — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 0.5347 on 460 degrees of freedom Multiple R-squared: 0.03778, Adjusted R-squared: 0.0336 F-statistic: 9.031 on 2 and 460 DF, p-value: 0.0001422 5.8.5 Validando el modelo Un test de hipótesis consiste en una prueba entre dos hipótesis contrapuestas Hipótesis nula \\(H_0\\) versus una hipótesis alternativa Generalmente, la hipótesis nula es una afirmación de que “no hay efecto” o “no hay diferencia de interés”. En muchos casos, la hipótesis nula representa el statu quo o una situación en la que no ocurre nada interesante. Además, generalmente la hipótesis alternativa es la afirmación que el experimentador o investigador quiere establecer o encontrar pruebas que la respalden. Se considera una hipótesis “contraria” a la hipótesis nula \\[y=a+bx\\] \\(H_0\\): no hay modelo, \\(b=0\\) \\(H_a\\): hay modelo, \\(b\\neq0\\) umbral \\(0.05\\), Un estadístico de prueba (test statistic) es una fórmula de estimación puntual/estadística de muestra que se utiliza para la comprobación de hipótesis: t-test statistic p-value es la probabilidad de obtener un estadístico de prueba igual o más extremo que el estadístico de prueba observado suponiendo que la hipótesis nula \\(H_0\\) es verdadera. 5.9 Anexo: R avanzado 5.9.1 Más de tidyverse [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) [sample_n](https://dplyr.tidyverse.org/reference/sample_n.html?q=sample_n#null) 5.9.2 Skimr package [skim](https://www.datanovia.com/en/blog/display-a-beautiful-summary-statistics-in-r-using-skimr-package/) skim() can handle data that has been grouped using dplyr::group_by. 5.10 Proyecto - Predicción de riesgos Importa en R el dataset riesgos.csv La compañía de seguros quiere tener un modelo para predecir los gastos médicos de los asegurados. Realiza un análisis del dataset y desarrolla uno o varios modelos que contesten a lo siguiente. Entregar en el book. Analiza la estructura del dataset. ¿Qué tipo datos tenemos? Plantea los problemas que podemos tener al trabajar con este dataset. Analiza estadísticamente los atributos. Detecta normalidad, sesgos, outliers, etc. Dibuja el atributo gastos con un histograma. ¿Qué conocimiento extraes de la información visualizada? Obten la matriz de correlación entre los atributos del dataset. ¿Qué atributos parecen estar más y menos relacionados? (cor). Visualiza las relaciones entre los atributos - scatterplot (plot, pairs, pairs.panels). Plantea un modelo lineal m1 de regresión entre gastos y otra variable (la que pienses mejor modela los gastos médicos de los asegurados). Intenta un modelo m2 usando funciones polinómicas. Evalua la eficiencia de los modelos (summary). Extrae toda la información acerca de la validez de los dos modelos creados. Mejora el modelo usando regresión generalizada. Crea un modelo m3 teniendo en cuenta todas las variables. Analiza qué variables son significativas. Mira la eficiencia del nuevo modelo. Usa anova para vez que modelo de los creados es más interesante. "],["series-temporales.html", "Capítulo 6 Series Temporales", " Capítulo 6 Series Temporales "],["clustering.html", "Capítulo 7 Clustering 7.1 Fundamentos 7.2 Clustering en R 7.3 Número optimo de clusters 7.4 Clustering jerárquico", " Capítulo 7 Clustering 7.1 Fundamentos 7.1.1 Medidas de similaridad Los métodos matemáticos en la que descansan las técnicas de clustering están basados en la búsqueda de patrones de similaridad/disimilaridad en los datos. Dado un conjunto de valores de un dominio, las medidas de disimilariad son funciones que asignan valores reales a pares de instancias del dominio. Estas funciones se usan o bien en la formación del cluster (grupos iniciales) o el proceso de modelización del cluster (grupos evolucionan). Estos algoritmos se denominan algoritmos de clustering basados en similaridad o más bien disimilaridad. Las funciones de similaridad/disimilaridad miden y expresan numericamente el grado en el que dos instancias del mismo dominio (descritas por un conjunto de atributos) sin similares/disimilares unas de otras. Se podrían clasisificar en: Basadas en diferencias: transforman y agregan de alguna forma las diferencias para cada dos instancias comparadas. Basadas en correlaciones: detectan patrones comunes de los valores inferiores y superiores de los atributos para cada dos instancias de valores. 7.1.2 Disimilaridad basada en diferencias Dadas dos instancias \\(x_1,x_2\\) con un conjunto de atributos \\(a_i\\), estas medidas devuelven una matriz con la distancia existente entre cada par de atributos en cada una de las instancias. Distancias a usar para calcular la matriz: Distancia euclidea: \\(d_{euc}=\\mid \\mid x-y \\mid\\mid_2\\) Distancia de Manhattan: \\(d_{euc}=\\displaystyle \\sum_{i=1}^n \\mid (x_i-y_i)\\mid\\) Distancia de Minkowski: Distancia de Canberra Distancia de Chebysehv Distancia de Hamming 7.1.3 Matriz de distancias library(cluster) library(factoextra) data(&quot;USArrests&quot;) usa_arrests &lt;- na.omit(USArrests) dist_euc &lt;- get_dist(usa_arrests) fviz_dist(dist_euc) La función get_dist por defecto calcula la distancia euclídea pero soporta otras funciones distancia. 7.1.4 Similaridad basada en correlaciones A veces debemos tener en cuenta que dos instancias son similares según el conocimiento del dominio más que en los valores de los atributos. Si atributos representan frecuencias de eventos, cuentas de frecuencias de palabras en textos. Las diferencias que importan más que cuantitativas son discretas: alto, bajo, etc. Medidas: Correlación de Pearson Correlación de Spearman Similaridad del coseno 7.1.5 Correlación de Pearson Establece el grado de relación entre dos atributos Normalmente es la usada \\[\\displaystyle d_{cor}=1-\\frac{\\sum ( x_i - \\bar{x} )( y_i - \\bar{y} ) }{\\sqrt{\\sum ( x_i - \\bar{x} )^2\\sum ( y_i - \\bar{y} )^2}} \\] 7.2 Clustering en R Clustering es una técnica de aprendizaje no supervisado. Se introduce el número de grupos (clusters) a identificar. Algoritmo: El algoritmo debe garantizar la convergencia. 7.2.1 Centros Clustering es muy sensible a la elección inicial de los centros. Y es un algoritmo muy exigente en recursos. Para calcular las distancias de puntos a centros se usa: Métodos de clustering 7.2.2 Objetivo Dado un dataset \\(D=\\{x_1,x_2, \\ldots,x_n\\}\\) donde \\(x_i\\) denota las filas y con \\(m\\) atributos \\(A_i\\) en las columnas tal que cada fila \\(x_i=\\{a_1,a_2,\\ldots,a_m\\}\\) con \\(a_k\\) una valor del atributo \\(A_k\\). Los métodos de clustering agrupan la población del dataset (las filas) en \\(k\\) grupos (clusters) con \\(k \\leq n\\). 7.2.3 k-means Características: Algoritmo: Algoritmo debe garantizar la convergencia. El método k-means agrupa el dataset en \\(k\\) grupos distintos \\(S=\\{S_1,\\ldots,S_k\\}\\) minimizando la media interna de la suma de cuadrados. \\[min_{S}\\sum_{j=1}^k\\sum_{x_j\\in S_j}\\mid\\mid x_i -\\overline{m_j}\\mid\\mid^2\\] donde \\(\\overline{m_j}\\) es el vector media de los \\(m\\) atributos promediado en todas las filas del cluster y \\(\\mid\\mid x_i -\\overline{m_j}\\mid\\mid^2= \\sum_{r=1}(x_r - m_r)^2\\) (suma de las diferencias al cuadrado de los \\(m\\) atributos - distancia euclidea entre los vectores de cada grupo y la media) 7.3 Número optimo de clusters 7.3.1 Proyecto - dataset USArrests We can compute k-means in R with the kmeans function. Here will group the data into two clusters (centers = 2). The kmeans function also has an nstart option that attempts multiple initial configurations and reports on the best one. For example, adding nstart = 25 will generate 25 initial configurations. This approach is often recommended. List of 9 $ cluster : Named int [1:50] 1 1 1 1 1 1 2 1 1 1 … ..- attr(, “names”)= chr [1:50] “Alabama” “Alaska” “Arizona” “Arkansas” … $ centers : num [1:2, 1:4] 11.86 4.84 255 109.76 67.62 … ..- attr(, “dimnames”)=List of 2 .. ..$ : chr [1:2] “1” “2” .. ..$ : chr [1:4] “Murder” “Assault” “UrbanPop” “Rape” $ totss : num 355808 $ withinss : num [1:2] 41637 54762 $ tot.withinss: num 96399 $ betweenss : num 259409 $ size : int [1:2] 21 29 $ iter : int 1 $ ifault : int 0 - attr(*, “class”)= chr “kmeans” We can also view our results by using fviz_cluster. This provides a nice illustration of the clusters. If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance. Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables. 7.3.2 Tamaño optimal de clusters Una forma sencilla de estimar el número K óptimo de clusters cuando no se dispone de información adicional en la que basarse es aplicar el algoritmo para un rango de valores de K, identificando aquel a partir del cual la reducción en la suma total de varianza intra-cluster deja de ser sustancial (en las siguientes secciones se describen otras alternativas). La función fviz_nbclust() automatiza este proceso. En este caso, dado que se sospecha de la presencia de outliers, se emplea la distancia de Manhattan como medida de similitud. A partir de 4 clusters la reducción en la suma total de cuadrados internos parece estabilizarse, indicando que K = 4 es una buena opción. 7.4 Clustering jerárquico 7.4.1 Proyecto dataset protein Country RedMeat WhiteMeat Eggs Milk Fish Cereals Starch Nuts Fr.Veg 1 Albania 10.1 1.4 0.5 8.9 0.2 42.3 0.6 5.5 1.7 2 Austria 8.9 14.0 4.3 19.9 2.1 28.0 3.6 1.3 4.3 3 Belgium 13.5 9.3 4.1 17.5 4.5 26.6 5.7 2.1 4.0 4 Bulgaria 7.8 6.0 1.6 8.3 1.2 56.7 1.1 3.7 4.2 5 Czechoslovakia 9.7 11.4 2.8 12.5 2.0 34.3 5.0 1.1 4.0 6 Denmark 10.6 10.8 3.7 25.0 9.9 21.9 4.8 0.7 2.4 [1] 25 10 RedMeat WhiteMeat Eggs Milk Fish Cereals Starch Nuts Fr.Veg 10.1 1.4 0.5 8.9 0.2 42.3 0.6 5.5 1.7 8.9 14.0 4.3 19.9 2.1 28.0 3.6 1.3 4.3 13.5 9.3 4.1 17.5 4.5 26.6 5.7 2.1 4.0 7.8 6.0 1.6 8.3 1.2 56.7 1.1 3.7 4.2 9.7 11.4 2.8 12.5 2.0 34.3 5.0 1.1 4.0 10.6 10.8 3.7 25.0 9.9 21.9 4.8 0.7 2.4 8.4 11.6 3.7 11.1 5.4 24.6 6.5 0.8 3.6 9.5 4.9 2.7 33.7 5.8 26.3 5.1 1.0 1.4 18.0 9.9 3.3 19.5 5.7 28.1 4.8 2.4 6.5 10.2 3.0 2.8 17.6 5.9 41.7 2.2 7.8 6.5 5.3 12.4 2.9 9.7 0.3 40.1 4.0 5.4 4.2 13.9 10.0 4.7 25.8 2.2 24.0 6.2 1.6 2.9 9.0 5.1 2.9 13.7 3.4 36.8 2.1 4.3 6.7 9.5 13.6 3.6 23.4 2.5 22.4 4.2 1.8 3.7 9.4 4.7 2.7 23.3 9.7 23.0 4.6 1.6 2.7 6.9 10.2 2.7 19.3 3.0 36.1 5.9 2.0 6.6 6.2 3.7 1.1 4.9 14.2 27.0 5.9 4.7 7.9 6.2 6.3 1.5 11.1 1.0 49.6 3.1 5.3 2.8 7.1 3.4 3.1 8.6 7.0 29.2 5.7 5.9 7.2 9.9 7.8 3.5 24.7 7.5 19.5 3.7 1.4 2.0 13.1 10.1 3.1 23.8 2.3 25.6 2.8 2.4 4.9 17.4 5.7 4.7 20.6 4.3 24.3 4.7 3.4 3.3 9.3 4.6 2.1 16.6 3.0 43.6 6.4 3.4 2.9 11.4 12.5 4.1 18.8 3.4 18.6 5.2 1.5 3.8 4.4 5.0 1.2 9.5 0.6 55.9 3.0 5.7 3.2 x Eggs 3 Nuts 3 Fish 4 Starch 4 Fr.Veg 4 WhiteMeat 8 RedMeat 10 Milk 17 Cereals 32 x Eggs 1.1 Starch 1.6 Fr.Veg 1.8 Nuts 2.0 RedMeat 3.3 Fish 3.4 WhiteMeat 3.7 Milk 7.1 Cereals 11.0 List of 9 $ cluster : int [1:25] 3 2 1 3 2 2 2 3 1 3 … $ centers : num [1:3, 1:2] 9 12.06 4.66 15.18 8.84 … ..- attr(, “dimnames”)=List of 2 .. ..$ : chr [1:3] “1” “2” “3” .. ..$ : chr [1:2] “WhiteMeat” “RedMeat” $ totss : num 596 $ withinss : num [1:3] 35.7 39.5 69.9 $ tot.withinss: num 145 $ betweenss : num 451 $ size : int [1:3] 5 8 12 $ iter : int 3 $ ifault : int 0 - attr(, “class”)= chr “kmeans” K-means clustering with 3 clusters of sizes 5, 8, 12 Cluster means: WhiteMeat RedMeat 1 9.000000 15.180000 2 12.062500 8.837500 3 4.658333 8.258333 Clustering vector: [1] 3 2 1 3 2 2 2 3 1 3 2 1 3 2 3 2 3 3 3 3 1 1 3 2 3 Within cluster sum of squares by cluster: [1] 35.66800 39.45750 69.85833 (between_SS / total_SS = 75.7 %) Available components: [1] “cluster” “centers” “totss” “withinss” “tot.withinss” [6] “betweenss” “size” “iter” “ifault” [1] 3 9 12 21 22 2 5 6 7 11 14 16 24 1 4 8 10 13 15 17 18 19 20 23 25 food.Country.Gr1. grpMeat.cluster.Gr1. 1 Belgium 1 2 France 1 3 Ireland 1 4 Switzerland 1 5 UK 1 6 Austria 2 7 Czechoslovakia 2 8 Denmark 2 9 E Germany 2 10 Hungary 2 11 Netherlands 2 12 Poland 2 13 W Germany 2 14 Albania 3 15 Bulgaria 3 16 Finland 3 17 Greece 3 18 Italy 3 19 Norway 3 20 Portugal 3 21 Romania 3 22 Spain 3 23 Sweden 3 24 USSR 3 25 Yugoslavia 3 food.Country.Gr1. grpProtein.cluster.Gr1. 1 Belgium 1 2 France 1 3 Ireland 1 4 Switzerland 1 5 UK 1 6 Bulgaria 2 7 Romania 2 8 Yugoslavia 2 9 Denmark 3 10 Finland 3 11 Norway 3 12 Sweden 3 13 Czechoslovakia 4 14 Hungary 4 15 Poland 4 16 Portugal 5 17 Spain 5 18 Albania 6 19 Greece 6 20 Italy 6 21 USSR 6 22 Austria 7 23 E Germany 7 24 Netherlands 7 25 W Germany 7 [1] “dist” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0.00000 23.176281 21.650173 15.688212 15.154537 30.157586 22.865914 30.99097 23.174124 12.136309 13.157127 27.902509 10.624500 28.302120 26.805410 17.643979 23.111036 10.319399 17.149927 29.987998 24.928899 24.308435 11.034038 29.143953 15.474818 23.17628 0.000000 7.868291 32.304489 10.305338 11.956588 10.742905 17.42125 11.010904 19.529721 16.974982 10.039422 14.688771 6.763875 13.684663 9.942334 22.931202 25.261235 17.440183 13.032268 7.583535 12.920526 19.042846 10.131634 31.946987 21.65017 7.868291 0.000000 32.786125 10.609901 11.119802 8.928606 17.60312 6.007495 18.254862 18.784036 9.146584 13.568714 9.675743 10.803703 12.201639 19.200781 25.878563 13.916537 11.632283 7.525955 6.830081 18.416840 9.066422 32.684859 15.68821 32.304489 32.786125 0.000000 24.005416 40.334105 33.614729 40.33510 33.263494 19.315538 18.399185 38.359744 21.013329 38.526744 38.174730 24.493877 33.293843 8.334867 28.887367 41.480598 35.509013 36.419500 16.675431 40.619823 4.875449 15.15454 10.305338 10.609901 24.005416 0.000000 19.420608 10.613670 24.01999 13.435029 15.025645 9.179869 17.582093 8.705171 16.352676 18.727253 8.255301 19.060430 17.311557 13.070578 20.430370 14.971640 16.497273 12.635268 17.151385 23.977698 30.15759 11.956588 11.119802 40.334105 19.420608 0.000000 15.184532 12.24990 12.718490 24.466099 26.734809 8.938121 21.595833 8.360024 6.688049 17.813759 23.926972 33.292191 21.187732 4.795832 9.650389 11.740528 25.334759 9.901010 39.874177 22.86591 10.742905 8.928606 33.614729 10.613670 15.184532 0.000000 23.82751 13.855324 22.109274 17.517991 16.177763 15.622740 13.268760 14.985326 14.874811 15.156187 26.731629 11.784312 15.584287 14.647184 14.780054 21.369137 10.558409 33.279423 30.99097 17.421251 17.603125 40.335096 24.019992 12.249898 23.827505 0.00000 18.181309 24.107053 29.983829 11.569356 23.755420 14.677534 11.688028 19.358977 31.176273 33.398054 26.571413 11.908820 13.054118 15.951175 24.692712 18.878824 39.330014 23.17412 11.010904 6.007495 33.263494 13.435029 12.718490 13.855324 18.18131 0.000000 18.254589 21.256293 10.153325 15.157836 12.350304 13.150285 14.013208 21.854061 27.116969 17.215110 14.027117 8.182298 6.987131 19.434248 12.459936 33.723137 12.13631 19.529721 18.254862 19.315538 15.025645 24.466099 22.109274 24.10705 18.254589 0.000000 14.933184 22.976510 7.976841 24.062834 21.419617 12.358802 22.156489 13.341664 16.268682 25.157305 20.039960 20.320187 8.175573 26.280411 18.747800 13.15713 16.974982 18.784036 18.399185 9.179869 26.734809 17.517991 29.98383 21.256293 14.933184 0.000000 25.019193 10.702803 23.208619 25.669437 11.992498 22.031568 11.638728 16.165704 27.655198 22.139557 24.172091 12.493198 24.701012 17.615902 27.90251 10.039422 9.146584 38.359744 17.582093 8.938121 16.177763 11.56936 10.153325 22.976510 25.019193 0.000000 20.037215 6.827884 10.883014 16.008748 27.137612 31.295687 21.806192 8.884256 5.097058 8.241966 23.017385 9.709789 37.953919 10.62450 14.688771 13.568714 21.013329 8.705171 21.595833 15.622740 23.75542 15.157836 7.976841 10.702803 20.037215 0.000000 19.872343 18.766726 9.066973 17.866169 14.311184 10.910545 21.877386 16.668833 17.261518 9.462029 21.096682 20.721245 28.30212 6.763875 9.675743 38.526744 16.352676 8.360024 13.268760 14.67753 12.350304 24.062834 23.208619 6.827884 19.872343 0.000000 11.551623 15.336884 25.384838 31.244840 20.661317 8.497647 6.297619 11.987076 24.233448 6.525335 37.969725 26.80541 13.684663 10.803703 38.174730 18.727253 6.688049 14.985326 11.68803 13.150285 21.419617 25.669437 10.883014 18.766726 11.551623 0.000000 16.913308 20.662768 31.020638 17.575836 5.557877 10.653638 10.523783 22.826301 12.152366 37.451569 17.64398 9.942334 12.201639 24.493877 8.255301 17.813759 14.874811 19.35898 14.013208 12.358802 11.992498 16.008748 9.066973 15.336884 16.913308 0.000000 21.676946 17.429859 15.496774 19.150196 13.500370 17.070735 10.812955 18.513238 23.728464 23.11104 22.931202 19.200781 33.293843 19.060430 23.926972 15.156187 31.17627 21.854061 22.156489 22.031568 27.137612 17.866169 25.384838 20.662768 21.676946 0.000000 27.650859 8.788629 24.082774 24.844517 22.770156 24.005208 22.834842 32.829408 10.31940 25.261235 25.878563 8.334867 17.311557 33.292191 26.731629 33.39805 27.116969 13.341664 11.638728 31.295687 14.311184 31.244840 31.020638 17.429859 27.650859 0.000000 22.286094 34.196052 28.574464 29.726924 9.880283 33.438600 6.910861 17.14993 17.440183 13.916537 28.887367 13.070578 21.187732 11.784312 26.57141 17.215110 16.268682 16.165704 21.806192 10.910545 20.661317 17.575836 15.496774 8.788629 22.286094 0.000000 20.789420 19.311396 17.655028 17.884910 19.048097 28.132721 29.98800 13.032268 11.632283 41.480598 20.430370 4.795832 15.584287 11.90882 14.027117 25.157305 27.655198 8.884256 21.877386 8.497647 5.557877 19.150196 24.082774 34.196052 20.789420 0.000000 9.537819 10.903211 26.293155 9.090105 40.833932 24.92890 7.583535 7.525955 35.509013 14.971640 9.650389 14.647184 13.05412 8.182298 20.039960 22.139557 5.097058 16.668833 6.297619 10.653638 13.500370 24.844517 28.574464 19.311396 9.537819 0.000000 7.969316 20.975700 9.624968 35.278889 24.30843 12.920526 6.830081 36.419500 16.497273 11.740528 14.780054 15.95118 6.987131 20.320187 24.172091 8.241966 17.261518 11.987076 10.523783 17.070735 22.770156 29.726924 17.655028 10.903211 7.969316 0.000000 21.605786 11.101802 36.405906 11.03404 19.042846 18.416840 16.675431 12.635268 25.334759 21.369137 24.69271 19.434248 8.175573 12.493198 23.017385 9.462029 24.233448 22.826301 10.812955 24.005208 9.880283 17.884910 26.293155 20.975700 21.605786 0.000000 26.583454 15.791770 29.14395 10.131634 9.066422 40.619823 17.151385 9.901010 10.558409 18.87882 12.459936 26.280411 24.701012 9.709789 21.096682 6.525335 12.152366 18.513238 22.834842 33.438600 19.048097 9.090105 9.624968 11.101802 26.583454 0.000000 40.275551 15.47482 31.946987 32.684859 4.875449 23.977698 39.874177 33.279423 39.33001 33.723137 18.747800 17.615902 37.953919 20.721245 37.969725 37.451569 23.728464 32.829408 6.910861 28.132721 40.833932 35.278889 36.405906 15.791770 40.275551 0.000000 id country 1 Albania 2 Austria 3 Belgium 4 Bulgaria 5 Czechoslovakia 6 Denmark 7 E Germany 8 Finland 9 France 10 Greece 11 Hungary 12 Ireland 13 Italy 14 Netherlands 15 Norway 16 Poland 17 Portugal 18 Romania 19 Spain 20 Sweden 21 Switzerland 22 UK 23 USSR 24 W Germany 25 Yugoslavia [1] 13.07058 [1] 13.07058 [,1] [,2] [1,] -6 -20 [2,] -4 -25 [3,] -12 -21 [4,] -3 -9 [5,] -14 -24 [6,] -15 1 [7,] -22 4 [8,] -10 -13 [9,] -5 -16 [10,] -18 2 [11,] -17 -19 [12,] -2 5 [13,] -23 8 [14,] 3 12 [15,] -11 9 [16,] -1 13 [17,] -7 11 [18,] -8 6 [19,] 7 14 [20,] 15 16 [21,] 18 19 [22,] 17 20 [23,] 10 22 [24,] 21 23 Country RedMeat WhiteMeat Eggs Milk Fish Cereals Starch Nuts Fr.Veg 1 Albania 10.1 1.4 0.5 8.9 0.2 42.3 0.6 5.5 1.7 2 Austria 8.9 14.0 4.3 19.9 2.1 28.0 3.6 1.3 4.3 3 Belgium 13.5 9.3 4.1 17.5 4.5 26.6 5.7 2.1 4.0 4 Bulgaria 7.8 6.0 1.6 8.3 1.2 56.7 1.1 3.7 4.2 5 Czechoslovakia 9.7 11.4 2.8 12.5 2.0 34.3 5.0 1.1 4.0 6 Denmark 10.6 10.8 3.7 25.0 9.9 21.9 4.8 0.7 2.4 cluster 1 1 2 2 3 2 4 3 5 1 6 2 1 2 3 4 7 12 3 3 cluster RedMeat WhiteMeat Eggs Milk Fish Cereals Starch 1 1 8.642857 6.871429 2.385714 14.042857 2.5428571 39.27143 3.742857 2 2 12.091667 9.441667 3.708333 23.000000 4.9916667 24.02500 4.616667 3 3 6.133333 5.766667 1.433333 9.633333 0.9333333 54.06667 2.400000 4 4 7.233333 6.233333 2.633333 8.200000 8.8666667 26.93333 6.033333 Nuts Fr.Veg 1 4.214286 4.657143 2 1.766667 3.491667 3 4.900000 3.400000 4 3.800000 6.233333 [1] RedMeat WhiteMeat Eggs Milk Fish Cereals Starch [8] Nuts Fr.Veg &lt;0 rows&gt; (or 0-length row.names) [1] RedMeat WhiteMeat Eggs Milk Fish Cereals Starch [8] Nuts Fr.Veg &lt;0 rows&gt; (or 0-length row.names) [1] RedMeat WhiteMeat Eggs Milk Fish Cereals Starch [8] Nuts Fr.Veg &lt;0 rows&gt; (or 0-length row.names) [1] RedMeat WhiteMeat Eggs Milk Fish Cereals Starch [8] Nuts Fr.Veg &lt;0 rows&gt; (or 0-length row.names) "],["reglas-de-asociación.html", "Capítulo 8 Reglas de Asociación 8.1 Introducción 8.2 Reglas de Asociación de transacciones 8.3 Reglas de Asociación - preprocesamiento 8.4 Ejercicio en Laboratorio 8.5 arulesViz 8.6 Construyendo un sistema recomendador", " Capítulo 8 Reglas de Asociación 8.1 Introducción Aprendizaje no supervisado: técnica que permite un aprendizaje a partir de observaciones que permite extraer patrones, tendencias y realizar predicciones - sin entrenamiento. Entre los problemas más relevantes - predición de comportamiento de clientes, tendencias de compras: tiendas online, servicios online que ofrecen música, películas, etc. La realidad actual es que hay mucha competencia, muchos servicios similares, mercados online, etc. Objetivo: Atraer clientes es la clave. Medios: datos recolectados de los consumidores, de los usuarios de los servicios, características de los consumidores extraidas de los datos recolectados, patrones previos de compra o de uso, etc. Herramientas: Métodos de Ciencia de Datos - búsqueda de patrones frecuentes, reglas, etc. Los métodos usados en esta área deben analizr los datos almacenados para diseñar sistemas recomendadores con los que: Ayudar a personalizar los servicios y la experiencia de compra, adivinar y sugerir tendencias de compra a partir de likes, dislikes. Controlar las horas punta de los servicios. Analizar combinaciones de productos que la gente suele comprar juntas. Analizar revisiones y precios que la competencia ofrece por los mismos productos Los problemas en este área surgen del llamado Market Basket Analysis que se enfrenta a cómo realizar recomendaciones basadas en productos. Las soluciones para este problema pueden usarse en otros problemas similares: recomendaciones de intereses de las personas, etc. Las técnicas más importantes son: Evaluación de la matriz de contingenica de los productos. Generación de itemsets frecuentes. Extracción de reglas de asociación. 8.1.1 Detectando y prediciendo tendencias Tendencia: patrón específico o comportamiento de compra-venta que aparece en un periodo de tiempo en una tienda. ¿Cómo detectar?: Almacenar todas las transacciones que se realicen en la tienda. items comprados stocks combinaciones de items comprados juntos transacción de cada venta realizada ¿Cómo tratar los datos?: pre-procesar normalizar agregar Aplicar algoritmos: localizar patrones y tendencias Recomendar: usar patrones y tendencias para sugerir nuevas compras. El principal método para conseguir todo esto está basado en el problema denominado Market Basket Analysis. Estos métodos están fundamentados en estadística basado en probabilidad y nociones probabilísticas como: * soporte * confianza * lift, etc. Objetivo: ¿Qué items (productos) se han comprado más frecuentemente? 8.2 Reglas de Asociación de transacciones Explorando el dataset Adult #install.packages(arules) library(arules) data(&quot;Adult&quot;) length(Adult) [1] 48842 dim(Adult) [1] 48842 115 Adult transactions in sparse format with 48842 transactions (rows) and 115 items (columns) inspect(Adult[1:2]) items transactionID [1] {age=Middle-aged, workclass=State-gov, education=Bachelors, marital-status=Never-married, occupation=Adm-clerical, relationship=Not-in-family, race=White, sex=Male, capital-gain=Low, capital-loss=None, hours-per-week=Full-time, native-country=United-States, income=small} 1 [2] {age=Senior, workclass=Self-emp-not-inc, education=Bachelors, marital-status=Married-civ-spouse, occupation=Exec-managerial, relationship=Husband, race=White, sex=Male, capital-gain=None, capital-loss=None, hours-per-week=Part-time, native-country=United-States, income=small} 2 Para calcular las reglas de asociación: data(&quot;Adult&quot;) rules &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9, target = &quot;rules&quot;)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.9 0.1 1 none FALSE TRUE 5 0.5 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[0 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.03s]. sorting and recoding items … [9 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. writing … [52 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. summary(rules) set of 52 rules rule length distribution (lhs + rhs):sizes 1 2 3 4 2 13 24 13 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 3.000 2.923 3.250 4.000 summary of quality measures: support confidence coverage lift Min. :0.5084 Min. :0.9031 Min. :0.5406 Min. :0.9844 1st Qu.:0.5415 1st Qu.:0.9155 1st Qu.:0.5875 1st Qu.:0.9937 Median :0.5974 Median :0.9229 Median :0.6293 Median :0.9997 Mean :0.6436 Mean :0.9308 Mean :0.6915 Mean :1.0036 3rd Qu.:0.7426 3rd Qu.:0.9494 3rd Qu.:0.7945 3rd Qu.:1.0057 Max. :0.9533 Max. :0.9583 Max. :1.0000 Max. :1.0586 count Min. :24832 1st Qu.:26447 Median :29178 Mean :31433 3rd Qu.:36269 Max. :46560 mining info: data ntransactions support confidence Adult 48842 0.5 0.9 call apriori(data = Adult, parameter = list(supp = 0.5, conf = 0.9, target = “rules”)) inspect(head(rules)) lhs rhs support confidence [1] {} =&gt; {capital-gain=None} 0.9173867 0.9173867 [2] {} =&gt; {capital-loss=None} 0.9532779 0.9532779 [3] {hours-per-week=Full-time} =&gt; {capital-gain=None} 0.5435895 0.9290688 [4] {hours-per-week=Full-time} =&gt; {capital-loss=None} 0.5606650 0.9582531 [5] {sex=Male} =&gt; {capital-gain=None} 0.6050735 0.9051455 [6] {sex=Male} =&gt; {capital-loss=None} 0.6331027 0.9470750 coverage lift count [1] 1.0000000 1.0000000 44807 [2] 1.0000000 1.0000000 46560 [3] 0.5850907 1.0127342 26550 [4] 0.5850907 1.0052191 27384 [5] 0.6684820 0.9866565 29553 [6] 0.6684820 0.9934931 30922 8.2.1 Parámetros de apriori parameter: lista con las restricciones en el proceso de extracción supp o support conf o confidence minlen - máximo número de items en itemset maxlen - máximo número de items en itemset maxtime - límite de tiempo target - indicar qué tipo de asociaciones queremos extraer: “rules”, “frequent itemsets”, “maximally frequent itemsets”, “closed frequent itemsets”. rules &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9,minlen=2)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.9 0.1 1 none FALSE TRUE 5 0.5 2 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[0 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.03s]. sorting and recoding items … [9 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. writing … [50 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. inspect(tail(rules)) lhs rhs support confidence coverage lift count [1] {workclass=Private, race=White, capital-loss=None} =&gt; {capital-gain=None} 0.5204742 0.9171628 0.5674829 0.9997559 25421 [2] {workclass=Private, capital-gain=None, native-country=United-States} =&gt; {capital-loss=None} 0.5414807 0.9517075 0.5689570 0.9983526 26447 [3] {workclass=Private, capital-loss=None, native-country=United-States} =&gt; {capital-gain=None} 0.5414807 0.9182030 0.5897179 1.0008898 26447 [4] {race=White, capital-gain=None, native-country=United-States} =&gt; {capital-loss=None} 0.6803980 0.9457029 0.7194628 0.9920537 33232 [5] {race=White, capital-loss=None, native-country=United-States} =&gt; {capital-gain=None} 0.6803980 0.9083504 0.7490480 0.9901500 33232 [6] {race=White, capital-gain=None, capital-loss=None} =&gt; {native-country=United-States} 0.6803980 0.9189249 0.7404283 1.0239581 33232 patterns &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9,maxlen=10, target=&quot;frequent itemsets&quot;)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen NA 0.1 1 none FALSE TRUE 5 0.5 1 maxlen target ext 10 frequent itemsets TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[0 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.03s]. sorting and recoding items … [9 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. sorting transactions … done [0.01s]. writing … [49 set(s)] done [0.00s]. creating S4 object … done [0.00s]. summary(patterns) set of 49 itemsets most frequent items: capital-loss=None capital-gain=None 23 21 native-country=United-States race=White 21 19 workclass=Private (Other) 14 20 element (itemset/transaction) length distribution:sizes 1 2 3 4 9 17 17 6 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 2.000 2.408 3.000 4.000 summary of quality measures: support count Min. :0.5051 Min. :24671 1st Qu.:0.5434 1st Qu.:26540 Median :0.5942 Median :29024 Mean :0.6449 Mean :31497 3rd Qu.:0.7404 3rd Qu.:36164 Max. :0.9533 Max. :46560 includes transaction ID lists: FALSE mining info: data ntransactions support confidence Adult 48842 0.5 1 call apriori(data = Adult, parameter = list(supp = 0.5, conf = 0.9, maxlen = 10, target = “frequent itemsets”)) inspect(tail(patterns)) items support count [1] {race=White, sex=Male, capital-loss=None, native-country=United-States} 0.5113632 24976 [2] {sex=Male, capital-gain=None, capital-loss=None, native-country=United-States} 0.5084149 24832 [3] {workclass=Private, race=White, capital-loss=None, native-country=United-States} 0.5181401 25307 [4] {workclass=Private, race=White, capital-gain=None, capital-loss=None} 0.5204742 25421 [5] {workclass=Private, capital-gain=None, capital-loss=None, native-country=United-States} 0.5414807 26447 [6] {race=White, capital-gain=None, capital-loss=None, native-country=United-States} 0.6803980 33232 appearance: especificar restricciones en la extracción de las asociaciones. rules1 &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9), appearance = list(items = c(&quot;income=small&quot;, &quot;sex=Male&quot;))) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.9 0.1 1 none FALSE TRUE 5 0.5 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[2 item(s)] done [0.00s]. set transactions …[2 item(s), 48842 transaction(s)] done [0.02s]. sorting and recoding items … [2 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 done [0.00s]. writing … [0 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. inspect(head(rules1)) rules2 &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9), appearance = list(none = c(&quot;income=small&quot;, &quot;sex=Male&quot;))) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.9 0.1 1 none FALSE TRUE 5 0.5 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[2 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.03s]. sorting and recoding items … [7 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. writing … [39 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. inspect(head(rules2)) lhs rhs support confidence [1] {} =&gt; {capital-gain=None} 0.9173867 0.9173867 [2] {} =&gt; {capital-loss=None} 0.9532779 0.9532779 [3] {hours-per-week=Full-time} =&gt; {capital-gain=None} 0.5435895 0.9290688 [4] {hours-per-week=Full-time} =&gt; {capital-loss=None} 0.5606650 0.9582531 [5] {workclass=Private} =&gt; {capital-gain=None} 0.6413742 0.9239073 [6] {workclass=Private} =&gt; {capital-loss=None} 0.6639982 0.9564974 coverage lift count [1] 1.0000000 1.000000 44807 [2] 1.0000000 1.000000 46560 [3] 0.5850907 1.012734 26550 [4] 0.5850907 1.005219 27384 [5] 0.6941976 1.007108 31326 [6] 0.6941976 1.003377 32431 8.3 Reglas de Asociación - preprocesamiento Explorando el dataset AdultUCI Contiene los datos del dataset que originalmente se llamó ‘Census Income’ Database en formato data.frame. El dataset Adult del apartado anterior tiene los datos preparados para el cómputo de las reglas. El tipo de datos de este dataset es transactions que es adecuado para el paquete arules. El dataset AdultUCI: library(arules) data(&quot;AdultUCI&quot;) #View(AdultUCI) str(AdultUCI) ‘data.frame’: 48842 obs. of 15 variables: $ age : int 39 50 38 53 28 37 49 52 31 42 … $ workclass : Factor w/ 8 levels “Federal-gov”,..: 7 6 4 4 4 4 4 6 4 4 … $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 … $ education : Ord.factor w/ 16 levels “Preschool”&lt;“1st-4th”&lt;..: 14 14 9 7 14 15 5 9 15 14 … $ education-num : int 13 13 9 7 13 14 5 9 14 13 … $ marital-status: Factor w/ 7 levels “Divorced”,“Married-AF-spouse”,..: 5 3 1 3 3 3 4 3 5 3 … $ occupation : Factor w/ 14 levels “Adm-clerical”,..: 1 4 6 6 10 4 8 4 10 4 … $ relationship : Factor w/ 6 levels “Husband”,“Not-in-family”,..: 2 1 2 1 6 6 2 1 2 1 … $ race : Factor w/ 5 levels “Amer-Indian-Eskimo”,..: 5 5 5 3 3 5 3 5 5 5 … $ sex : Factor w/ 2 levels “Female”,“Male”: 2 2 2 2 1 1 1 2 1 2 … $ capital-gain : int 2174 0 0 0 0 0 0 0 14084 5178 … $ capital-loss : int 0 0 0 0 0 0 0 0 0 0 … $ hours-per-week: int 40 13 40 40 40 40 16 45 50 40 … $ native-country: Factor w/ 41 levels “Cambodia”,“Canada”,..: 39 39 39 39 5 39 23 39 39 39 … $ income : Ord.factor w/ 2 levels “small”&lt;“large”: 1 1 1 1 1 1 1 2 2 2 … En la mayoría de los datasets es necesario un primer paso de preprocesamiento. A continuación aplicaremos el preprocesamiento a AdultUCI hasta convertirlo en transacciones que arules maneja adecuadamente. 8.3.1 Discretización de items Borrar algunas columnas que no son interesantes: {} AdultUCI$fnlwgt &lt;-NULL ## o AdultUCI[[&quot;fnlwgt&quot;]] &lt;- NULL AdultUCI$`education-num` &lt;- NULL Convertir a discretos valores numéricos: age, hours-per-week, capital-gain, capital-loss}. Usaremos comandos {} para hacerlo. A continuación un ejemplo de cómo funcionan estos dos comandos: # ejemplo de funcionamiento de cut y ordered v &lt;- 1:100 v2 &lt;- cut(v,c(0,25,50,75,100),labels=c(&quot;bajo&quot;,&quot;medio&quot;,&quot;alto&quot;,&quot;muyalto&quot;)) v3 &lt;- ordered(v2) Aplicamos estas funciones a AdultUCI: AdultUCI$age &lt;- ordered(cut(AdultUCI[[ &quot;age&quot;]], c(15,25,45,65,100)), labels = c(&quot;Young&quot;, &quot;Middle-aged&quot;, &quot;Senior&quot;, &quot;Old&quot;)) AdultUCI[[ &quot;hours-per-week&quot;]] &lt;- ordered(cut(AdultUCI[[ &quot;hours-per-week&quot;]], c(0,25,40,60,168)), labels = c(&quot;Part-time&quot;, &quot;Full-time&quot;, &quot;Over-time&quot;, &quot;Workaholic&quot;)) AdultUCI[[ &quot;capital-gain&quot;]] &lt;- ordered(cut(AdultUCI[[ &quot;capital-gain&quot;]], c(-Inf,0,median(AdultUCI[[ &quot;capital-gain&quot;]][AdultUCI[[ &quot;capital-gain&quot;]]&gt;0]), Inf)), labels = c(&quot;None&quot;, &quot;Low&quot;, &quot;High&quot;)) AdultUCI[[ &quot;capital-loss&quot;]] &lt;- ordered(cut(AdultUCI[[ &quot;capital-loss&quot;]], c(-Inf,0, median(AdultUCI[[ &quot;capital-loss&quot;]][AdultUCI[[ &quot;capital-loss&quot;]]&gt;0]), Inf)), labels = c(&quot;None&quot;, &quot;Low&quot;, &quot;High&quot;)) Llamamos a apriori: reg &lt;- apriori(AdultUCI) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.8 0.1 1 none FALSE TRUE 5 0.1 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 4884 set item appearances …[0 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.03s]. sorting and recoding items … [31 item(s)] done [0.00s]. creating transaction tree … done [0.02s]. checking subsets of size 1 2 3 4 5 6 7 8 9 done [0.07s]. writing … [6137 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. inspect(head(reg)) lhs rhs support [1] {} =&gt; {race=White} 0.8550428 [2] {} =&gt; {native-country=United-States} 0.8974243 [3] {} =&gt; {capital-gain=None} 0.9173867 [4] {} =&gt; {capital-loss=None} 0.9532779 [5] {relationship=Unmarried} =&gt; {capital-loss=None} 0.1019819 [6] {occupation=Sales} =&gt; {race=White} 0.1005282 confidence coverage lift count [1] 0.8550428 1.0000000 1.000000 41762 [2] 0.8974243 1.0000000 1.000000 43832 [3] 0.9173867 1.0000000 1.000000 44807 [4] 0.9532779 1.0000000 1.000000 46560 [5] 0.9719024 0.1049302 1.019537 4981 [6] 0.8920785 0.1126899 1.043314 4910 8.3.2 Tipo de dato transactions Ver https://www.r-bloggers.com/data-frames-and-transactions/} Comparamos AdultUCI que hemos procesado con Adult. Adult1 &lt;- as(AdultUCI, &quot;transactions&quot;) class(Adult1) [1] “transactions” attr(,“package”) [1] “arules” length(Adult1) [1] 48842 dim(Adult1) [1] 48842 115 Adult1 transactions in sparse format with 48842 transactions (rows) and 115 items (columns) inspect(Adult1[1:2]) items transactionID [1] {age=Middle-aged, workclass=State-gov, education=Bachelors, marital-status=Never-married, occupation=Adm-clerical, relationship=Not-in-family, race=White, sex=Male, capital-gain=Low, capital-loss=None, hours-per-week=Full-time, native-country=United-States, income=small} 1 [2] {age=Senior, workclass=Self-emp-not-inc, education=Bachelors, marital-status=Married-civ-spouse, occupation=Exec-managerial, relationship=Husband, race=White, sex=Male, capital-gain=None, capital-loss=None, hours-per-week=Part-time, native-country=United-States, income=small} 2 data(&quot;Adult&quot;) class(Adult) [1] “transactions” attr(,“package”) [1] “arules” length(Adult) [1] 48842 dim(Adult) [1] 48842 115 Adult transactions in sparse format with 48842 transactions (rows) and 115 items (columns) inspect(Adult[1:2]) items transactionID [1] {age=Middle-aged, workclass=State-gov, education=Bachelors, marital-status=Never-married, occupation=Adm-clerical, relationship=Not-in-family, race=White, sex=Male, capital-gain=Low, capital-loss=None, hours-per-week=Full-time, native-country=United-States, income=small} 1 [2] {age=Senior, workclass=Self-emp-not-inc, education=Bachelors, marital-status=Married-civ-spouse, occupation=Exec-managerial, relationship=Husband, race=White, sex=Male, capital-gain=None, capital-loss=None, hours-per-week=Part-time, native-country=United-States, income=small} 2 8.3.3 Métodos de arules summary(): Visión del conjunto de reglas. length(): Número de reglas. items(): Elementos involucrados. sort(): Ordenar. subset(): Elementos involucrados. (see help(subset). Seleccionar reglas que cumplan ciertos criterios. union(), intersect(), setequal(), match() (usar ayuda help(xxx) ). write(): Escribir reglas con formato más adequado. data(&quot;Adult&quot;) r1 &lt;- apriori(Adult[1:1000], parameter = list(support = 0.5)) r2 &lt;- apriori(Adult[1001:2000], parameter = list(support = 0.5)) #Convertir en un dataframe dfr1 &lt;-DATAFRAME(r1) r_comb &lt;- c(r1, r2) duplicated(r_comb) intersect(r1,r2) union(r1,r2) lhs(reglas1) rhs(reglas1) class(lhs(reglas1)) 8.4 Ejercicio en Laboratorio Utilizar con el dataset Adult los métodos y operaciones vistos en el presente documento. 8.5 arulesViz Vamos a usar el dataset Groceries para ver los comandos de visualización de reglas de asociación más interesantes. Extraemos las reglas: Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.5 0.1 1 none FALSE TRUE 5 0.001 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 9 set item appearances …[0 item(s)] done [0.00s]. set transactions …[169 item(s), 9835 transaction(s)] done [0.00s]. sorting and recoding items … [157 item(s)] done [0.00s]. creating transaction tree … done [0.00s]. checking subsets of size 1 2 3 4 5 6 done [0.01s]. writing … [5668 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. set of 5668 rules lhs rhs support confidence coverage [1] {honey} =&gt; {whole milk} 0.001118454 0.7333333 0.001525165 [2] {tidbits} =&gt; {rolls/buns} 0.001220132 0.5217391 0.002338587 [3] {cocoa drinks} =&gt; {whole milk} 0.001321810 0.5909091 0.002236909 [4] {pudding powder} =&gt; {whole milk} 0.001321810 0.5652174 0.002338587 [5] {cooking chocolate} =&gt; {whole milk} 0.001321810 0.5200000 0.002541942 [6] {cereals} =&gt; {whole milk} 0.003660397 0.6428571 0.005693950 lift count [1] 2.870009 11 [2] 2.836542 12 [3] 2.312611 13 [4] 2.212062 13 [5] 2.035097 13 [6] 2.515917 36 El comando básico de visualización de reglas es plot. Mostramos distintas opciones de uso de plot (colores). Opción de visualización interactiva: Representación matricial de las reglas: set of 52 rules Itemsets in Antecedent (LHS) [1] “{Instant food products,soda}” [2] “{soda,popcorn}” [3] “{flour,baking powder}” [4] “{ham,processed cheese}” [5] “{whole milk,Instant food products}” [6] “{other vegetables,curd,yogurt,whipped/sour cream}” [7] “{processed cheese,domestic eggs}” [8] “{tropical fruit,other vegetables,yogurt,white bread}” [9] “{hamburger meat,yogurt,whipped/sour cream}” [10] “{tropical fruit,other vegetables,whole milk,yogurt,domestic eggs}” [11] “{liquor,red/blush wine}” [12] “{other vegetables,yogurt,whipped/sour cream,cream cheese }” [13] “{yogurt,whipped/sour cream,hard cheese}” [14] “{tropical fruit,root vegetables,other vegetables,whole milk,rolls/buns}” [15] “{tropical fruit,whole milk,yogurt,sliced cheese}” [16] “{other vegetables,butter,sugar}” [17] “{whole milk,whipped/sour cream,hard cheese}” [18] “{other vegetables,hard cheese,domestic eggs}” [19] “{tropical fruit,other vegetables,whipped/sour cream,fruit/vegetable juice}” [20] “{tropical fruit,onions,yogurt}” [21] “{tropical fruit,other vegetables,yogurt,domestic eggs}” [22] “{butter,yogurt,pastry}” [23] “{whole milk,butter,hard cheese}” [24] “{tropical fruit,other vegetables,butter,fruit/vegetable juice}” [25] “{whole milk,curd,yogurt,cream cheese }” [26] “{tropical fruit,other vegetables,hard cheese}” [27] “{other vegetables,whole milk,whipped/sour cream,napkins}” [28] “{citrus fruit,whole milk,cream cheese }” [29] “{tropical fruit,other vegetables,frozen fish}” [30] “{butter,yogurt,hard cheese}” [31] “{curd,yogurt,sugar}” [32] “{other vegetables,whole milk,butter,soda}” [33] “{whole milk,cream cheese ,sugar}” [34] “{frozen vegetables,specialty chocolate}” [35] “{citrus fruit,other vegetables,whole milk,cream cheese }” [36] “{tropical fruit,whipped/sour cream,shopping bags}” [37] “{citrus fruit,tropical fruit,grapes}” [38] “{other vegetables,butter,hard cheese}” [39] “{whole milk,butter,sliced cheese}” [40] “{citrus fruit,other vegetables,soda,fruit/vegetable juice}” [41] “{tropical fruit,other vegetables,whole milk,yogurt,oil}” [42] “{tropical fruit,grapes,fruit/vegetable juice}” [43] “{frankfurter,tropical fruit,domestic eggs}” [44] “{tropical fruit,whole milk,yogurt,frozen meals}” [45] “{other vegetables,curd,yogurt,cream cheese }” [46] “{root vegetables,whole milk,flour}” [47] “{citrus fruit,whole milk,sugar}” [48] “{tropical fruit,other vegetables,misc. beverages}” [49] “{ham,tropical fruit,other vegetables}” [50] “{citrus fruit,grapes,fruit/vegetable juice}” [51] “{whole milk,whipped/sour cream,rolls/buns,pastry}” Itemsets in Consequent (RHS) [1] “{tropical fruit}” “{citrus fruit}” [3] “{root vegetables}” “{pip fruit}” [5] “{fruit/vegetable juice}” “{domestic eggs}” [7] “{whipped/sour cream}” “{butter}” [9] “{curd}” “{beef}” [11] “{bottled beer}” “{white bread}” [13] “{cream cheese }” “{sugar}” [15] “{salty snack}” “{hamburger meat}” E interactiva: Representación matricial mostrando los items: Representación mediante grafos de las reglas (solo para conjuntos pequeños de reglas): Available control parameters (with default values): layout = stress circular = FALSE ggraphdots = NULL edges = nodes = nodetext = colors = c(“#EE0000FF”, “#EEEEEEFF”) engine = ggplot2 max = 100 verbose = FALSE El paquete Graphviz permite una mejor visualización: Permite visualización dinámica: 8.6 Construyendo un sistema recomendador El dataset lastfm.csv del CV incluye las transacciones recogidas en una radio online que almacena el identificador del usuario, artista, sexo del usuario y el país. Objetivo: Construir un sistema de recomendación de grupos de música a los usuarios a partir de dataset anterior. library(arules) lastfm &lt;- read.csv(&quot;data/lastfm.csv&quot;) lastfm[1:20,] user artist sex country 1 1 red hot chili peppers f Germany 2 1 the black dahlia murder f Germany 3 1 goldfrapp f Germany 4 1 dropkick murphys f Germany 5 1 le tigre f Germany 6 1 schandmaul f Germany 7 1 edguy f Germany 8 1 jack johnson f Germany 9 1 eluveitie f Germany 10 1 the killers f Germany 11 1 judas priest f Germany 12 1 rob zombie f Germany 13 1 john mayer f Germany 14 1 the who f Germany 15 1 guano apes f Germany 16 1 the rolling stones f Germany 17 3 devendra banhart m United States 18 3 boards of canada m United States 19 3 cocorosie m United States 20 3 aphex twin m United States length(lastfm$user) ## 289,955 filas [1] 289955 class(lastfm$user) [1] “integer” # Necesitamos convertir este atributo a factor #para poder analizarlo con paquete {\\tt arules} lastfm$user &lt;- factor(lastfm$user) # Ejecuta en tu ordenador # levels(lastfm$user) ## 15,000 users # levels(lastfm$artist) ## 1,004 artists Llamamos a apriori: reglas1 &lt;- apriori(lastfm,parameter=list(support=.01, confidence=.5)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.5 0.1 1 none FALSE TRUE 5 0.01 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 2899 set item appearances …[0 item(s)] done [0.00s]. set transactions …[16165 item(s), 289955 transaction(s)] done [0.26s]. sorting and recoding items … [21 item(s)] done [0.00s]. creating transaction tree … done [0.06s]. checking subsets of size 1 2 done [0.00s]. writing … [19 rule(s)] done [0.00s]. creating S4 object … done [0.01s]. inspect(reglas1) lhs rhs support confidence coverage [1] {} =&gt; {sex=m} 0.73053750 0.7305375 1.00000000 [2] {country=Czech Republic} =&gt; {sex=m} 0.01039472 0.8033049 0.01293994 [3] {country=Mexico} =&gt; {sex=m} 0.01023607 0.7804365 0.01311583 [4] {country=Norway} =&gt; {sex=m} 0.01239503 0.7744021 0.01600593 [5] {country=Turkey} =&gt; {sex=m} 0.01088445 0.6627467 0.01642324 [6] {country=Italy} =&gt; {sex=m} 0.01501612 0.7615882 0.01971685 [7] {country=France} =&gt; {sex=m} 0.01707161 0.8302583 0.02056181 [8] {country=Australia} =&gt; {sex=m} 0.01497474 0.6776963 0.02209653 [9] {country=Canada} =&gt; {sex=m} 0.01568174 0.6563222 0.02389336 [10] {country=Spain} =&gt; {sex=m} 0.02482109 0.7720446 0.03214982 [11] {country=Netherlands} =&gt; {sex=m} 0.02690417 0.8064716 0.03336035 [12] {country=Finland} =&gt; {sex=m} 0.02661103 0.7596731 0.03502957 [13] {country=Russian Federation} =&gt; {sex=m} 0.03062544 0.7605344 0.04026832 [14] {country=Brazil} =&gt; {sex=m} 0.03001845 0.7300788 0.04111673 [15] {country=Sweden} =&gt; {sex=m} 0.03193254 0.7479603 0.04269283 [16] {country=Poland} =&gt; {sex=m} 0.03854391 0.6531471 0.05901261 [17] {country=Germany} =&gt; {sex=m} 0.06069907 0.7257433 0.08363712 [18] {country=United Kingdom} =&gt; {sex=m} 0.07730510 0.8110211 0.09531824 [19] {country=United States} =&gt; {sex=m} 0.13852839 0.6744182 0.20540429 lift count [1] 1.0000000 211823 [2] 1.0996080 3014 [3] 1.0683045 2968 [4] 1.0600442 3594 [5] 0.9072043 3156 [6] 1.0425040 4354 [7] 1.1365033 4950 [8] 0.9276680 4342 [9] 0.8984100 4547 [10] 1.0568172 7197 [11] 1.1039428 7801 [12] 1.0398825 7716 [13] 1.0410615 8880 [14] 0.9993722 8704 [15] 1.0238492 9259 [16] 0.8940638 11176 [17] 0.9934374 17600 [18] 1.1101703 22415 [19] 0.9231808 40167 Comentario: En versiones anteriores de arules el anterior comando daba error. Teníamos que convertir a factor las variables discretas. Es un paquete vivo que va evolucionando día a día. ¿Cual es la recomendación que podemos obtener con estas reglas? No es el tipo de reglas que queremos obtener para nuestro sistema de recomendación. Los datos deben ser manipulados para poder encontrar lo que nos interesa. Usaremos los Comandos: split, lapply. Primero me quedo con una lista de lo que escucha cada usuario: lista.musica.por.usuario &lt;- split(x=lastfm[,&quot;artist&quot;],f=lastfm$user) lista.musica.por.usuario[1:2] $1 [1] “red hot chili peppers” “the black dahlia murder” [3] “goldfrapp” “dropkick murphys” [5] “le tigre” “schandmaul” [7] “edguy” “jack johnson” [9] “eluveitie” “the killers” [11] “judas priest” “rob zombie” [13] “john mayer” “the who” [15] “guano apes” “the rolling stones” $3 [1] “devendra banhart” “boards of canada” “cocorosie” [4] “aphex twin” “animal collective” “atmosphere” [7] “joanna newsom” “air” “portishead” [10] “massive attack” “broken social scene” “arcade fire” [13] “plaid” “prefuse 73” “m83” [16] “the flashbulb” “pavement” “goldfrapp” [19] “amon tobin” “sage francis” “four tet” [22] “max richter” “autechre” “radiohead” [25] “neutral milk hotel” “beastie boys” “aesop rock” [28] “mf doom” “the books” A continuación: Un grupo/cantante podría estar dos veces en un usuario: eliminar repeticiones Convertir a formato transacciones Mirar la música escuchada por los primeros usuarios ## Eliminar duplicados lista.musica.por.usuario &lt;- lapply(lista.musica.por.usuario,unique) # Convertimos en transacciones la lista de música. lista.musica.por.usuario1 &lt;- as(lista.musica.por.usuario,&quot;transactions&quot;) lista.musica.por.usuario[1:5] $1 [1] “red hot chili peppers” “the black dahlia murder” [3] “goldfrapp” “dropkick murphys” [5] “le tigre” “schandmaul” [7] “edguy” “jack johnson” [9] “eluveitie” “the killers” [11] “judas priest” “rob zombie” [13] “john mayer” “the who” [15] “guano apes” “the rolling stones” $3 [1] “devendra banhart” “boards of canada” “cocorosie” [4] “aphex twin” “animal collective” “atmosphere” [7] “joanna newsom” “air” “portishead” [10] “massive attack” “broken social scene” “arcade fire” [13] “plaid” “prefuse 73” “m83” [16] “the flashbulb” “pavement” “goldfrapp” [19] “amon tobin” “sage francis” “four tet” [22] “max richter” “autechre” “radiohead” [25] “neutral milk hotel” “beastie boys” “aesop rock” [28] “mf doom” “the books” $4 [1] “tv on the radio” “tool” [3] “kyuss” “dj shadow” [5] “air” “a tribe called quest” [7] “the cinematic orchestra” “beck” [9] “bon iver” “röyksopp” [11] “bonobo” “the decemberists” [13] “snow patrol” “battles” [15] “the prodigy” “pink floyd” [17] “rjd2” “the flaming lips” [19] “michael jackson” “mgmt” [21] “the rolling stones” “late of the pier” [23] “flight of the conchords” “simian mobile disco” [25] “muse” “fleetwood mac” [27] “led zeppelin” $5 [1] “dream theater” “ac/dc” [3] “metallica” “iron maiden” [5] “bob marley &amp; the wailers” “megadeth” [7] “children of bodom” “trivium” [9] “nightwish” “sublime” [11] “volbeat” $6 [1] “lily allen” “kanye west” “sigur rós” [4] “pink floyd” “stevie wonder” “metallica” [7] “thievery corporation” “iron maiden” “the streets” [10] “muse” “faith no more” “manu chao” [13] “tenacious d” “depeche mode” “justin timberlake” [16] “green day” “snow patrol” “dream theater” [19] “u2” “jay-z” “type o negative” [22] “pearl jam” “queen” # en la versión actual de R esto va bien #error ¿? (en versiones anteriores de R daba error, si os pasa intentar siguientes comandos) #lista.musica.por.usuario2 &lt;- as(lapply(lista.musica.por.usuario, &quot;[[&quot;, 1), &quot;transactions&quot;) #lista.musica.por.usuario2 Visualizamos lo que hemos conseguido hasta el momento: str(lista.musica.por.usuario1) Formal class ‘transactions’ [package “arules”] with 3 slots ..@ data :Formal class ‘ngCMatrix’ [package “Matrix”] with 5 slots .. .. ..@ i : int [1:289953] 280 288 299 373 383 429 457 468 512 714 … .. .. ..@ p : int [1:15001] 0 16 45 72 83 106 128 147 177 184 … .. .. ..@ Dim : int [1:2] 1004 15000 .. .. ..@ Dimnames:List of 2 .. .. .. ..$ : NULL .. .. .. ..$ : NULL .. .. ..@ factors : list() ..@ itemInfo :‘data.frame’: 1004 obs. of 1 variable: .. ..$ labels: chr [1:1004] “…and you will know us by the trail of dead” “[unknown]” “2pac” “3 doors down” … ..@ itemsetInfo:‘data.frame’: 15000 obs. of 1 variable: .. ..$ transactionID: chr [1:15000] “1” “3” “4” “5” … write(head(lista.musica.por.usuario1)) “dropkick murphys” “edguy” “eluveitie” “goldfrapp” “guano apes” “jack johnson” “john mayer” “judas priest” “le tigre” “red hot chili peppers” “rob zombie” “schandmaul” “the black dahlia murder” “the killers” “the rolling stones” “the who” “aesop rock” “air” “amon tobin” “animal collective” “aphex twin” “arcade fire” “atmosphere” “autechre” “beastie boys” “boards of canada” “broken social scene” “cocorosie” “devendra banhart” “four tet” “goldfrapp” “joanna newsom” “m83” “massive attack” “max richter” “mf doom” “neutral milk hotel” “pavement” “plaid” “portishead” “prefuse 73” “radiohead” “sage francis” “the books” “the flashbulb” “a tribe called quest” “air” “battles” “beck” “bon iver” “bonobo” “dj shadow” “fleetwood mac” “flight of the conchords” “kyuss” “late of the pier” “led zeppelin” “mgmt” “michael jackson” “muse” “pink floyd” “rjd2” “röyksopp” “simian mobile disco” “snow patrol” “the cinematic orchestra” “the decemberists” “the flaming lips” “the prodigy” “the rolling stones” “tool” “tv on the radio” “ac/dc” “bob marley &amp; the wailers” “children of bodom” “dream theater” “iron maiden” “megadeth” “metallica” “nightwish” “sublime” “trivium” “volbeat” “depeche mode” “dream theater” “faith no more” “green day” “iron maiden” “jay-z” “justin timberlake” “kanye west” “lily allen” “manu chao” “metallica” “muse” “pearl jam” “pink floyd” “queen” “sigur rós” “snow patrol” “stevie wonder” “tenacious d” “the streets” “thievery corporation” “type o negative” “u2” “ac/dc” “aerosmith” “alice in chains” “audioslave” “buckethead” “camel” “disturbed” “dream theater” “jethro tull” “king crimson” “led zeppelin” “oasis” “pearl jam” “pink floyd” “porcupine tree” “rammstein” “rush” “soundgarden” “stone temple pilots” “the verve” “tool” “type o negative” write(head(lista.musica.por.usuario1),format=&quot;single&quot;) “1” “dropkick murphys” “1” “edguy” “1” “eluveitie” “1” “goldfrapp” “1” “guano apes” “1” “jack johnson” “1” “john mayer” “1” “judas priest” “1” “le tigre” “1” “red hot chili peppers” “1” “rob zombie” “1” “schandmaul” “1” “the black dahlia murder” “1” “the killers” “1” “the rolling stones” “1” “the who” “3” “aesop rock” “3” “air” “3” “amon tobin” “3” “animal collective” “3” “aphex twin” “3” “arcade fire” “3” “atmosphere” “3” “autechre” “3” “beastie boys” “3” “boards of canada” “3” “broken social scene” “3” “cocorosie” “3” “devendra banhart” “3” “four tet” “3” “goldfrapp” “3” “joanna newsom” “3” “m83” “3” “massive attack” “3” “max richter” “3” “mf doom” “3” “neutral milk hotel” “3” “pavement” “3” “plaid” “3” “portishead” “3” “prefuse 73” “3” “radiohead” “3” “sage francis” “3” “the books” “3” “the flashbulb” “4” “a tribe called quest” “4” “air” “4” “battles” “4” “beck” “4” “bon iver” “4” “bonobo” “4” “dj shadow” “4” “fleetwood mac” “4” “flight of the conchords” “4” “kyuss” “4” “late of the pier” “4” “led zeppelin” “4” “mgmt” “4” “michael jackson” “4” “muse” “4” “pink floyd” “4” “rjd2” “4” “röyksopp” “4” “simian mobile disco” “4” “snow patrol” “4” “the cinematic orchestra” “4” “the decemberists” “4” “the flaming lips” “4” “the prodigy” “4” “the rolling stones” “4” “tool” “4” “tv on the radio” “5” “ac/dc” “5” “bob marley &amp; the wailers” “5” “children of bodom” “5” “dream theater” “5” “iron maiden” “5” “megadeth” “5” “metallica” “5” “nightwish” “5” “sublime” “5” “trivium” “5” “volbeat” “6” “depeche mode” “6” “dream theater” “6” “faith no more” “6” “green day” “6” “iron maiden” “6” “jay-z” “6” “justin timberlake” “6” “kanye west” “6” “lily allen” “6” “manu chao” “6” “metallica” “6” “muse” “6” “pearl jam” “6” “pink floyd” “6” “queen” “6” “sigur rós” “6” “snow patrol” “6” “stevie wonder” “6” “tenacious d” “6” “the streets” “6” “thievery corporation” “6” “type o negative” “6” “u2” “7” “ac/dc” “7” “aerosmith” “7” “alice in chains” “7” “audioslave” “7” “buckethead” “7” “camel” “7” “disturbed” “7” “dream theater” “7” “jethro tull” “7” “king crimson” “7” “led zeppelin” “7” “oasis” “7” “pearl jam” “7” “pink floyd” “7” “porcupine tree” “7” “rammstein” “7” “rush” “7” “soundgarden” “7” “stone temple pilots” “7” “the verve” “7” “tool” “7” “type o negative” Es una lista de transacciones - clase de datos definida en arules. Calculamos la frecuencia relativa de las canciones escuchadas: itfreq1 &lt;-itemFrequency(lista.musica.por.usuario1) head(itfreq1) …and you will know us by the trail of dead 0.009800000 [unknown] 0.036866667 2pac 0.022733333 3 doors down 0.030933333 30 seconds to mars 0.032800000 311 0.008333333 itfreq1: es una vector numérico los nombres de la lista (names(itfreq), los nombres de cada grupo ) cada posición por tanto es la frecuencia del grupo de esa posición Dibujar las frecuencias usando la lista de transacciones obtenida: itemFrequencyPlot(lista.musica.por.usuario1,support=.08,cex.names=1) Y obtenemos las reglas de asociación con soporte 0.1 y confianza 0.5: reglas2 &lt;- apriori(lista.musica.por.usuario1,parameter= list(support=.01, confidence=.5)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.5 0.1 1 none FALSE TRUE 5 0.01 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 150 set item appearances …[0 item(s)] done [0.00s]. set transactions …[1004 item(s), 15000 transaction(s)] done [0.04s]. sorting and recoding items … [655 item(s)] done [0.00s]. creating transaction tree … done [0.00s]. checking subsets of size 1 2 3 4 done [0.01s]. writing … [50 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. reglas2 set of 50 rules inspect(reglas2) lhs rhs support [1] {t.i.} =&gt; {kanye west} 0.01040000 [2] {the pussycat dolls} =&gt; {rihanna} 0.01040000 [3] {the fray} =&gt; {coldplay} 0.01126667 [4] {sonata arctica} =&gt; {nightwish} 0.01346667 [5] {judas priest} =&gt; {iron maiden} 0.01353333 [6] {the kinks} =&gt; {the beatles} 0.01360000 [7] {travis} =&gt; {coldplay} 0.01373333 [8] {the flaming lips} =&gt; {radiohead} 0.01306667 [9] {megadeth} =&gt; {metallica} 0.01626667 [10] {simon &amp; garfunkel} =&gt; {the beatles} 0.01540000 [11] {broken social scene} =&gt; {radiohead} 0.01506667 [12] {blur} =&gt; {radiohead} 0.01753333 [13] {keane} =&gt; {coldplay} 0.02226667 [14] {snow patrol} =&gt; {coldplay} 0.02646667 [15] {beck} =&gt; {radiohead} 0.02926667 [16] {snow patrol, the killers} =&gt; {coldplay} 0.01040000 [17] {radiohead, snow patrol} =&gt; {coldplay} 0.01006667 [18] {death cab for cutie, the shins} =&gt; {radiohead} 0.01006667 [19] {the beatles, the shins} =&gt; {radiohead} 0.01066667 [20] {led zeppelin, the doors} =&gt; {pink floyd} 0.01066667 [21] {pink floyd, the doors} =&gt; {led zeppelin} 0.01066667 [22] {pink floyd, the doors} =&gt; {the beatles} 0.01000000 [23] {the beatles, the strokes} =&gt; {radiohead} 0.01046667 [24] {oasis, the killers} =&gt; {coldplay} 0.01113333 [25] {oasis, the beatles} =&gt; {coldplay} 0.01060000 [26] {oasis, radiohead} =&gt; {coldplay} 0.01273333 [27] {beck, the beatles} =&gt; {radiohead} 0.01300000 [28] {bob dylan, the rolling stones} =&gt; {the beatles} 0.01146667 [29] {david bowie, the rolling stones} =&gt; {the beatles} 0.01000000 [30] {led zeppelin, the rolling stones} =&gt; {the beatles} 0.01066667 [31] {radiohead, the rolling stones} =&gt; {the beatles} 0.01060000 [32] {coldplay, the smashing pumpkins} =&gt; {radiohead} 0.01093333 [33] {the beatles, the smashing pumpkins} =&gt; {radiohead} 0.01146667 [34] {radiohead, u2} =&gt; {coldplay} 0.01140000 [35] {coldplay, sigur rós} =&gt; {radiohead} 0.01206667 [36] {sigur rós, the beatles} =&gt; {radiohead} 0.01046667 [37] {bob dylan, pink floyd} =&gt; {the beatles} 0.01033333 [38] {bob dylan, radiohead} =&gt; {the beatles} 0.01386667 [39] {bloc party, the killers} =&gt; {coldplay} 0.01106667 [40] {david bowie, pink floyd} =&gt; {the beatles} 0.01006667 [41] {david bowie, radiohead} =&gt; {the beatles} 0.01393333 [42] {placebo, radiohead} =&gt; {muse} 0.01366667 [43] {led zeppelin, radiohead} =&gt; {the beatles} 0.01306667 [44] {death cab for cutie, the killers} =&gt; {coldplay} 0.01086667 [45] {death cab for cutie, the beatles} =&gt; {radiohead} 0.01246667 [46] {muse, the killers} =&gt; {coldplay} 0.01513333 [47] {red hot chili peppers, the killers} =&gt; {coldplay} 0.01086667 [48] {the beatles, the killers} =&gt; {coldplay} 0.01253333 [49] {radiohead, the killers} =&gt; {coldplay} 0.01506667 [50] {muse, the beatles} =&gt; {radiohead} 0.01380000 confidence coverage lift count [1] 0.5672727 0.01833333 8.854413 156 [2] 0.5777778 0.01800000 13.415893 156 [3] 0.5168196 0.02180000 3.260006 169 [4] 0.5101010 0.02640000 8.236292 202 [5] 0.5075000 0.02666667 8.562992 203 [6] 0.5298701 0.02566667 2.979030 204 [7] 0.5628415 0.02440000 3.550304 206 [8] 0.5297297 0.02466667 2.938589 196 [9] 0.5281385 0.03080000 4.743759 244 [10] 0.5238095 0.02940000 2.944956 231 [11] 0.5472155 0.02753333 3.035589 226 [12] 0.5228628 0.03353333 2.900496 263 [13] 0.6374046 0.03493333 4.020634 334 [14] 0.5251323 0.05040000 3.312441 397 [15] 0.5092807 0.05746667 2.825152 439 [16] 0.5954198 0.01746667 3.755802 156 [17] 0.6344538 0.01586667 4.002021 151 [18] 0.5033333 0.02000000 2.792160 151 [19] 0.5673759 0.01880000 3.147425 160 [20] 0.5970149 0.01786667 5.689469 160 [21] 0.5387205 0.01980000 6.802027 160 [22] 0.5050505 0.01980000 2.839489 150 [23] 0.5607143 0.01866667 3.110471 157 [24] 0.6626984 0.01680000 4.180183 167 [25] 0.5196078 0.02040000 3.277594 159 [26] 0.5876923 0.02166667 3.707058 191 [27] 0.5909091 0.02200000 3.277972 195 [28] 0.5910653 0.01940000 3.323081 172 [29] 0.5703422 0.01753333 3.206572 150 [30] 0.5776173 0.01846667 3.247474 160 [31] 0.5638298 0.01880000 3.169958 159 [32] 0.6283525 0.01740000 3.485683 164 [33] 0.6209386 0.01846667 3.444556 172 [34] 0.5213415 0.02186667 3.288529 171 [35] 0.5801282 0.02080000 3.218167 181 [36] 0.6434426 0.01626667 3.569393 157 [37] 0.6150794 0.01680000 3.458092 155 [38] 0.5730028 0.02420000 3.221530 208 [39] 0.5236593 0.02113333 3.303150 166 [40] 0.5741445 0.01753333 3.227949 151 [41] 0.5225000 0.02666667 2.937594 209 [42] 0.5137845 0.02660000 4.504247 205 [43] 0.5283019 0.02473333 2.970213 196 [44] 0.5884477 0.01846667 3.711823 163 [45] 0.5013405 0.02486667 2.781105 187 [46] 0.5089686 0.02973333 3.210483 227 [47] 0.5093750 0.02133333 3.213047 163 [48] 0.5340909 0.02346667 3.368950 188 [49] 0.5243619 0.02873333 3.307582 226 [50] 0.5073529 0.02720000 2.814458 207 Primero nos quedamos con las reglas más interesantes. Filtramos aquellas con lift mayor que 1: inspect(subset(reglas2, subset=lift &gt; 1)) lhs rhs support [1] {t.i.} =&gt; {kanye west} 0.01040000 [2] {the pussycat dolls} =&gt; {rihanna} 0.01040000 [3] {the fray} =&gt; {coldplay} 0.01126667 [4] {sonata arctica} =&gt; {nightwish} 0.01346667 [5] {judas priest} =&gt; {iron maiden} 0.01353333 [6] {the kinks} =&gt; {the beatles} 0.01360000 [7] {travis} =&gt; {coldplay} 0.01373333 [8] {the flaming lips} =&gt; {radiohead} 0.01306667 [9] {megadeth} =&gt; {metallica} 0.01626667 [10] {simon &amp; garfunkel} =&gt; {the beatles} 0.01540000 [11] {broken social scene} =&gt; {radiohead} 0.01506667 [12] {blur} =&gt; {radiohead} 0.01753333 [13] {keane} =&gt; {coldplay} 0.02226667 [14] {snow patrol} =&gt; {coldplay} 0.02646667 [15] {beck} =&gt; {radiohead} 0.02926667 [16] {snow patrol, the killers} =&gt; {coldplay} 0.01040000 [17] {radiohead, snow patrol} =&gt; {coldplay} 0.01006667 [18] {death cab for cutie, the shins} =&gt; {radiohead} 0.01006667 [19] {the beatles, the shins} =&gt; {radiohead} 0.01066667 [20] {led zeppelin, the doors} =&gt; {pink floyd} 0.01066667 [21] {pink floyd, the doors} =&gt; {led zeppelin} 0.01066667 [22] {pink floyd, the doors} =&gt; {the beatles} 0.01000000 [23] {the beatles, the strokes} =&gt; {radiohead} 0.01046667 [24] {oasis, the killers} =&gt; {coldplay} 0.01113333 [25] {oasis, the beatles} =&gt; {coldplay} 0.01060000 [26] {oasis, radiohead} =&gt; {coldplay} 0.01273333 [27] {beck, the beatles} =&gt; {radiohead} 0.01300000 [28] {bob dylan, the rolling stones} =&gt; {the beatles} 0.01146667 [29] {david bowie, the rolling stones} =&gt; {the beatles} 0.01000000 [30] {led zeppelin, the rolling stones} =&gt; {the beatles} 0.01066667 [31] {radiohead, the rolling stones} =&gt; {the beatles} 0.01060000 [32] {coldplay, the smashing pumpkins} =&gt; {radiohead} 0.01093333 [33] {the beatles, the smashing pumpkins} =&gt; {radiohead} 0.01146667 [34] {radiohead, u2} =&gt; {coldplay} 0.01140000 [35] {coldplay, sigur rós} =&gt; {radiohead} 0.01206667 [36] {sigur rós, the beatles} =&gt; {radiohead} 0.01046667 [37] {bob dylan, pink floyd} =&gt; {the beatles} 0.01033333 [38] {bob dylan, radiohead} =&gt; {the beatles} 0.01386667 [39] {bloc party, the killers} =&gt; {coldplay} 0.01106667 [40] {david bowie, pink floyd} =&gt; {the beatles} 0.01006667 [41] {david bowie, radiohead} =&gt; {the beatles} 0.01393333 [42] {placebo, radiohead} =&gt; {muse} 0.01366667 [43] {led zeppelin, radiohead} =&gt; {the beatles} 0.01306667 [44] {death cab for cutie, the killers} =&gt; {coldplay} 0.01086667 [45] {death cab for cutie, the beatles} =&gt; {radiohead} 0.01246667 [46] {muse, the killers} =&gt; {coldplay} 0.01513333 [47] {red hot chili peppers, the killers} =&gt; {coldplay} 0.01086667 [48] {the beatles, the killers} =&gt; {coldplay} 0.01253333 [49] {radiohead, the killers} =&gt; {coldplay} 0.01506667 [50] {muse, the beatles} =&gt; {radiohead} 0.01380000 confidence coverage lift count [1] 0.5672727 0.01833333 8.854413 156 [2] 0.5777778 0.01800000 13.415893 156 [3] 0.5168196 0.02180000 3.260006 169 [4] 0.5101010 0.02640000 8.236292 202 [5] 0.5075000 0.02666667 8.562992 203 [6] 0.5298701 0.02566667 2.979030 204 [7] 0.5628415 0.02440000 3.550304 206 [8] 0.5297297 0.02466667 2.938589 196 [9] 0.5281385 0.03080000 4.743759 244 [10] 0.5238095 0.02940000 2.944956 231 [11] 0.5472155 0.02753333 3.035589 226 [12] 0.5228628 0.03353333 2.900496 263 [13] 0.6374046 0.03493333 4.020634 334 [14] 0.5251323 0.05040000 3.312441 397 [15] 0.5092807 0.05746667 2.825152 439 [16] 0.5954198 0.01746667 3.755802 156 [17] 0.6344538 0.01586667 4.002021 151 [18] 0.5033333 0.02000000 2.792160 151 [19] 0.5673759 0.01880000 3.147425 160 [20] 0.5970149 0.01786667 5.689469 160 [21] 0.5387205 0.01980000 6.802027 160 [22] 0.5050505 0.01980000 2.839489 150 [23] 0.5607143 0.01866667 3.110471 157 [24] 0.6626984 0.01680000 4.180183 167 [25] 0.5196078 0.02040000 3.277594 159 [26] 0.5876923 0.02166667 3.707058 191 [27] 0.5909091 0.02200000 3.277972 195 [28] 0.5910653 0.01940000 3.323081 172 [29] 0.5703422 0.01753333 3.206572 150 [30] 0.5776173 0.01846667 3.247474 160 [31] 0.5638298 0.01880000 3.169958 159 [32] 0.6283525 0.01740000 3.485683 164 [33] 0.6209386 0.01846667 3.444556 172 [34] 0.5213415 0.02186667 3.288529 171 [35] 0.5801282 0.02080000 3.218167 181 [36] 0.6434426 0.01626667 3.569393 157 [37] 0.6150794 0.01680000 3.458092 155 [38] 0.5730028 0.02420000 3.221530 208 [39] 0.5236593 0.02113333 3.303150 166 [40] 0.5741445 0.01753333 3.227949 151 [41] 0.5225000 0.02666667 2.937594 209 [42] 0.5137845 0.02660000 4.504247 205 [43] 0.5283019 0.02473333 2.970213 196 [44] 0.5884477 0.01846667 3.711823 163 [45] 0.5013405 0.02486667 2.781105 187 [46] 0.5089686 0.02973333 3.210483 227 [47] 0.5093750 0.02133333 3.213047 163 [48] 0.5340909 0.02346667 3.368950 188 [49] 0.5243619 0.02873333 3.307582 226 [50] 0.5073529 0.02720000 2.814458 207 Ordenamos por confianza estas reglas anteriores: inspect(sort(subset(reglas2, subset=lift &gt; 1), by=&quot;confidence&quot;)) lhs rhs support [1] {oasis, the killers} =&gt; {coldplay} 0.01113333 [2] {sigur rós, the beatles} =&gt; {radiohead} 0.01046667 [3] {keane} =&gt; {coldplay} 0.02226667 [4] {radiohead, snow patrol} =&gt; {coldplay} 0.01006667 [5] {coldplay, the smashing pumpkins} =&gt; {radiohead} 0.01093333 [6] {the beatles, the smashing pumpkins} =&gt; {radiohead} 0.01146667 [7] {bob dylan, pink floyd} =&gt; {the beatles} 0.01033333 [8] {led zeppelin, the doors} =&gt; {pink floyd} 0.01066667 [9] {snow patrol, the killers} =&gt; {coldplay} 0.01040000 [10] {bob dylan, the rolling stones} =&gt; {the beatles} 0.01146667 [11] {beck, the beatles} =&gt; {radiohead} 0.01300000 [12] {death cab for cutie, the killers} =&gt; {coldplay} 0.01086667 [13] {oasis, radiohead} =&gt; {coldplay} 0.01273333 [14] {coldplay, sigur rós} =&gt; {radiohead} 0.01206667 [15] {the pussycat dolls} =&gt; {rihanna} 0.01040000 [16] {led zeppelin, the rolling stones} =&gt; {the beatles} 0.01066667 [17] {david bowie, pink floyd} =&gt; {the beatles} 0.01006667 [18] {bob dylan, radiohead} =&gt; {the beatles} 0.01386667 [19] {david bowie, the rolling stones} =&gt; {the beatles} 0.01000000 [20] {the beatles, the shins} =&gt; {radiohead} 0.01066667 [21] {t.i.} =&gt; {kanye west} 0.01040000 [22] {radiohead, the rolling stones} =&gt; {the beatles} 0.01060000 [23] {travis} =&gt; {coldplay} 0.01373333 [24] {the beatles, the strokes} =&gt; {radiohead} 0.01046667 [25] {broken social scene} =&gt; {radiohead} 0.01506667 [26] {pink floyd, the doors} =&gt; {led zeppelin} 0.01066667 [27] {the beatles, the killers} =&gt; {coldplay} 0.01253333 [28] {the kinks} =&gt; {the beatles} 0.01360000 [29] {the flaming lips} =&gt; {radiohead} 0.01306667 [30] {led zeppelin, radiohead} =&gt; {the beatles} 0.01306667 [31] {megadeth} =&gt; {metallica} 0.01626667 [32] {snow patrol} =&gt; {coldplay} 0.02646667 [33] {radiohead, the killers} =&gt; {coldplay} 0.01506667 [34] {simon &amp; garfunkel} =&gt; {the beatles} 0.01540000 [35] {bloc party, the killers} =&gt; {coldplay} 0.01106667 [36] {blur} =&gt; {radiohead} 0.01753333 [37] {david bowie, radiohead} =&gt; {the beatles} 0.01393333 [38] {radiohead, u2} =&gt; {coldplay} 0.01140000 [39] {oasis, the beatles} =&gt; {coldplay} 0.01060000 [40] {the fray} =&gt; {coldplay} 0.01126667 [41] {placebo, radiohead} =&gt; {muse} 0.01366667 [42] {sonata arctica} =&gt; {nightwish} 0.01346667 [43] {red hot chili peppers, the killers} =&gt; {coldplay} 0.01086667 [44] {beck} =&gt; {radiohead} 0.02926667 [45] {muse, the killers} =&gt; {coldplay} 0.01513333 [46] {judas priest} =&gt; {iron maiden} 0.01353333 [47] {muse, the beatles} =&gt; {radiohead} 0.01380000 [48] {pink floyd, the doors} =&gt; {the beatles} 0.01000000 [49] {death cab for cutie, the shins} =&gt; {radiohead} 0.01006667 [50] {death cab for cutie, the beatles} =&gt; {radiohead} 0.01246667 confidence coverage lift count [1] 0.6626984 0.01680000 4.180183 167 [2] 0.6434426 0.01626667 3.569393 157 [3] 0.6374046 0.03493333 4.020634 334 [4] 0.6344538 0.01586667 4.002021 151 [5] 0.6283525 0.01740000 3.485683 164 [6] 0.6209386 0.01846667 3.444556 172 [7] 0.6150794 0.01680000 3.458092 155 [8] 0.5970149 0.01786667 5.689469 160 [9] 0.5954198 0.01746667 3.755802 156 [10] 0.5910653 0.01940000 3.323081 172 [11] 0.5909091 0.02200000 3.277972 195 [12] 0.5884477 0.01846667 3.711823 163 [13] 0.5876923 0.02166667 3.707058 191 [14] 0.5801282 0.02080000 3.218167 181 [15] 0.5777778 0.01800000 13.415893 156 [16] 0.5776173 0.01846667 3.247474 160 [17] 0.5741445 0.01753333 3.227949 151 [18] 0.5730028 0.02420000 3.221530 208 [19] 0.5703422 0.01753333 3.206572 150 [20] 0.5673759 0.01880000 3.147425 160 [21] 0.5672727 0.01833333 8.854413 156 [22] 0.5638298 0.01880000 3.169958 159 [23] 0.5628415 0.02440000 3.550304 206 [24] 0.5607143 0.01866667 3.110471 157 [25] 0.5472155 0.02753333 3.035589 226 [26] 0.5387205 0.01980000 6.802027 160 [27] 0.5340909 0.02346667 3.368950 188 [28] 0.5298701 0.02566667 2.979030 204 [29] 0.5297297 0.02466667 2.938589 196 [30] 0.5283019 0.02473333 2.970213 196 [31] 0.5281385 0.03080000 4.743759 244 [32] 0.5251323 0.05040000 3.312441 397 [33] 0.5243619 0.02873333 3.307582 226 [34] 0.5238095 0.02940000 2.944956 231 [35] 0.5236593 0.02113333 3.303150 166 [36] 0.5228628 0.03353333 2.900496 263 [37] 0.5225000 0.02666667 2.937594 209 [38] 0.5213415 0.02186667 3.288529 171 [39] 0.5196078 0.02040000 3.277594 159 [40] 0.5168196 0.02180000 3.260006 169 [41] 0.5137845 0.02660000 4.504247 205 [42] 0.5101010 0.02640000 8.236292 202 [43] 0.5093750 0.02133333 3.213047 163 [44] 0.5092807 0.05746667 2.825152 439 [45] 0.5089686 0.02973333 3.210483 227 [46] 0.5075000 0.02666667 8.562992 203 [47] 0.5073529 0.02720000 2.814458 207 [48] 0.5050505 0.01980000 2.839489 150 [49] 0.5033333 0.02000000 2.792160 151 [50] 0.5013405 0.02486667 2.781105 187 ¿Recomendación a usuarios que escuchan Coldplay?: r1 &lt;-subset(reglas2, subset = lhs %ain% c(&quot;coldplay&quot;)) inspect(r1) lhs rhs support confidence [1] {coldplay, the smashing pumpkins} =&gt; {radiohead} 0.01093333 0.6283525 [2] {coldplay, sigur rós} =&gt; {radiohead} 0.01206667 0.5801282 coverage lift count [1] 0.0174 3.485683 164 [2] 0.0208 3.218167 181 Probad con otros grupos. "],["formal-concept-analysis.html", "Capítulo 9 Formal Concept Analysis 9.1 Background in FCA 9.2 Working with Formal Contexts - datasets 9.3 Concept Lattice 9.4 Exercises 9.5 Implications in FCA 9.6 Exercise 9.7 Simplification Logic for Mushroom Dataset”", " Capítulo 9 Formal Concept Analysis Port-Royal logic (traditional logic): formal notion of concept, Arnauld A., Nicole P.: La logique ou l’art de penser, 1662 (Logic Or The Art Of Thinking, CUP, 2003): concept = extent (objects) + intent (attributes) G. Birkhoff (1940s): work on lattices and related mathematical structures, emphasizes applicational aspects of lattices in data analysis. Barbut M., Monjardet B.: Ordre et classiffication, algebre et combinatoire. Hachette, Paris, 1970. Wille R.: Restructuring lattice theory: an approach based on hierarchies of concepts. In: I. Rival (Ed.): Ordered Sets. Reidel, Dordrecht, 1982, pp. 445-470. Ganter B., Wille R.: Formal Concept Analysis. Springer, 1999. Application of FCA: knowledge extraction clustering and classification machine learning concepts, ontologies rules, association rules, attribute implications 9.1 Background in FCA FCA provides methods to describe the relationship between a set of objects \\(G\\) and a set of attributes \\(M\\). We show the main methods of FCA using the main functionalities and data structures of the fcaR package. We load the fcaR package by: Formal Context, \\(\\mathbf{ K} := (G, M, I)\\) objects &lt;- c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;, &quot;Pluto&quot;) attributes &lt;- c(&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;, &quot;near&quot;, &quot;far&quot;, &quot;moon&quot;, &quot;no_moon&quot;) planets &lt;- matrix(0, nrow = length(objects), ncol = length(attributes)) rownames(planets) &lt;- objects colnames(planets) &lt;- attributes planets[&quot;Mercury&quot;, c(&quot;small&quot;, &quot;near&quot;, &quot;no_moon&quot;)] &lt;- 1 planets[&quot;Venus&quot;, c(&quot;small&quot;, &quot;near&quot;, &quot;no_moon&quot;)] &lt;- 1 planets[&quot;Earth&quot;, c(&quot;small&quot;, &quot;near&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Mars&quot;, c(&quot;small&quot;, &quot;near&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Jupiter&quot;, c(&quot;large&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Saturn&quot;, c(&quot;large&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Uranus&quot;, c(&quot;medium&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Neptune&quot;, c(&quot;medium&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Pluto&quot;, c(&quot;small&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 fc_planets &lt;- FormalContext$new(planets) knitr::kable(planets, format = &quot;html&quot;, booktabs = TRUE) small medium large near far moon no_moon Mercury 1 0 0 1 0 0 1 Venus 1 0 0 1 0 0 1 Earth 1 0 0 1 0 1 0 Mars 1 0 0 1 0 1 0 Jupiter 0 0 1 0 1 1 0 Saturn 0 0 1 0 1 1 0 Uranus 0 1 0 0 1 1 0 Neptune 0 1 0 0 1 1 0 Pluto 1 0 0 0 1 1 0 Two mappings can be defined: intent: \\((\\ )&#39;\\colon 2^G \\to 2^M\\) with, for all \\(A\\subseteq G\\), \\(A&#39; = \\{m \\in M \\mid g\\, I\\, m \\mbox{ for all } g \\in A\\}\\) for all \\(A\\subseteq G\\). extent: \\((\\ )&#39;\\colon 2^M \\to 2^G\\) with, for all \\(B\\subseteq M\\), \\(B&#39; = \\{g \\in G \\mid g\\, I\\, m \\mbox{ for all } m \\in B\\}\\). That is, the intent of a set of objects is the set of their common attributes: # Define a set of objects S &lt;- Set$new(attributes = fc_planets$objects) S$assign(Earth = 1, Mars = 1) cat(&quot;Given the set of objects:&quot;) Given the set of objects: S {Earth, Mars} cat(&quot;The intent is:&quot;) The intent is: # Compute the intent of S fc_planets$intent(S) {small, near, moon} Analogously, the extent of a set of attributes is the set of objects which possess all the attributes in the given set: # Define a set of objects S &lt;- Set$new(attributes = fc_planets$attributes) S$assign(moon = 1, large = 1) cat(&quot;Given the set of attributes:&quot;) Given the set of attributes: S {large, moon} cat(&quot;The extent is:&quot;) The extent is: # Compute the extent of S fc_planets$extent(S) {Jupiter, Saturn} This pair of mappings is a Galois connection. The composition of intent and extent is the closure of a set of attributes: # Compute the closure of S print(&quot;El conjunto de objetos &quot;) [1] “El conjunto de objetos” S {large, moon} print(&quot;tiene como cerrado&quot;) [1] “tiene como cerrado” Sc &lt;- fc_planets$closure(S) Sc {large, far, moon} This means that all planets which have the attributes moon and large also have far in common. Definition: A formal concept is a pair \\((A,B)\\) such that \\(A \\subseteq G\\), \\(B \\subseteq M\\), \\(A&#39; = B\\) and \\(B&#39; = A\\). Consequently, \\(A\\) and \\(B\\) are closed sets of objects and attributes, respectively. # Define a set of objects S &lt;- Set$new(attributes = fc_planets$attributes) S$assign(moon = 1, large = 1, far= 1) print(&quot;Given the set of attributes:&quot;) [1] “Given the set of attributes:” S {large, far, moon} print(&quot;The extent is:&quot;) [1] “The extent is:” # Compute the extent of S extent &lt;- fc_planets$extent(S) extent {Jupiter, Saturn} print(&quot;And the intent of this one is:&quot;) [1] “And the intent of this one is:” fc_planets$intent(extent) {large, far, moon} \\(\\big(\\{Jupiter, Saturn\\},\\{large, far, moon\\}\\big)\\) is a concept. It is a maximal cluster. 9.1.1 Datasets We are going to work with two datasets, a crisp one and a fuzzy one. The classical (binary) dataset is the well-known planets formal context, presented in Wille R (1982). “Restructuring Lattice Theory: An Approach Based on Hierarchies of Concepts.” In Ordered Sets, pp. 445–470. Springer. small medium large near far moon no_moon Mercury 1 0 0 1 0 0 1 Venus 1 0 0 1 0 0 1 Earth 1 0 0 1 0 1 0 Mars 1 0 0 1 0 1 0 Jupiter 0 0 1 0 1 1 0 Saturn 0 0 1 0 1 1 0 Uranus 0 1 0 0 1 1 0 Neptune 0 1 0 0 1 1 0 Pluto 1 0 0 0 1 1 0 The other formal context is fuzzy and is defined by the following matrix I: P1 P2 P3 P4 P5 P6 O1 0.0 0.0 0.5 0.5 1.0 0 O2 1.0 1.0 1.0 0.0 0.0 0 O3 0.5 0.5 0.0 0.0 0.0 1 O4 0.0 0.0 0.0 1.0 0.5 0 O5 0.0 0.0 1.0 0.5 0.0 0 O6 0.5 0.5 0.0 0.0 0.0 1 9.2 Working with Formal Contexts - datasets The first step when using the fcaR package to analyse a formal context is to create an object of class FormalContext which will store all the information related to the context. In our examples, we create two objects: Internally, the object stores information about whether the context is binary or the names of objects and attributes, which are taken from the rownames and colnames of the provided matrix. 9.2.1 Plotting, printing and latex-ing the FormalContext Once created the FormalContext objects, we can print them or plot them as heatmaps (with functions print() and plot()): FormalContext with 9 objects and 7 attributes. small medium large near far moon no_moon Mercury X X X Venus X X X Earth X X X Mars X X X Jupiter X X X Saturn X X X Uranus X X X Neptune X X X Pluto X X X FormalContext with 6 objects and 6 attributes. P1 P2 P3 P4 P5 P6 O1 0 0 0.5 0.5 1 0 O2 1 1 1 0 0 0 O3 0.5 0.5 0 0 0 1 O4 0 0 0 1 0.5 0 O5 0 0 1 0.5 0 0 O6 0.5 0.5 0 0 0 1 Also, we can export the formal context as a LaTeX table: 9.2.2 Closures The basic operation in FCA is the computation of closures given an attribute set, by using the two derivation operators, extent and intent. The intent of a (probably fuzzy) set of objects is the set of their common attributes: {Earth, Mars} {small, near, moon} Analogously, the extent of a set of attributes is the set of objects which possess all the attributes in the given set: {large, moon} {Jupiter, Saturn} The composition of intent and extent is the closure of a set of attributes: {large, far, moon} This means that all planets which have the attributes moon and large also have far in common. We can check whether a set is closed (that is, it is equal to its closure), using is_closed(): [1] FALSE [1] TRUE 9.2.3 Clarification and Reduction An interesting point when managing formal contexts is the ability to reduce the context, removing redundancies, while retaining all the knowledge. This is accomplished by two functions: clarify(), which removes duplicated attributes and objects (columns and rows in the original matrix); and reduce(), which uses closures to remove dependent attributes, but only on binary formal contexts. The resulting FormalContext is equivalent to the original one in both cases. FormalContext with 5 objects and 7 attributes. small medium large near far moon no_moon Pluto X X X [Mercury, Venus] X X X [Earth, Mars] X X X [Jupiter, Saturn] X X X [Uranus, Neptune] X X X FormalContext with 5 objects and 5 attributes. P3 P4 P5 P6 [P1, P2] O1 0.5 0.5 1 0 0 O2 1 0 0 0 1 O4 0 1 0.5 0 0 O5 1 0.5 0 0 0 [O3, O6] 0 0 0 1 0.5 Note that merged attributes or objects are stored in the new formal context by using squared brackets to unify them, e.g. [Mercury, Venus]. 9.2.4 Extracting Implications and Concepts The function to extract the canonical basis of implications and the concept lattice is find_implications(). Its use is to store a ConceptLattice and an ImplicationSet objects internally in the FormalContext object after running the NextClosure algorithm. It can be used both for binary and fuzzy formal contexts, resulting in binary or fuzzy concepts and implications: We can inspect the results as: A set of 12 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 3: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 4: ({Jupiter, Saturn}, {large, far, moon}) 5: ({Uranus, Neptune}, {medium, far, moon}) 6: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 7: ({Earth, Mars, Pluto}, {small, moon}) 8: ({Pluto}, {small, far, moon}) 9: ({Mercury, Venus, Earth, Mars}, {small, near}) 10: ({Mercury, Venus}, {small, near, no_moon}) 11: ({Earth, Mars}, {small, near, moon}) 12: ({}, {small, medium, large, near, far, moon, no_moon}) Implication set with 10 implications. Rule 1: {no_moon} -&gt; {small, near} Rule 2: {far} -&gt; {moon} Rule 3: {near} -&gt; {small} Rule 4: {large} -&gt; {far, moon} Rule 5: {medium} -&gt; {far, moon} Rule 6: {medium, large, far, moon} -&gt; {small, near, no_moon} Rule 7: {small, near, moon, no_moon} -&gt; {medium, large, far} Rule 8: {small, near, far, moon} -&gt; {medium, large, no_moon} Rule 9: {small, large, far, moon} -&gt; {medium, near, no_moon} Rule 10: {small, medium, far, moon} -&gt; {large, near, no_moon} 9.2.5 Saving and loading A FormalContext is saved and loaded (in RDS format) using its own methods save() and load(), which are more efficient than the base saveRDS() and readRDS(). 9.3 Concept Lattice We are going to use the previously computed concept lattices for the two FormalContext objects. 9.3.1 Plot, print and LaTeX The concept lattice can be plotted using a Hasse diagram and the function plot() inside the ConceptLattice component: If one desires to get the list of concepts printed, or in \\(\\LaTeX\\) format, just: A set of 12 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 3: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 4: ({Jupiter, Saturn}, {large, far, moon}) 5: ({Uranus, Neptune}, {medium, far, moon}) 6: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 7: ({Earth, Mars, Pluto}, {small, moon}) 8: ({Pluto}, {small, far, moon}) 9: ({Mercury, Venus, Earth, Mars}, {small, near}) 10: ({Mercury, Venus}, {small, near, no_moon}) 11: ({Earth, Mars}, {small, near, moon}) 12: ({}, {small, medium, large, near, far, moon, no_moon}) 9.3.2 Getting all extents, intents and retrieving concepts For a ConceptLattice, one may want to retrieve particular concepts, using a subsetting as in R: A set of 2 concepts: 1: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 2: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) Or get all the extents and all the intents of all concepts, as sparse matrices: 9 x 12 sparse Matrix of class “dgCMatrix” [1,] 1 . . . . 1 . . 1 1 . . [2,] 1 . . . . 1 . . 1 1 . . [3,] 1 1 . . . 1 1 . 1 . 1 . [4,] 1 1 . . . 1 1 . 1 . 1 . [5,] 1 1 1 1 . . . . . . . . [6,] 1 1 1 1 . . . . . . . . [7,] 1 1 1 . 1 . . . . . . . [8,] 1 1 1 . 1 . . . . . . . [9,] 1 1 1 . . 1 1 1 . . . . 7 x 12 sparse Matrix of class “dgCMatrix” [1,] . . . . . 1 1 1 1 1 1 1 [2,] . . . . 1 . . . . . . 1 [3,] . . . 1 . . . . . . . 1 [4,] . . . . . . . . 1 1 1 1 [5,] . . 1 1 1 . . 1 . . . 1 [6,] . 1 1 1 1 . 1 1 . . 1 1 [7,] . . . . . . . . . 1 . 1 9.3.3 Concept support First, the support of an itemset is: \\[ supp(X)=\\frac{X^\\prime}{G} \\] The support of a concept $\\langle A, B\\rangle$ (A is the extent of the concept and B is the intent) is the cardinality (relative) of the extent - number of objects of the extent. The support of concepts can be computed using the function support(): [1] 1.0000000 0.7777778 0.5555556 0.2222222 0.2222222 0.5555556 0.3333333 [8] 0.1111111 0.4444444 0.2222222 0.2222222 0.0000000 The support of itemsets and concepts is used to mine all the knowledge: Algorithm Titanic (computing iceberg concept lattices) 9.3.4 Sublattices When the concept lattice is too large, it can be useful in certain occasions to just work with a sublattice of the complete lattice. To this end, we use the sublattice() function. For instance, to build the sublattice of those concepts with support greater than 0.5, we can do: A set of 13 concepts: 1: ({O1, O2, O3, O4, O5, O6}, {}) 2: ({O1, O4, O5}, {P4 [0.5]}) 3: ({O1, O4}, {P4 [0.5], P5 [0.5]}) 4: ({O1, O2, O5}, {P3 [0.5]}) 5: ({O1, O5}, {P3 [0.5], P4 [0.5]}) 6: ({O1}, {P3 [0.5], P4 [0.5], P5}) 7: ({O1 [0.5], O2, O5}, {P3}) 8: ({O1 [0.5], O5}, {P3, P4 [0.5]}) 9: ({O1 [0.5]}, {P3, P4, P5}) 10: ({O2, O3, O6}, {P1 [0.5], P2 [0.5]}) 11: ({O3, O6}, {P1 [0.5], P2 [0.5], P6}) 12: ({O2}, {P1, P2, P3}) 13: ({}, {P1, P2, P3, P4, P5, P6}) And we can plot just the sublattice: 9.3.5 Subconcepts, superconcepts, infimum and supremum It may be interesting to use the notions of subconcept and superconcept. Given a concept, we can compute all its subconcepts and all its superconcepts: A set of 1 concepts: 1: ({Uranus, Neptune}, {medium, far, moon}) A set of 2 concepts: 1: ({Uranus, Neptune}, {medium, far, moon}) 2: ({}, {small, medium, large, near, far, moon, no_moon}) A set of 4 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 3: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 4: ({Uranus, Neptune}, {medium, far, moon}) Also, we can define infimum and supremum of a set of concepts as the greatest common subconcept of all the given concepts, and the lowest common superconcept of them, and can be computed by: A set of 3 concepts: 1: ({Uranus, Neptune}, {medium, far, moon}) 2: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 3: ({Earth, Mars, Pluto}, {small, moon}) ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) ({}, {small, medium, large, near, far, moon, no_moon}) 9.3.6 Join- and meet- irreducible elements Theorem: In a complete lattice, an element is called supremum-irreducible or join-irreducible if it cannot be written as the supremum of other elements and infimum-irreducible or meet-irreducible if it can not be expressed as the infimum of other elements. The irreducible elements with respect to join (supremum) and meet (infimum) can be computed for a given concept lattice: A set of 5 concepts: 1: ({Jupiter, Saturn}, {large, far, moon}) 2: ({Uranus, Neptune}, {medium, far, moon}) 3: ({Pluto}, {small, far, moon}) 4: ({Mercury, Venus}, {small, near, no_moon}) 5: ({Earth, Mars}, {small, near, moon}) A set of 7 concepts: 1: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 2: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 3: ({Jupiter, Saturn}, {large, far, moon}) 4: ({Uranus, Neptune}, {medium, far, moon}) 5: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 6: ({Mercury, Venus, Earth, Mars}, {small, near}) 7: ({Mercury, Venus}, {small, near, no_moon}) This are the concepts used to build the standard context, mentioned above. 9.4 Exercises Compute the intent of Earth and Earth,Mars, Mercury (use the argument attributes in the class Set). {Mercury, Earth, Mars} Given the set of objects:{Earth} The intent is:{small, near, moon} {small, near} Compute the extent of large and far,large (use the argument attributes in the class Set) and save the result in a variable e1, e2. Given the set of objects:{large} {large, far} The extent is:{Jupiter, Saturn} {Jupiter, Saturn} Compute the intent of variables e1 and also of e2. {large, far, moon} {large, far, moon} With the information from the above questions tell me a concept. Check with any command of fcaR package. Compute the closure of no_moon {small, near, no_moon} ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) Compute all the concepts and plot them. How many are there? Show the fist and the last (use subsetting). A set of 2 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({}, {small, medium, large, near, far, moon, no_moon}) Compute the major concept (in lattice) that has moon. The same with no_moon. Locate both in the lattice to understand the meaning. ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) ({Mercury, Venus}, {small, near, no_moon}) Compute the minor concept (in lattice) that has Pluto The same with Earth. Locate both in the lattice to understand the meaning. ({Pluto}, {small, far, moon}) ({Earth, Mars}, {small, near, moon}) Compute the meet irreducible elements in the lattice. A set of 7 concepts: 1: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 2: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 3: ({Jupiter, Saturn}, {large, far, moon}) 4: ({Uranus, Neptune}, {medium, far, moon}) 5: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 6: ({Mercury, Venus, Earth, Mars}, {small, near}) 7: ({Mercury, Venus}, {small, near, no_moon}) Compute the sublattice of the concept in the irreducible elements A set of 12 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 3: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 4: ({Jupiter, Saturn}, {large, far, moon}) 5: ({Uranus, Neptune}, {medium, far, moon}) 6: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 7: ({Earth, Mars, Pluto}, {small, moon}) 8: ({Pluto}, {small, far, moon}) 9: ({Mercury, Venus, Earth, Mars}, {small, near}) 10: ({Mercury, Venus}, {small, near, no_moon}) 11: ({Earth, Mars}, {small, near, moon}) 12: ({}, {small, medium, large, near, far, moon, no_moon}) Compute the sublattice of the concept in the irreducible elements removing the first element in the list of irreducible elements. Plot this sublattice. A set of 9 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 3: ({Jupiter, Saturn}, {large, far, moon}) 4: ({Uranus, Neptune}, {medium, far, moon}) 5: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 6: ({Pluto}, {small, far, moon}) 7: ({Mercury, Venus, Earth, Mars}, {small, near}) 8: ({Mercury, Venus}, {small, near, no_moon}) 9: ({}, {small, medium, large, near, far, moon, no_moon}) Develop a function returning the index and also the labels in all the concepts (inside the formal context) having a vector with attributes. HOMEWORK. 9.5 Implications in FCA This is a summary of some of the functionalities introduced in package fcaR: Computing implications and concepts using Ganter’s algorithm. Visualization of the concept lattice. Removal of redundancies in implications. Computation of closures. 9.5.0.1 Data The datasets in this vignette come from this paper. 9.5.1 Crisp Version The crisp version of the data appears in Table 3 in the mentioned paper. 9.5.2 Computing Implications and Concepts Once we create the formal context object, with the previous data matrix I, we can compute all concepts and implications using Ganter’s algorithm: [1] 7 LHS RHS 2.142857 1.857143 The obtained implications are: Implication set with 7 implications. Rule 1: {P6} -&gt; {P1, P2} Rule 2: {P5} -&gt; {P4} Rule 3: {P3, P4, P5} -&gt; {P2} Rule 4: {P2, P4} -&gt; {P3, P5} Rule 5: {P1, P4} -&gt; {P2, P3, P5, P6} Rule 6: {P1, P3} -&gt; {P2} Rule 7: {P1, P2, P3, P6} -&gt; {P4, P5} 9.5.2.1 Visualization We provide functions to plot both the concept lattice and the formal context: 9.5.3 Redudancy Removal Let us apply some simplifcation rules: [1] 7 LHS RHS 1.714286 1.857143 The transformed ruleset: Implication set with 7 implications. Rule 1: {P6} -&gt; {P1, P2} Rule 2: {P5} -&gt; {P4} Rule 3: {P3, P5} -&gt; {P2} Rule 4: {P2, P4} -&gt; {P3, P5} Rule 5: {P1, P4} -&gt; {P2, P3, P5, P6} Rule 6: {P1, P3} -&gt; {P2} Rule 7: {P3, P6} -&gt; {P4, P5} 9.5.4 Fuzzy version The fuzzy version of the data appears in Table 2 in the mentioned paper. 9.5.4.1 Computing Implications and Concepts As before, we build the formal context object and compute all implications: [1] 12 LHS RHS 1.541667 1.916667 The extracted ruleset is: Implication set with 12 implications. Rule 1: {P6 [0.5]} -&gt; {P1 [0.5], P2 [0.5], P6} Rule 2: {P5 [0.5]} -&gt; {P4 [0.5]} Rule 3: {P3 [0.5], P4 [0.5], P5 [0.5]} -&gt; {P2, P5} Rule 4: {P3 [0.5], P4} -&gt; {P3} Rule 5: {P2 [0.5], P4 [0.5]} -&gt; {P2, P3 [0.5], P5} Rule 6: {P2 [0.5], P3 [0.5]} -&gt; {P2} Rule 7: {P2, P3, P4 [0.5], P5} -&gt; {P4} Rule 8: {P1 [0.5], P4 [0.5]} -&gt; {P1, P2, P3, P4, P5, P6} Rule 9: {P1 [0.5], P3 [0.5]} -&gt; {P1, P2, P3} Rule 10: {P1 [0.5], P2} -&gt; {P1} Rule 11: {P1, P2 [0.5]} -&gt; {P2} Rule 12: {P1, P2, P3, P6} -&gt; {P4, P5} 9.5.4.2 Visualization Let us plot the concept lattice and the formal context. 9.5.4.3 Redudancy Removal Let us apply some functions to remove redudancies in the set of implications: [1] 12 LHS RHS 1.541667 1.916667 [1] 12 LHS RHS 1.458333 1.916667 [1] 12 LHS RHS 1.458333 1.916667 The reduced ruleset is: Implication set with 12 implications. Rule 1: {P6 [0.5]} -&gt; {P1 [0.5], P2 [0.5], P6} Rule 2: {P5 [0.5]} -&gt; {P4 [0.5]} Rule 3: {P3 [0.5], P5 [0.5]} -&gt; {P2, P5} Rule 4: {P3 [0.5], P4} -&gt; {P3} Rule 5: {P2 [0.5], P4 [0.5]} -&gt; {P2, P3 [0.5], P5} Rule 6: {P2 [0.5], P3 [0.5]} -&gt; {P2} Rule 7: {P2, P3, P5} -&gt; {P4} Rule 8: {P1 [0.5], P4 [0.5]} -&gt; {P1, P2, P3, P4, P5, P6} Rule 9: {P1 [0.5], P3 [0.5]} -&gt; {P1, P2, P3} Rule 10: {P1 [0.5], P2} -&gt; {P1} Rule 11: {P1, P2 [0.5]} -&gt; {P2} Rule 12: {P1, P2, P3, P6} -&gt; {P4, P5} 9.5.4.4 Closures Let us show how to compute the closure of a set S with respect to the implication set. {P2 [0.5], P3 [0.5]} $closure {P2, P3 [0.5]} 9.6 Exercise From an implication extracted from a formal cotext return the string representig the implication: &gt; cadena &lt;- impl.as.character(Implication) &gt; cadena &gt; &quot;a,b -&gt; c,d&quot; Use the function: impl.as.character &lt;- function(Implication ){ xxxx return{cadena} } From a string add the implication represented in the string to an implicational set: cadena &lt;- &quot;a,b -&gt; c,d&quot; implicationsNew &lt;- add_implication(cadena,Implications) Use the function: add_implication &lt;- function(stringImplication,ImplicationSet){ xxxx return{ImplicationSetNew} } 9.7 Simplification Logic for Mushroom Dataset” This is a simple example of some of the functionalities introduced in package fcaR: Import from/export to arules format. Removal of redundancies in implications. Computation of closures. 9.7.1 Data In this example, we’ll use the well-known Mushroom dataset, from the arules package. We’ll use the a priori algorithm to extract a large number of implications from the dataset. Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 1 0.1 1 none FALSE TRUE 5 0.1 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 812 set item appearances …[0 item(s)] done [0.00s]. set transactions …[114 item(s), 8124 transaction(s)] done [0.01s]. sorting and recoding items … [53 item(s)] done [0.00s]. creating transaction tree … done [0.00s]. checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [2.03s]. writing … [1799427 rule(s)] done [0.15s]. creating S4 object … done [0.68s]. The number of implications extracted by the algorithm (with this configuration) is length(mush) = 1799427. 9.7.2 Preprocessing Next step is to remove the redundant rules in the implications set. To this end, let us use the is.redundant() function in arules. user system elapsed 4.588 0.189 4.803 After this, the number of implications is length(mush_clean) = 2002, that is, just a 0.111 percent of the original ruleset. 9.7.3 Importing in fcaR To use the functionalities in this package, one must import all objects (the formal context and implications above) into our data model. This is accomplished by just typing: We can check some of the properties of the ruleset: [1] 2002 LHS RHS 3.038462 1.000000 9.7.4 Applying Rules We can improve the redudancy removal performed by arules, using some simplification rules. Currently, the following rules are implemented: Composition. Generalization. Simplification. R-Simplification We can apply them one by one, just to see their effect, or we could use them sequentially. [1] 961 LHS RHS 3.122789 2.083247 [1] 961 LHS RHS 3.122789 1.072841 Also, we can compute the support of each implication: [1] 1.0000000 0.1024126 0.1280158 0.1290005 0.1319547 0.1378631 9.7.5 Computing Closure Given a fuzzy set represented by a sparse matrix, we can compute its closure with respect to the implications in the ruleset. $closure {CapColor=white, GillAttached=free, ColorAboveRing=white, ColorBelowRing=white, VeilType=partial, VeilColor=white} 9.7.6 Re-Exporting to arules After this phase of redudancy removal, we can export the obtained ruleset to arules format. set of 961 rules [1] “rules” attr(,“package”) [1] “arules” "],["social-network-analysis.html", "Capítulo 10 Social Network Analysis 10.1 R for SNA 10.2 igraph 10.3 Exportar grafos 10.4 De Twitter 10.5 Medidas de bondad, calidad 10.6 Proyecto SNA - USairports 10.7 Proyecto - películas + actores", " Capítulo 10 Social Network Analysis Disciplina con base sólida de Matemática Aplicada: Teoría de Grafos y Matemática Discreta. Unida con Álgebra Lineal: las bases de Pagerank (algoritmo de Google) The Mathematics of Google Search. Describir las relaciones entre los elementos de una red y extraer conocimiento acerca de las estructuras sociales que existen en esa red. Tópico de enorme interés para extraer conocimiento de redes sociales en cualquier área. en una red los actores no intervienen aislados decribir todos los actores intervinientes en las redes redes de alta complejidad Existen muy destacadas aplicaciones para SNA: Gephi https://gephi.org https://gephi.org/tutorials/gephi-tutorial-quick_start.pdf Exploratory Data Analysis: intuition-oriented analysis by networks manipulations in real time. Link Analysis: revealing the underlying structures of associations between objects. Social Network Analysis: easy creation of social data connectors to map community organizations and small-world networks. Biological Network analysis: representing patterns of biological data. Cytoscape https://cytoscape.org Cytoscape is an open source software platform for visualizing molecular interaction networks and biological pathways and integrating these networks with annotations, gene expression profiles and other state data. Library for visualization 10.1 R for SNA Usaremos el paquete igraph. Nos servirá para analizar más adelante datos extraído de Twiter. Ventajas de usar R: Reproducible research no es posible con las aplicaciones GUI. Herramientas sólidas para manipular los datos. Cada vez más paquetes diseñados para hacer de R una herramienta completa de análisis de redes. Paquetes statnet y igraph. Thomas Lin Pedersen ha publicado los paquetes tidygraph y ggraph, que aprovechan la potencia de igraph de forma coherente con el flujo de trabajo de tidyverse. Crear gráficos de red interactivos con el marco htmlwidgets que traduce el código de R a JavaScript. 10.1.1 Elementos de una red nodos o vértices de grafo (nodes, vertices) [1] “Tom Hanks” “Gary Sinise” “Bill Paxton” “Kevin Bacon” “Ed Harris” [6] “Sean Connery” “Robin Wright” “Nicolas Cage” arcos o enlaces (edges, links) [,1] [,2] [1,] “Tom Hanks” “Gary Sinise” [2,] “Tom Hanks” “Robin Wright” [3,] “Gary Sinise” “Robin Wright” [4,] “Tom Hanks” “Gary Sinise” [5,] “Tom Hanks” “Bill Paxton” [6,] “Tom Hanks” “Kevin Bacon” [7,] “Tom Hanks” “Ed Harris” [8,] “Gary Sinise” “Bill Paxton” [9,] “Gary Sinise” “Kevin Bacon” [10,] “Gary Sinise” “Ed Harris” [11,] “Bill Paxton” “Kevin Bacon” [12,] “Bill Paxton” “Ed Harris” [13,] “Kevin Bacon” “Ed Harris” [14,] “Ed Harris” “Sean Connery” [15,] “Ed Harris” “Nicolas Cage” [16,] “Sean Connery” “Nicolas Cage” Nodos y arcos pueden contender atributos adicionales con importante información: [1] “Forest Gump” “Forest Gump” “Forest Gump” “Apollo 13” “Apollo 13” [6] “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” [11] “Apollo 13” “Apollo 13” “Apollo 13” “The Rock” “The Rock” [16] “The Rock” 10.1.2 Representación de redes 10.1.2.1 Grafos como listas de arcos data.frame o matriz (si los datos del mismo tipo) que contiene dos columnas: primera columna: nodos que son el origen de una conexión segunda columna: nodos que son el destino de la conexión Si el sentido es importante, la red se denomina dirigida, en otro caso, no dirigida. alumnos1 &lt;- c(&quot;Luis&quot;, &quot;Ana&quot;, &quot;Fran&quot;, &quot;Pedro&quot;, &quot;Laura&quot;, &quot;Susana&quot;) alumnos2 &lt;- c(&quot;Juan&quot;, &quot;Jose&quot;, &quot;Amalia&quot;, &quot;Lucía&quot;, &quot;Maite&quot;, &quot;Eduardo&quot;) grupos &lt;- data.frame(integrante1 = alumnos1, integrante2 = alumnos2, stringsAsFactors = F) print(grupos) integrante1 integrante2 1 Luis Juan 2 Ana Jose 3 Fran Amalia 4 Pedro Lucía 5 Laura Maite 6 Susana Eduardo str(grupos) ‘data.frame’: 6 obs. of 2 variables: $ integrante1: chr “Luis” “Ana” “Fran” “Pedro” … $ integrante2: chr “Juan” “Jose” “Amalia” “Lucía” … 10.1.2.2 Grafos como matrices # Se pueden usar matrices &#39;sparse&#39; A &lt;- rbind(c(0,1,0), c(1,0,1), c(1,0,0)) nodeNames &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) dimnames(A) &lt;- list(nodeNames, nodeNames) A A B C A 0 1 0 B 1 0 1 C 1 0 0 str(A) num [1:3, 1:3] 0 1 1 1 0 0 0 1 0 - attr(*, “dimnames”)=List of 2 ..$ : chr [1:3] “A” “B” “C” ..$ : chr [1:3] “A” “B” “C” Caminos de longitud dos, tres, etc: # Multiplicación matricial A2 &lt;- A %*% A A2 A B C A 1 0 1 B 1 1 0 C 0 1 0 A3 &lt;- A %*% A %*% A A3 A B C A 1 1 0 B 1 1 1 C 1 0 1 Representado arcos: Arcos &lt;- rbind(c(&quot;A&quot;,&quot;B&quot;), c(&quot;B&quot;,&quot;A&quot;), c(&quot;B&quot;,&quot;C&quot;), c(&quot;C&quot;,&quot;A&quot;)) Arcos [,1] [,2] [1,] “A” “B” [2,] “B” “A” [3,] “B” “C” [4,] “C” “A” 10.2 igraph # Instalar la primera vez - descomentar #install.packages(&quot;igraph&quot;) #install.packages(&quot;igraphdata&quot;) library(igraph) library(igraphdata) # Importar la red de datasets ya establecidos: igraphdata # Limpia la memoria - Cuidado - borra todas las variables rm(list=ls()) #Lista de datasets de redes de nodos en igraph # data(package=&quot;igraphdata&quot;) # El paquete tiene un conjunto de datasets # Carga data set y vemos que contiene #Red social entre miembros de club de karate de universidad data(karate,package=&quot;igraphdata&quot;) plot(karate) 10.2.1 Acceder a elementos de grado IGRAPH 6f42903 D-W- 81 817 – + attr: Type (g/c), Date (g/c), Citation (g/c), Author (g/c), Group | (v/n), weight (e/n) + edges from 6f42903: [1] 57-&gt;52 76-&gt;42 12-&gt;69 43-&gt;34 28-&gt;47 58-&gt;51 7-&gt;29 40-&gt;71 5-&gt;37 48-&gt;55 [11] 6-&gt;58 21-&gt; 8 28-&gt;69 43-&gt;21 67-&gt;58 65-&gt;42 5-&gt;67 52-&gt;75 37-&gt;64 4-&gt;36 [21] 12-&gt;49 19-&gt;46 37-&gt; 9 74-&gt;36 62-&gt; 1 15-&gt; 2 72-&gt;49 46-&gt;62 2-&gt;29 40-&gt;12 [31] 22-&gt;29 71-&gt;69 4-&gt; 3 37-&gt;69 5-&gt; 6 77-&gt;13 23-&gt;49 52-&gt;35 20-&gt;14 62-&gt;70 [41] 34-&gt;35 76-&gt;72 7-&gt;42 37-&gt;42 51-&gt;80 38-&gt;45 62-&gt;64 36-&gt;53 62-&gt;77 17-&gt;61 [51] 7-&gt;68 46-&gt;29 44-&gt;53 18-&gt;58 12-&gt;16 72-&gt;42 52-&gt;32 58-&gt;21 38-&gt;17 15-&gt;51 [61] 22-&gt; 7 22-&gt;69 5-&gt;13 29-&gt; 2 77-&gt;12 37-&gt;35 18-&gt;46 10-&gt;71 22-&gt;47 20-&gt;19 + … omitted several edges + 81/81 vertices, from 6f42903: [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 [76] 76 77 78 79 80 81 + 817/817 edges from 6f42903: [1] 57-&gt;52 76-&gt;42 12-&gt;69 43-&gt;34 28-&gt;47 58-&gt;51 7-&gt;29 40-&gt;71 5-&gt;37 48-&gt;55 [11] 6-&gt;58 21-&gt; 8 28-&gt;69 43-&gt;21 67-&gt;58 65-&gt;42 5-&gt;67 52-&gt;75 37-&gt;64 4-&gt;36 [21] 12-&gt;49 19-&gt;46 37-&gt; 9 74-&gt;36 62-&gt; 1 15-&gt; 2 72-&gt;49 46-&gt;62 2-&gt;29 40-&gt;12 [31] 22-&gt;29 71-&gt;69 4-&gt; 3 37-&gt;69 5-&gt; 6 77-&gt;13 23-&gt;49 52-&gt;35 20-&gt;14 62-&gt;70 [41] 34-&gt;35 76-&gt;72 7-&gt;42 37-&gt;42 51-&gt;80 38-&gt;45 62-&gt;64 36-&gt;53 62-&gt;77 17-&gt;61 [51] 7-&gt;68 46-&gt;29 44-&gt;53 18-&gt;58 12-&gt;16 72-&gt;42 52-&gt;32 58-&gt;21 38-&gt;17 15-&gt;51 [61] 22-&gt; 7 22-&gt;69 5-&gt;13 29-&gt; 2 77-&gt;12 37-&gt;35 18-&gt;46 10-&gt;71 22-&gt;47 20-&gt;19 [71] 19-&gt;31 68-&gt;13 49-&gt;69 30-&gt;63 5-&gt;49 53-&gt;75 62-&gt;57 73-&gt;81 29-&gt;69 71-&gt;40 [81] 19-&gt;58 49-&gt;42 37-&gt; 5 18-&gt; 2 20-&gt;80 75-&gt;53 15-&gt;54 76-&gt;58 40-&gt;23 5-&gt;12 [91] 20-&gt;54 6-&gt;47 51-&gt;14 78-&gt; 4 52-&gt;49 29-&gt;55 27-&gt;35 66-&gt; 6 21-&gt;29 4-&gt;61 + … omitted several edges + 81/81 vertices, from 6f42903: [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 [76] 76 77 78 79 80 81 Class ‘igraph’ hidden list of 10 $ : num 81 $ : logi TRUE $ : num [1:817] 56 75 11 42 27 57 6 39 4 47 … $ : num [1:817] 51 41 68 33 46 50 28 70 36 54 … $ : num [1:817] 580 411 719 376 569 215 533 620 527 592 … $ : num [1:817] 241 433 238 352 258 274 115 24 263 25 … $ : num [1:82] 0 6 23 27 37 65 74 91 93 101 … $ : num [1:82] 0 9 28 32 40 50 58 76 82 87 … $ :List of 4 ..$ : num [1:3] 1 0 1 ..$ :List of 4 .. ..$ Type : chr “TSPE” .. ..$ Date : chr “Mon Mar 19 21:56:02 2007” .. ..$ Citation: chr “Nepusz T., Petroczi A., Negyessy L., Bazso F.: Fuzzy communities and the concept of bridgeness in complex netwo”| truncated .. ..$ Author : chr “Nepusz T., Petroczi A., Negyessy L., Bazso F.” ..$ :List of 1 .. ..$ Group: num [1:81] 3 1 3 3 2 2 2 1 3 2 … ..$ :List of 1 .. ..$ weight: num [1:817] 4 14 4 4 10 2 6 2 4 4 … $ :&lt;environment: 0x7fabc388aba8&gt; [1] 4 14 4 4 10 2 [1] 3 1 3 3 2 2 10.2.2 Construir/Modificar un grafo Añadir arcos a un grafo vacío: # Un grafo dirigido vacío g &lt;- make_empty_graph(n = 0, directed = TRUE) g IGRAPH bc5ae5b D— 0 0 – + edges from bc5ae5b: g &lt;- g + vertices(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;)) g IGRAPH e49fa06 DN– 3 0 – + attr: name (v/c) + edges from e49fa06 (vertex names): # Arcos: A to C , B to C g &lt;- g + edges(c(&quot;A&quot;,&quot;C&quot;, &quot;B&quot;,&quot;C&quot;)) g IGRAPH fa17c69 DN– 3 2 – + attr: name (v/c) + edges from fa17c69 (vertex names): [1] A-&gt;C B-&gt;C # Eliminar arco A g &lt;- g - V(g)[&quot;A&quot;] g IGRAPH d883f6e DN– 2 1 – + attr: name (v/c) + edge from d883f6e (vertex names): [1] B-&gt;C # Eliminará todos los arcos conectados con A Lista de arcos: graph() and get.edgelist(): # Un grafo dirigido vacío # graph() id desde 1. g1 &lt;- graph( c(1,2, 1,3, 2,3, 3,4 ));g1 IGRAPH 9565c9e D— 4 4 – + edges from 9565c9e: [1] 1-&gt;2 1-&gt;3 2-&gt;3 3-&gt;4 summary(g1) IGRAPH 9565c9e D— 4 4 – plot(g1) # El parámetro &quot;directed&quot; a FALSE para # grafos no dirigidos. g2 &lt;- graph( c(1,2, 1,3, 2,3, 3,4 , 3, 5, 1, 3), directed=FALSE); g2 IGRAPH a2c80ef U— 5 6 – + edges from a2c80ef: [1] 1–2 1–3 2–3 3–4 3–5 1–3 summary(g2) IGRAPH a2c80ef U— 5 6 – plot(g2) #Obtener la lista de arcos a partir de un grafo edgelist&lt;-get.edgelist(g2) ; edgelist [,1] [,2] [1,] 1 2 [2,] 1 3 [3,] 2 3 [4,] 3 4 [5,] 3 5 [6,] 1 3 edgelist &lt;- as_edgelist(g2) ; edgelist [,1] [,2] [1,] 1 2 [2,] 1 3 [3,] 2 3 [4,] 3 4 [5,] 3 5 [6,] 1 3 # Obtener el grafo a partir de la lista de arcos g3&lt;-graph( t(edgelist)); g3; plot(g3) IGRAPH eea1d55 D— 5 6 – + edges from eea1d55: [1] 1-&gt;2 1-&gt;3 2-&gt;3 3-&gt;4 3-&gt;5 1-&gt;3 g3&lt;-graph( edgelist); g3; plot(g3) IGRAPH 92fc84f D— 5 6 – + edges from 92fc84f: [1] 1-&gt;1 2-&gt;3 3-&gt;1 2-&gt;3 3-&gt;4 5-&gt;3 # algunos parámetros de plot plot(g3, vertex.color=&quot;green&quot;, edge.arrow.size=0.5, vertex.size=25, edge.curved=0.5, layout_as_star=TRUE) Matrices de adyacencia: graph.adjacency(), get.adjacency() adjm_u&lt;-matrix( c(0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0), nrow=6, ncol=6, byrow = TRUE) #grafo a partir de matriz de adyacencia g_adj_u &lt;- graph.adjacency(adjm_u, mode=&quot;undirected&quot;) plot(g_adj_u) # Matriz de adyacencia a partir de grafo A &lt;- get.adjacency(g_adj_u); A 6 x 6 sparse Matrix of class “dgCMatrix” [1,] . 1 . . 1 . [2,] 1 . 1 . 1 . [3,] . 1 . 1 . . [4,] . . 1 . 1 1 [5,] 1 1 . 1 . . [6,] . . . 1 . . A &lt;- as_adjacency_matrix(g_adj_u, sparse = FALSE) A [,1] [,2] [,3] [,4] [,5] [,6] [1,] 0 1 0 0 1 0 [2,] 1 0 1 0 1 0 [3,] 0 1 0 1 0 0 [4,] 0 0 1 0 1 1 [5,] 1 1 0 1 0 0 [6,] 0 0 0 1 0 0 Grafo a partir de data frame # Primero, crear el data frame node1 = c(&quot;Ella&quot;, &quot;Tu&quot;, &quot;El&quot;); node2 = c(&quot;El&quot;, &quot;Ella&quot;, &quot;Tu&quot;) weight = c(10, -2, 3) df = data.frame(node1, node2, weight); df node1 node2 weight 1 Ella El 10 2 Tu Ella -2 3 El Tu 3 # Crear el grafo g &lt;- graph.data.frame(df, directed=FALSE); g IGRAPH 9066860 UNW- 3 3 – + attr: name (v/c), weight (e/n) + edges from 9066860 (vertex names): [1] Ella–El Ella–Tu Tu –El plot(g) # Si se conocen los vértices # g &lt;- graph.data.frame(df, vertices=listvertices, directed=FALSE);g # Obtener los nombres de los nodos V(g)$name [1] “Ella” “Tu” “El” # Obtener los pesos de los arcos E(g)$weight [1] 10 -2 3 Grafo a partir de literales #?graph_from_literal g &lt;- graph_from_literal(A--C, A-+D, C-+A, , D-+C) g IGRAPH f5f047e DN– 4 3 – + attr: name (v/c) + edges from f5f047e (vertex names): [1] A-&gt;D C-&gt;A D-&gt;C plot(g) #IGRAPH DN-- 4 4 -- #+ attr: name (v/c) #+ edges (vertex names): #[1] A-&gt;D D-&gt;C D-&gt;B B-&gt;A G3 &lt;-graph_from_literal(A-B, B -+C) plot(G3) G3 &lt;-graph_from_literal(A-B, B -C) plot(G3) grafo aleatorio 10.2.3 Visualización Buscar ayuda de los comandos plot.igraph, igraph.plotting. A continuación dibujamos algunos grafos interesantes: #library(igraph) # Trees g &lt;- make_tree(27, children=3) g; plot(g) IGRAPH d2f5e35 D— 27 26 – Tree + attr: name (g/c), children (g/n), mode (g/c) + edges from d2f5e35: [1] 1-&gt; 2 1-&gt; 3 1-&gt; 4 2-&gt; 5 2-&gt; 6 2-&gt; 7 3-&gt; 8 3-&gt; 9 3-&gt;10 4-&gt;11 4-&gt;12 4-&gt;13 [13] 5-&gt;14 5-&gt;15 5-&gt;16 6-&gt;17 6-&gt;18 6-&gt;19 7-&gt;20 7-&gt;21 7-&gt;22 8-&gt;23 8-&gt;24 8-&gt;25 [25] 9-&gt;26 9-&gt;27 # Cliques g &lt;- make_full_graph(n=6) g; plot(g) IGRAPH fe2cf58 U— 6 15 – Full graph + attr: name (g/c), loops (g/l) + edges from fe2cf58: [1] 1–2 1–3 1–4 1–5 1–6 2–3 2–4 2–5 2–6 3–4 3–5 3–6 4–5 4–6 5–6 # Lattices g &lt;- make_lattice(dimvector = c(5,5), circular = FALSE) V(g)$label &lt;- NA g; plot(g) IGRAPH 2bca810 U— 25 40 – Lattice graph + attr: name (g/c), dimvector (g/n), nei (g/n), mutual (g/l), circular | (g/l), label (v/l) + edges from 2bca810: [1] 1– 2 1– 6 2– 3 2– 7 3– 4 3– 8 4– 5 4– 9 5–10 6– 7 [11] 6–11 7– 8 7–12 8– 9 8–13 9–10 9–14 10–15 11–12 11–16 [21] 12–13 12–17 13–14 13–18 14–15 14–19 15–20 16–17 16–21 17–18 [31] 17–22 18–19 18–23 19–20 19–24 20–25 21–22 22–23 23–24 24–25 #Stars g &lt;- make_star(n=10,mode = &quot;undirected&quot;) g; plot(g) IGRAPH 46f5d07 U— 10 9 – Star + attr: name (g/c), mode (g/c), center (g/n) + edges from 46f5d07: [1] 1– 2 1– 3 1– 4 1– 5 1– 6 1– 7 1– 8 1– 9 1–10 Anillo con conexiones cruzadas: g &lt;- make_ring(10, directed=TRUE, mutual=TRUE) V(g)$name &lt;- LETTERS[1:10] g &lt;- g + edges(9,5, 7,1, 1,5) plot(g) 10.2.4 Layout Un layout es un conjunto de coordenadas x,y preestablecidas. Se pueden especificar manualmente o usando layout_functions Determina la posición de los nodos en la red. Hay layouts ya diseñados o puedes diseñarlo desde 0. Intentar minimizar cruces de arcos. Algoritmos que lo consiguen: por ejemplo - Kamada Kawai algorithm, the Fruchterman Reingold algorithm, etc. Lykamada &lt;- layout.kamada.kawai(g) plot(g, layout=Lykamada) Lyfruchtermant &lt;- layout.fruchterman.reingold(g) plot(g, layout=Lyfruchtermant) lo &lt;- layout_in_circle(g) head(lo, n=4) [,1] [,2] [1,] 1.000000 0.0000000 [2,] 0.809017 0.5877853 [3,] 0.309017 0.9510565 [4,] -0.309017 0.9510565 # lo es una matriz de coordenadas lo [,1] [,2] [1,] 1.000000 0.000000e+00 [2,] 0.809017 5.877853e-01 [3,] 0.309017 9.510565e-01 [4,] -0.309017 9.510565e-01 [5,] -0.809017 5.877853e-01 [6,] -1.000000 1.224647e-16 [7,] -0.809017 -5.877853e-01 [8,] -0.309017 -9.510565e-01 [9,] 0.309017 -9.510565e-01 [10,] 0.809017 -5.877853e-01 plot(g, layout=lo) # See ?layout_ for a full list # Para redes tipo árbol: layout_as_tree gTree &lt;- make_tree(15) plot(gTree, layout=layout_as_tree(gTree, root = 1)) # layout como un grid plot(g, layout=layout_on_grid(g)) Mallas: 10.2.5 Dibujar grafos ponderados 10.2.6 Cambiar aspecto y propiedades de un grafo V(g)$shape V(g)$size V(g)$color vertex.shape vertex.color vertex.size set_edge_attr set_vertex_attr set_graph_attr Note: colores en R http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf 10.2.7 Ejemplos - plantillas Plot vocales como rectángulos g &lt;- make_ring(10, directed=TRUE, mutual=TRUE) V(g)$name &lt;- LETTERS[1:10] g &lt;- g + edges(9,5, 7,1, 1,5) plot(g) vowel &lt;- V(g)$name %in% c(&quot;A&quot;,&quot;E&quot;,&quot;I&quot;,&quot;O&quot;,&quot;U&quot;) + 1 # gives 1 or 2 plot(g, layout=lo, vertex.shape=c(&quot;circle&quot;, &quot;square&quot;)[vowel]) #colores plot(g, layout=lo, vertex.color=c(&quot;tomato2&quot;, &quot;royalblue&quot;)[vowel]) #tamaño plot(g, layout=lo, vertex.size=c(15,30)[vowel]) #Propiedades usando atributos V(g)$shape &lt;- &quot;circle&quot; # Aplicado a todos los vértices V(g)$size &lt;- 15 V(g)$color &lt;- &quot;orange&quot; isVowel &lt;- V(g)$name %in% c(&quot;A&quot;,&quot;E&quot;,&quot;I&quot;,&quot;O&quot;,&quot;U&quot;) # Sobreescribir los nodos vocales V(g)[isVowel]$shape &lt;- &quot;square&quot; V(g)[isVowel]$color &lt;- &quot;royalblue&quot; V(g)[isVowel]$size &lt;- 25 plot(g, layout=lo) Propiedades de los arcos E(g)$width &lt;- 1 v1 &lt;-V(g)[isVowel] v1 3/10 vertices, named, from 9abb6b2: [1] A E I E(g)[v1 %--% v1]$width &lt;- 4 # Ver http://igraph.org/r/doc/igraph-es-indexing.html plot(g, layout=lo) #Arcos curvados plot(g, layout=lo, edge.curved=0.3*which_mutual(g)) Agrupaciones por índices: groupList &lt;- list(vowelGroup = which(isVowel), constGroup1 = c(2,3,4), constGroup2 = c(6,7,8)) groupColours &lt;- c(rgb(0,0.3,1,0.5), rgb(0.8,0.4,0.1,0.5), rgb(0.8,0.4,0.1,0.5)) plot(g, layout=layout_with_fr, # Fruchterman?Reingold layout mark.groups=groupList, # Mark the groups mark.col= groupColours, # Eliminar el borde mark.border = NA, edge.curved=0.1*which_mutual(g)) text(0.45,0.1,&quot;Vocales&quot;, cex=1.5) text(0.5,0.9,&quot;Grupo consonantes 1&quot;, cex=1.5) text(-0.8,-1,&quot;Grupo consonantes 2&quot;, cex=1.5) 10.3 Exportar grafos igraph permite importar y exportar de/desde un considerable número de formatos. Se usan los comandos read_graph y write_graph. Un formato abierto (open) es graphml. write_graph(g, &quot;gr1.graphml&quot;, format=&quot;graphml&quot;) Otros formatos: edgelist: Fichero de texto con arcos en cada línea. pajek: Pajek es un programa popular en Windows para análisis de redes. gml: Graph Modelling Language es uno de los formatos abiertos más populares. graphml: Graph Markup Language es un formato abierto basado en XML. dot: Formato usado por GraphViz. ** Gephi: Para exportar al formato nativo GEXF de Gephise usa el paquete rgexf al que puede convertirse desde un objeto igraph ** Referencias: https://www.r-project.org/nosvn/conferences/useR-2010/slides/Zhang.pdf https://programminghistorian.org/en/lessons/temporal-network-analysis-with-r 10.4 De Twitter El primer paso será extraer los términos usando las técnicas de text mining y crear una matriz de términos (DTM - Document Term Matrix:). Los documentos serían los tweets y los términos serían las palabras o grupos de palabras destacadas en los datos extraidos. Objetivo: Construir una red de términos (personas) basada en sus co-ocurrencias en los mismos tweets (pertenencia a los mismos grupos). 10.5 Medidas de bondad, calidad Analysis of the Networks to extract knowledge. &gt;&gt; Goal of a SNA proyect \\[[http://snap.stanford.edu/class/cs224w-2015/slides/06-applicationsI.pdf\\](http://snap.stanford.edu/class/cs224w-2015/slides/06-applicationsI.pdf)](http://snap.stanford.edu/class/cs224w-2015/slides/06-applicationsI.pdf](http://snap.stanford.edu/class/cs224w-2015/slides/06-applicationsI.pdf)) * Locate people in the network for… higher compensation positive performance evaluations more promotions more good ideas &gt; Ego network is a special type of network consisting of one central node and all other nodes directly connected to it. The central node is known as ego, while the other surrounding nodes directly connected to it are known as alters. * \\[[https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f\\](https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f)](https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f](https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f)) * \\[[http://olizardo.bol.ucla.edu/classes/soc-111/lessons-winter-2022/5-lesson-egonet-metrics.html\\](http://olizardo.bol.ucla.edu/classes/soc-111/lessons-winter-2022/5-lesson-egonet-metrics.html)](http://olizardo.bol.ucla.edu/classes/soc-111/lessons-winter-2022/5-lesson-egonet-metrics.html](http://olizardo.bol.ucla.edu/classes/soc-111/lessons-winter-2022/5-lesson-egonet-metrics.html)) Vamos a usar este grafo como ejemplo de las medidas de bondad del grafo. ```{r} library(igraph) g1 &lt;- graph( c(1,2, 1,3, 2,3, 3,4, 3,5, 1,5, 4,2, 3,6, 4,8, 8,1, 9,1, 10,2, 7,6, 5,10)) g1 summary(g1) plot(g1) ``` **Vértices, arcos**. ```{r} class(g1) V(g1) V(g1)\\[1\\] E(g1) E(g1)\\[1\\] class(V(g1)) class(E(g1)) ``` 10.5.1 Centrality Importancia de los nodos en un grafo. - Número de arcos de entrada-salida de los nodos. - Redes con alta centralidad tienen pocos nodos con muchas conexiones. - Redes con baja centralidad tienen muchos nodos con similar o menos conexiones. - Ver https://en.wikipedia.org/wiki/Centrality#PageRank_centrality 10.5.2 Degree Número de arcos conectados a un vértice. Señala la importancia de un vértice o el nivel de actividad del vértice en la red. - Cómo de central es un nodo en la red - Cuántos arcos de entrada-salida tiene o con cuántos nodos se conecta directamente via un arco. &gt; &gt; `centr_degree`, `igraph::degree` ```{r} g1 plot(g1) igraph::degree(g1) igraph::degree(g1, mode=“in”) igraph::degree(g1, mode=“out”) deg &lt;- centr_degree(g1) deg ``` 10.5.3 Betweenness Mide el grado en el que la información fluye a través de un vértice particular y su importancia relativa como un intermediario en la red. Describe nodos que son conexiones clave o puentes entre grupos de nodos. - El número de caminos más cortos que pasan por un nodo dado (medida relativa) - la suma de las longitudes de los caminos más cortos entre otros nodos pasando por el nodo, dividida por las longitudes de camino más cortas (no necesariamente a través del nodo) entre los otros nodos. &gt; &gt; `igraph::betweenness` ```{r} igraph::betweenness(g1) ``` - Por el vértice 6 no pasa ningún *camino más corto* entre dos vértices. - Por el vértice 3 pasan 25 *caminos más cortos* entre dos vértices. - … 10.5.4 Edge_betweenness Similar al anterior pero teniendo en cuenta cada arco. &gt; &gt; `igraph::edge_betweenness` ```{r} g1 &lt;- set.edge.attribute(g1, “weight”, value= 1) bg &lt;- edge_betweenness(g1) plot(g1, edge.label = bg) ``` 10.5.5 Closeness Distancia a otros nodos. Un nodo con valor alto de este estimador es más central y puede difundir la información a muchos otros nodos. - Se obtiene como 1 divido por la suma de las distancias geodésicas desde un vértice al resto. Alcanzará su valor máximo cuando un vértice esté conectado a todos los demás. Longitud media de los caminos más cortos (geodésicos). - Mide cuantos pasos se requieren desde un vértice para alcanzar el resto de vértices de la red. - Caminos cortos entre vértices señalan que estos están cercanos unos a otros. &gt; &gt; `centr_clo`, `igraph::closeness` ```{r} igraph::closeness(g1) ``` 10.5.6 Eigenvector No todas las conexiones tienen la misma importancia - medida de la importancia de un nodo. - La medida *Eigenvector Centrality* se calcula como el autovalor de mayor módulo de la matriz de adyacencia que contiene los pesos. - *a high score to vertices that either have a lot of connections, or are connected to someone with a lot of connections* &gt; &gt; Eigenvector Centrality: `eigen_centrality` ```{r} eigen_centrality(g1) ``` 10.5.7 Pagerank Algoritmo de Google para realizar un ranking con la importancia de los resultados de la búsqueda. Nodos son más importantes si tienen muchos enlaces de entrada. &gt; &gt; `page.rank` ```{r} page.rank(g1) ``` # Paths Caminos de un vértice a otro o de grupos de vértices a otros. 10.5.8 Diameter El máximo camino más corto entre cualquier par de nodos. En grafos muy grandes indica la posibilidad de que la información se difunda más o menos fácilmente. El algoritmo tiene costo $O(n^3)$. En Twitter hay cientos de millones de usuarios,… &gt; &gt; `get_diameter` &gt; &gt; `diameter`- el camino más largo entre dos nodos. ```{r} plot(g1) diameter(g1) ``` 10.5.9 Caminos y distancias **Distancia geodésica**: El menor número de arcos a atravesar para conectar dos nodos. ```{r} sp &lt;- shortest_paths(g1, from=“1”, to=“10”) sp$vpath sp1 &lt;- shortest_paths(g1, from=“1”, to=“9”) sp1$vpath distances(g1) ``` Relacionado con distancias: &gt; &gt; `distance_table, mean_distance` # Clustering *Whether your friends are likely to be friends*. **Grupos**: Subconjunto de vértices que comparten características en común. - Una primera forma es buscar los triángulos en el grafo. \\*La medida de clustering (transitividad) es la frecuencia relativa de triángulos cerrados. $$C=\\frac{3*\\mbox{ número de triángulos }}{\\mbox{número de triples conectados}}$$ 10.5.10 Transitivity *friends of friends to be friends and enemies of enemies to be enemies* Probabilidad de que vértices adyacentes de un vértice estén conectados - se denomina también coeficiente de agrupación o *clustering*. &gt; &gt; `transitivity`, `shortest_paths` 10.5.10.1 Global clustering: ```{r} transitivity(g1, type = “global”) ``` 10.5.10.2 Local clustering Fracción de triples conectados a través de cada vértice que son cerrados. ```{r} transitivity(g1, type = “local”) ``` # Otras medidas y definiciones - Densidad: Número de conexiones respecto al total de conexiones posibles. Un grafo completo tiene una densidad igual a 1 - `edge_density`. - Popularidad: nodos que son centrales tienden a ser más populares. - Cliques: todos con todos - `clique_num(g, min=k)` encuenta cliques con un mínimo de k vértices. - Componentes: Una componente es el conjunto de vértices de la que tienen conexiones entre ellos. Una red puede tener varias componentes - `components` - Nodos a distancia k - `random_walk`. - Hub, Authorities - `hub_score, authority.score` un nodo se denomina hub tiene muchos enlaces de salida y se denomina authorities si tiene muchos de entrada. - detección de comunidades: `cluster_edge_betweenness` 10.6 Proyecto SNA - USairports Nota: Buscar información en https://igraph.org/r/ Cargamos la librería igraphdata y el dataset USairports. Cambia el nombre al grafo (nuevo nombre US) usando:graph_attr. Visualizar el grafo. Usar función V para acceder a los vértices del grafo. Visualizar los cinco primeros vértices. Usar función E para acceder a los arcos del grafo. Visualizar los cinco primeros arcos. Mira las propiedades asociadas a los vértices vertex_attr_names, vertex_attr. Usando estas propiedades extrae los nombres de los 5 primeros aeropuertos y de sus ciudades. 6.Examina las propiedades asociadas a los arcos edge_attr_names, `edge_attr```. Usando estas propiedades extrae los nombres de los 5 primeros compañías aéreas (carrier), de cuantas salidas tiene cada una de estas compañías. Mira las conexiones entre los cinco primeros aeropuertos. Usa vcount para saber el tamaño del grafo. Usa V() y las funciones de manejo de listas para añadirle un nuevo atributo a los vértices: Group que va a tener como valor para cada aeropuerto los valores “A” o “B” aleatoriamente. Analizar qué hacen las siguientes órdenes: Analizar qué hacen las siguientes órdenes: Eliminar del grafo el aeropuerto último de la lista de aeropuertos. Eliminar la conexión entre BJC y MIA. Encontrar las conexiones directas desde BOS. Encontrar las conexiones desde BOS. Caminos y distancias entre dos aeropuertos. Explica cómo calcularlas. ¿Qué aeropuertos están a más de 1000 km? ¿Cuantas conexiones de entrada y salida tiene el primer aeropuerto? ¿Cuales son las ciudades vecinas de Bangor (conectadas por vuelos directamente) Usa neighbors. Analizar qué hacen las siguientes órdenes: Pasar el grafo a un dataframe. Pasar el grafo a una matriz y encontrar los aeropuertos conectados por tres escalas. Guardar en un grafo y visualizarlo. Aplicar a este grafo, las medidas de calidad de grafos: tamaño, diámetro, clustering, transitividad, etc. Buscar información en https://igraph.org/r/ Eliminar ciclos en el grafo. Buscar información de simplify. 10.7 Proyecto - películas + actores En el ejercicio verás funciones aplicadas al grado y sus resultados como resumen, repaso y explicación de nuevas funcionalidades. Aparecen resultados sin el código correspondiente. Esos son los apartados a resolver (los resultados que aquí aparecen son orientativos - no tiene porqué salir exáctamente lo mismo). 10.7.1 crear grafo Crea un grafo dirigido usando igraph con los dos data.frames leidos anteriormente Visualiza el grafo IGRAPH 81ec364 UN– 8 16 – + attr: name (v/c), Gender (v/c), BestActorActress (v/c), Movie (e/c) + edges from 81ec364 (vertex names): [1] Tom Hanks –Gary Sinise Tom Hanks –Robin Wright [3] Gary Sinise –Robin Wright Tom Hanks –Gary Sinise [5] Tom Hanks –Bill Paxton Tom Hanks –Kevin Bacon [7] Tom Hanks –Ed Harris Gary Sinise –Bill Paxton [9] Gary Sinise –Kevin Bacon Gary Sinise –Ed Harris [11] Bill Paxton –Kevin Bacon Bill Paxton –Ed Harris [13] Kevin Bacon –Ed Harris Ed Harris –Sean Connery [15] Ed Harris –Nicolas Cage Sean Connery–Nicolas Cage 10.7.2 Análisis del grafo ¿Nodos en el grafo? [1] 8 [1] 8 ¿Arcos en el grafo? [1] 16 Nodo 3 del grafo 1/8 vertex, named, from 81ec364: [1] Robin Wright Arco 1 del grafo 1/16 edge from 81ec364 (vertex names): [1] Tom Hanks–Gary Sinise Cambia algunas propiedades del grafo: Cambia el layout del grafo (he usado el de estrella pero puedes usar el que quieras. Agrupar los nodos de protagonistas que han ganado un oscar 10.7.3 Cambiar atributos del grafo Para acceder a los atributos del grafo usamos: IGRAPH 81ec364 UN– 8 16 – + attr: name (v/c), Gender (v/c), BestActorActress (v/c), Movie (e/c) + edges from 81ec364 (vertex names): [1] Tom Hanks –Gary Sinise Tom Hanks –Robin Wright [3] Gary Sinise –Robin Wright Tom Hanks –Gary Sinise [5] Tom Hanks –Bill Paxton Tom Hanks –Kevin Bacon [7] Tom Hanks –Ed Harris Gary Sinise –Bill Paxton [9] Gary Sinise –Kevin Bacon Gary Sinise –Ed Harris [11] Bill Paxton –Kevin Bacon Bill Paxton –Ed Harris [13] Kevin Bacon –Ed Harris Ed Harris –Sean Connery [15] Ed Harris –Nicolas Cage Sean Connery–Nicolas Cage $name [1] “Tom Hanks” “Gary Sinise” “Robin Wright” “Bill Paxton” “Kevin Bacon” [6] “Ed Harris” “Sean Connery” “Nicolas Cage” $Gender [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” $BestActorActress [1] “Winner” “None” “None” “None” “None” “Nominated” [7] “None” “Winner” $Movie [1] “Forest Gump” “Forest Gump” “Forest Gump” “Apollo 13” “Apollo 13” [6] “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” [11] “Apollo 13” “Apollo 13” “Apollo 13” “The Rock” “The Rock” [16] “The Rock” 8/8 vertices, named, from 81ec364: [1] Tom Hanks Gary Sinise Robin Wright Bill Paxton Kevin Bacon [6] Ed Harris Sean Connery Nicolas Cage [1] “Tom Hanks” “Gary Sinise” “Robin Wright” “Bill Paxton” “Kevin Bacon” [6] “Ed Harris” “Sean Connery” “Nicolas Cage” [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” [1] “Winner” “None” “None” “None” “None” “Nominated” [7] “None” “Winner” 1/16 edge from 81ec364 (vertex names): [1] Tom Hanks–Gary Sinise [1] “Forest Gump” “Forest Gump” “Forest Gump” “Apollo 13” “Apollo 13” [6] “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” [11] “Apollo 13” “Apollo 13” “Apollo 13” “The Rock” “The Rock” [16] “The Rock” $name [1] “Tom Hanks” “Gary Sinise” “Robin Wright” “Bill Paxton” “Kevin Bacon” [6] “Ed Harris” “Sean Connery” “Nicolas Cage” $Gender [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” $BestActorActress [1] “Winner” “None” “None” “None” “None” “Nominated” [7] “None” “Winner” $color [1] “green” “green” “green” “green” “green” “green” “green” “green” 8 x 8 sparse Matrix of class “dgCMatrix” Tom Hanks Gary Sinise Robin Wright Bill Paxton Kevin Bacon Tom Hanks . 2 1 1 1 Gary Sinise 2 . 1 1 1 Robin Wright 1 1 . . . Bill Paxton 1 1 . . 1 Kevin Bacon 1 1 . 1 . Ed Harris 1 1 . 1 1 Sean Connery . . . . . Nicolas Cage . . . . . Ed Harris Sean Connery Nicolas Cage Tom Hanks 1 . . Gary Sinise 1 . . Robin Wright . . . Bill Paxton 1 . . Kevin Bacon 1 . . Ed Harris . 1 1 Sean Connery 1 . 1 Nicolas Cage 1 1 . Recordar usar funciones vectoriales: $name [1] “Tom Hanks” “Gary Sinise” “Robin Wright” “Bill Paxton” “Kevin Bacon” [6] “Ed Harris” “Sean Connery” “Nicolas Cage” $Gender [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” $BestActorActress [1] “Winner” “None” “None” “None” “None” “Nominated” [7] “None” “Winner” $color [1] “green” “green” “green” “green” “green” “green” “green” “green” $Movie [1] “Forest Gump” “Forest Gump” “Forest Gump” “Apollo 13” “Apollo 13” [6] “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” [11] “Apollo 13” “Apollo 13” “Apollo 13” “The Rock” “The Rock” [16] “The Rock” $color [1] “red” “red” “red” “blue” “blue” “blue” “blue” “blue” “blue” “blue” [11] “blue” “blue” “blue” “red” “red” “red” Más información de los grafos, vértices y arcos: color, size, shape, etc. https://igraph.org/r/doc/ [[https://igraph.org/r/doc/plot.common.html](https://igraph.org/r/doc/plot.common.html](https://igraph.org/r/doc/plot.common.html Calcula los índices de los vértices del grafo correspondientes a los protagonistas que han ganado un oscar (no mirando en el dataset). Usando estos índices, destaca en amarillo los actores que han ganado un oscar. 2/8 vertices, named, from 81ec364: [1] Tom Hanks Nicolas Cage Destaca con un arco rojo qué actores han trabajado juntos en Apollo13 o en The Rock. 13/16 edges from 81ec364 (vertex names): [1] Tom Hanks –Gary Sinise Tom Hanks –Bill Paxton [3] Tom Hanks –Kevin Bacon Tom Hanks –Ed Harris [5] Gary Sinise –Bill Paxton Gary Sinise –Kevin Bacon [7] Gary Sinise –Ed Harris Bill Paxton –Kevin Bacon [9] Bill Paxton –Ed Harris Kevin Bacon –Ed Harris [11] Ed Harris –Sean Connery Ed Harris –Nicolas Cage [13] Sean Connery–Nicolas Cage Para los ganadores de oscar, hacemos que el tamaño del nodo se corresponda con la importancia de ese actor en el dataset y además que la etiqueta sea más grande : Definimos la importancia como el número de películas multiplicado por 15. Ayuda: ver funciones strength(grafo) Eliminar el nombre (la etiqueta - atributo label) de los nodos con protagonistas con importancia menor que 4. 10.7.4 Modificar el grafo Para practicar todo lo visto: - Modifica los atributos del grafo para que se muestre parecido al siguiente (función legend para las leyendas): 13/16 edges from 81ec364 (vertex names): [1] Tom Hanks –Gary Sinise Tom Hanks –Bill Paxton [3] Tom Hanks –Kevin Bacon Tom Hanks –Ed Harris [5] Gary Sinise –Bill Paxton Gary Sinise –Kevin Bacon [7] Gary Sinise –Ed Harris Bill Paxton –Kevin Bacon [9] Bill Paxton –Ed Harris Kevin Bacon –Ed Harris [11] Ed Harris –Sean Connery Ed Harris –Nicolas Cage [13] Sean Connery–Nicolas Cage "],["pagerank-1.html", "Capítulo 11 pagerank", " Capítulo 11 pagerank "],["análisis-de-componentes-principales.html", "Capítulo 12 Análisis de Componentes Principales", " Capítulo 12 Análisis de Componentes Principales "],["text-imining.html", "Capítulo 13 Text Imining", " Capítulo 13 Text Imining "],["references.html", "References", " References "]]
