[["index.html", "Ciencia de Datos - UMA Capítulo 1 Introducción", " Ciencia de Datos - UMA Domingo López, Ángel Mora 2023-09-06 Capítulo 1 Introducción En este libro queremos recopilar material que se ha estado utilizando en las asignaturas de: Laboratorio de Computación Científica - E.T.S.I. Informática - Universidad de Málaga Introducción a Ciencia de Datos I - Máster Universitario en Ingeniería Informática - Universidad de Málaga Docentes: Domingo López-Rodíguez - dominlopez@uma.es https://dominlopez.netlify.com Ángel Mora Bonilla - amora@uma.es - https://amorabonilla.github.io Contenido: Resúmenes de material Ejercicios Proyectos "],["análisis-exploratorio-de-datos.html", "Capítulo 2 Análisis Exploratorio de Datos 2.1 Importación de datos 2.2 dplyr para Manipulación de Datos 2.3 Ordenar filas con arrange() 2.4 Añadir o renombrar variables con mutate() 2.5 Proyecto Starwarks", " Capítulo 2 Análisis Exploratorio de Datos https://dplyr.tidyverse.org/ https://dplyr.tidyverse.org/articles/dplyr.html https://r4ds.had.co.nz/transform.html Descargar a local desde directorio Datasets de CV, un dataset descargado del repositorio UCI: breast-cancer.data, y breast-cancer.names1.csv (nombres de las columnas) Usar el paquete dplyr, otro de los paquetes centrales del denominado tidyverse. En tidyverse - dplyr Instalar tidyverse Cargar paquete tidyverse 2.1 Importación de datos Importa en R el fichero breast-cancer.data. Colocar los nombres al dataset que están en breast-cancer.names1.csv. 2.2 dplyr para Manipulación de Datos dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges: 5 funciones clave de dplyr: Escoger observaciones (filas) según sus valores (filter()) - filtrar, consultar las filas según condiciones, etc. Reordenar las filas (arrange()). Seleccionar variables por su nombre (select()). Añadir nuevas variables como función de variables ya existentes (mutate()). Encontrar valores representativos de cada variable (summarise()). Estas se combinan con: group_by() which allows you to perform any operation “by group”. Todos estos verbos funcionan de la misma manera: El primer argumento es un dataframe. Los demás argumentos describen qué hacer con el dataframe, usando los nombres de las variables (columnas) sin necesidad de utilizar comillas. El resultado es un nuevo dataframe. 2.2.1 Backends In addition to data frames/tibbles, dplyr makes working with other computational backends accessible and efficient. Below is a list of alternative backends: dtplyr: for large, in-memory datasets. Translates your dplyr code to high performance data.table code. dbplyr: for data stored in a relational database. Translates your dplyr code to SQL. sparklyr: for very large datasets stored in Apache Spark. 2.2.2 Filtrado de filas con filter() filter() extrae un subconjunto de las observaciones (filas), basándose en los valores de una o más columnas. Argumentos: - nombre del dataframe - expresiones (lógicas) para filtrar el dataframe extraer las filas del dataset para los pacientes que el tumor ha sido recurrente extraer las filas del dataset que tienen cancer en el pecho izquierdo, no radiadas y de edad 40 a 49 extraer las filas del dataset con edad menor de 35 años y que tienen cancer en el pecho izquierdo 2.3 Ordenar filas con arrange() Para ordenar con arrange() indicar columnas por las que ordenar ascendentemente, en caso de querer descendente: desc(variable). Ejemplo: num_casos 286 deg_malig n media lasuma 1 71 1 71 2 130 2 260 3 85 3 255 reordenar el dataset según el tamaño del tumor reordenar el dataset según el tamaño del tumor y el grado de que tiene el tumor reordenar el dataset según el tamaño del tumor (pero descendiente) y el grado de maligno que tiene el tumor - ayuda - desc(columna) extraer los 10 pacientes con pre-menopausia, con mayor tamaño de tumor y menor número de nodos invasores y ordenados por edad. 2.3.1 Selección de columnas select() seleccionar únicamente aquellas variables en las que estamos interesados - ayuda - select(dataset, col1, col2, col3) Seleccionar del dataset las columnas clase, tamaño de tumor y grado de tumor Podemos seleccionar intervalos de columnas (de col1 a col4): ‘deseleccionar’ columnas - NO SELECCIONAR COLUMNAS Otros argumentos interesantes select(): everything(), es mover algunas columnas o variables al principio del dataframe. starts_with('abc'): encuentra todas las columnas cuyo nombre comienza por “abc”. ends_with('xyz'): encuentra todas las columnas cuyo nombre termina en “xyz”. contains('ijk'): para seleccionar las columnas cuyo nombre contenga la cadena de caracteres “ijk”. Y otras funciones más complejas (que filtran las columnas por expresiones regulares), que se pueden ver al hacer ?select. 2.4 Añadir o renombrar variables con mutate() El verbo mutate() se usa para añadir nuevas columnas al final del dataframe. añadimos columna dist_grado_peor que es 4 menos el grado de malignidad. añadir una columna media_tumor que tenga el valor medio de los valores almacenados en variable tumor_size 2.4.1 rename() función rename(), que, internamente, se comporta como select(), pero guardando todas las variables que no se mencionan explícitamente: 2.4.2 Uso del operador %&gt;% %&gt;% utiliza la salida del término que hay a la izquierda del símbolo %&gt;% como primer argumento de la función que está a la derecha de dicho símbolo. x %&gt;% f(y) es igual que hacer f(x, y) 2.4.3 Otros verbos auxiliares 2.4.3.1 summarize() num_casos 286 n() contar n_distinct() valores únicos mean(), min(), max() first(), last(), nth() 2.4.3.2 group_by() summarize se suele usar con group by deg_malig n media 1 71 1 2 130 2 3 85 3 2.4.3.3 slice() Usando slice() extraer los 10 pacientes con pre-menopausia, con mayor tamaño de tumor y menor número de nodos invasores y ordenados por edad. Recurrencia age menopause tumor_size inv_nodes node_caps 1 no-recurrence-events 30-39 premeno 30-34 0-2 no 2 no-recurrence-events 40-49 premeno 20-24 0-2 no 3 no-recurrence-events 40-49 premeno 20-24 0-2 no 4 no-recurrence-events 60-69 ge40 15-19 0-2 no 5 no-recurrence-events 40-49 premeno 0-4 0-2 no 6 no-recurrence-events 60-69 ge40 15-19 0-2 no 7 no-recurrence-events 50-59 premeno 25-29 0-2 no 8 no-recurrence-events 60-69 ge40 20-24 0-2 no 9 no-recurrence-events 40-49 premeno 50-54 0-2 no 10 no-recurrence-events 40-49 premeno 20-24 0-2 no deg_malig breast breast_quad irradiat 1 3 left left_low no 2 2 right right_up no 3 2 left left_low no 4 2 right left_up no 5 2 right right_low no 6 2 left left_low no 7 2 left left_low no 8 1 left left_low no 9 2 left left_low no 10 2 right left_up no 2.4.3.4 transmute() Si sólo se desea guardar las nuevas variables, se puede usar la función transmute(): 2.4.4 Operaciones con dos tablas https://dplyr.tidyverse.org/articles/two-table.html 2.5 Proyecto Starwarks Usando el dataset starwarks que está en paquete dplyr: Obtener los 10 humanos más viejos, masculinos, con planeta natal Tatooine. Encontrar a aquellos personajes de ojos azules y rubios/as de especie humana, procedentes de Tatooine, ordenados por edad de menor a mayor. Calcular su altura media. Encontrar aquellos personajes de especie Human o Naboo y calcular una variable con los valores pesado o ligero (si su massa es mayor que 79 es pesado). Mostrar las variables name, height mass y heavyorlight y ordenar por mass de mayor a menor. Calcular el indice de masa corporal de todos los personajes (eliminando los que tienen masa o altura NA). A continuación mostrar el nombre, altura, masa y IMC de cada personaje, con orden de IMC descendente. Obtener los personajes cuya única nave fuese un X-wing y ordenarlos de forma descendente según su masa Obtener los personajes de masa superior a la media de las masas, obviando valores nulos, y ordenarlos de forma decreciente. Obtener las alturas medias de los personajes con el campo “gender” igual a “female”, “male” y “hermaphrodite”, ignorando NA. Filtrar por las especies que sean “Droid”, ordenados por altura descendiente y masa. Reemplazar las masas y alturas con valor NA por 1 y mostrar solo la media de todas esas masas y la mediana de esas alturas. Sacar aquellas filas con las cadenas “Jedi” ó “Clones” en la columna films. Agrupar por homeworld y sacar la media de la columnas height y mass (por separado). Reemplazar los valores NA en la columna mass por 0. Filtrar los datos de aquellos personajes que hayan aparecido solo en la película “Return of the Jedi”y que tengan un mundo natal, ordenados por peso. Para ello transforma los valores NA en 0. Seleccionar los humanos que midan más de 170 cm y que hayan salido en Attack of the Clones, agrupandolos por homeworld obviando los NA y hallar la media de sus pesos sustituyendo los NA por la mediana y mostrarlos en orden descendiente. Encontrar para cada homeworld cuantas especies diferentes lo habitan y ordenalos de mayor a menor variedad. Controlar que no se tiene en cuenta NA como especie Filtrar a los personajes mayores de 25, y luego ordenarlos por el número de películas en el que aparecen (dato que no viene directamente y tenemos que obtenerlo). Encontrar cuantas especies diferentes habitan cada homeworld y ordenarlos de mayor a menor variedad, controlando que NA no es una especie. De todos los personajes de Star Wars filtrar por los que tengan mass mayor o igual a 70, agruparlos por species y gender y calcular la media de height de estos (eliminando los valores NA previamente). Mostrar el resultado ordenado de mayor a menor altura Filtrar por aquellos personajes que tienen los ojos azules y un homeworld y birth_year asignados (diferentes de NA) Añadir una columna ficticia en la que se indica la edad que tendrían si no hubiesen muerto y actualmente estemos en 2019, es decir, restar a 2019 su año de nacimiento Agrupar según el país dónde viven Obtener como resultado la media de los valores height, mass y la columna edad previamente calculada Ordenar por la columna mass de forma descendentemente Sustituir los valores NA del peso y la altura por la media de todos los pesos y alturas respectivamente. Filtrar los humanos cuyo peso sea mayor o igual a 70, agrupados por homeworld, calcular la mediana de la altura y el número de humanos de cada homeworld. Ordenar por número de humanos que hay en cada homeworld. Obtener todos los humanos, quitando los que su altura es NA Añadir una columna con la diferencia entre su altura y la altura media de los humanos Agrupémoslos por su homeworld y la columna nueva pasa a ser la media Obten las 3 homeworld que están más por debajo de la media "],["algebra-lineal.html", "Capítulo 3 Algebra Lineal 3.1 grandes sistemas 3.2 matrices dispersas", " Capítulo 3 Algebra Lineal ## sistemas de ecuaciones lineales 3.1 grandes sistemas 3.2 matrices dispersas "],["test-estadísticos-bayesiano.html", "Capítulo 4 Test estadísticos bayesiano", " Capítulo 4 Test estadísticos bayesiano "],["modelling-techniques.html", "Capítulo 5 Modelling techniques 5.1 Interpolation 5.2 Linear Regression 5.3 Generalized Regression 5.4 Improving the models 5.5 Model validation 5.6 Proyecto - regresion EconomistData 5.7 Proyecto - Analysing Boston dataset 5.8 Poyecto - evaluación de cursos - variables categóricas 5.9 Anexo: R avanzado 5.10 Proyecto - Predicción de riesgos", " Capítulo 5 Modelling techniques There are two types of basic numerical techniques: interpolation and approximation (adjustment). The goal is to get a function that fits the best of your ability to a set of data (a cloud of data) from observations, data collected from sensors, datasets, etc. The simplest fitting model is the linear model (LM) named in Machine Learning community as linear regression. Although regression is probably the most simple approach for supervised learning, linear regression is still a useful and widely used statistical learning method. Other more complex statistical learning approaches are generalizations of linear regression. 5.1 Interpolation The normal input is a data table \\[\\{(x_i,y_i)\\}\\] And the goal is to find an interpolating function \\[ \\phi(x)=a_0+a_{1}f_{1}(x)\\] (the easiest approach considers polynomial functions \\(f_1(x)=x\\)). Then, the model will be: \\[ \\phi(x)=a_0+a_{1}x\\] Existence and uniqueness of the solution Simply, if we consider that \\(\\phi(x)\\) verifies the conditions \\((x_i,y_i)\\), the following linear system is buit: \\[ \\left. \\begin{array}{ll} a_0+a_{1}x_{1}&amp;=y_{1}\\\\ a_0+a_{1}x_{2}&amp;=y_{2}\\\\ \\ldots&amp;\\ldots\\\\ a_0+a_{1}x_{m}&amp;=y_{m}\\\\ \\end{array}\\right\\}\\] Solving the system above, the coefficients \\(a_0\\) and \\(a_{1}\\) are obtained (if they exist) and we will have the Interpolation function: \\[ \\phi (x)=a_0+a_{1}x\\] \\(a_0\\) is named the intercept. \\(a_1\\) is named the slope. The condition for existence of the interpolation function \\(\\phi(x)\\) is that the system should be consistent, its uniqueness depends on the solution(s) of the system. 5.1.1 Polynomial interpolation Assuming we have \\(n+1\\) points \\[\\{ (x_{0}, y_{0}), (x_{1}, y_{1}), (x_{2}, y_{2}), \\ldots, (x_{n}, y_{n})\\} \\] polynomial interpolation consists of finding a polynomial function \\(\\phi(x)\\) passing through the given points. \\[ \\phi (x)=a_0+a_{1}x+a_{2}x^2+\\ldots+a_{n}x^n\\] Note that we are using that a basis of the polynomials of grade less than or equal to \\(n\\) is \\(B=\\{1, x,x^2,\\ldots,x^n\\}\\). The equations will be: \\[ \\left. \\begin{array}{lr} a_0+a_{1}x_{0}+a_{2}x_{0}^2+\\ldots&amp;=y_{0}\\\\ a_0+a_{1}x_{1}+a_{2}x_{1}^2+\\ldots&amp;=y_{1}\\\\ \\ldots&amp;\\ldots\\\\ a_0+a_{1}x_{n}+a_{2}x_{n}^2+\\ldots&amp;=y_{n}\\\\ \\\\ \\end{array}\\right\\}\\] 5.2 Linear Regression Minimizing the residual sum of squares With a large number of observations, interpolation methods are not adequate, hence an adjusting linear regression model \\(y=\\phi(x)\\) is used. Such a model reflects the effect of changing a variable \\(x\\) (independent variable) in variable \\(y\\) (dependent variable). It considers that there exist a linear relationship between the variables. A common way to estimate the parameters of a statistical model is to adjust a function which minimize the errors. The most used method is that of minimizing the residual sum of squares, RSS, which is defined by: \\[ RSS(a)={\\mid\\mid \\epsilon \\mid\\mid_2}^2= \\displaystyle\\sum_{i=1}^{N}{\\epsilon_i}^2=\\displaystyle\\sum_{i=1}^{N}(y_i-a^TX)^2 \\] We compute the adjusting function which minimizes the RSS. 5.2.1 Linear Regression in R The easier model is linear regression: \\[ \\phi(x) = a_0 + a_1 x\\] Goal: Prediction of future observations. Find relationships, functions between dataset variables. Description of the data structure. In mathematical terms, regression is to find a linear function that approaches the data cloud. 5.2.2 lm() function Linear model: y is the dependent variable, the output. x is the independent variable, the predictor dataset a dataset with the attributes x and y Call: lm(formula = CPI ~ HDI) Coefficients: (Intercept) HDI -1.540 8.497 The linear model is \\[\\phi(x)=-1.540 + 8.497x\\] Which is the meaning of the coefficient \\(8.497\\)? - If \\(x\\) is increased in 1, then \\(Y\\) is increased in 1 Which is the meaning of the coefficient -1.540? - Value expected of \\(y\\), if \\(x\\) has the value 0 5.2.3 Evaluate the model Does the data fit the model found? Trying a more complicated model?: Polynomial regression. If the point cloud is \\(n\\) in size, interpolation can be achieved with a polynomial grade of \\(n-1\\). Very high grade polynomials introduce errors: oscillations, cost of computation, etc. For each model found, calculate in R the errors made, the accuracy, etc. Call: lm(formula = CPI ~ HDI) Residuals: Min 1Q Median 3Q Max -2.9180 -1.1872 -0.2029 1.0744 3.4453 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.5400 0.4453 -3.458 0.000686 HDI 8.4975 0.6539 12.994 &lt; 2e-16 — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 1.506 on 171 degrees of freedom Multiple R-squared: 0.4968, Adjusted R-squared: 0.4939 F-statistic: 168.9 on 1 and 171 DF, p-value: &lt; 2.2e-16 5.2.4 Accuracy of the model There is some measures of the accuracy of the model. 5.2.4.1 RSE We define the Residual Standard Error as follows: \\[RSE=\\sqrt{\\frac{1}{n-2} \\,\\, RSS}\\] RSE is an estimate of the standard deviation of \\(\\epsilon_i\\). RSE is considered a measure of the lack of fit of the model to the data. If the predictions using the model are very good, we can conclude that the model fits the data very well. On the other hand, if the model doesn’t fit the data well then the RSE may be quite large. 5.2.4.2 R-squared R-squared (called the coefficient of determination) or fraction of variance explained is \\[R^2=1-\\frac{RSS}{TSS}\\] where \\[TSS=\\sum_{i=1}^n (y_i-\\overline{y})\\] RSE is an absolute measure of lack of fit of the model. \\(R^2\\) takes the form of a proportion measure (the proportion of variance explained), that is, the proportion of the variance in the outcome variable that can be accounted for by the predictor. \\(R^2\\) can be estimated with the correlation \\(r\\) between the input (\\(X\\) variable) and the output (\\(Y\\) variable) as follows: \\(R^2=r^2\\). If \\(R^2\\) is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the linear model is wrong, or the inherent error is high, or both. Note: The Pearson correlation is equivalent to running a linear regression model that uses only one predictor variable - the squared correlation \\(r^2\\) is identical to the \\(R^2\\) value for a linear regression with only a single predictor. 5.2.4.3 R-squared and Adjusted R-squared R language returns Multiple R-squared and Adjusted R-squared. If you add more predictors into the model, the \\(R^2\\) value will increase (or at least it will be the same). In a regression model with \\(K\\) predictors, fit to a data set containing \\(N\\) observations, the adjusted \\(R^2\\) is: \\[ \\mbox{adj. } R^2 = 1 - \\left(\\frac{\\mbox{SS}_{res}}{\\mbox{SS}_{tot}} \\times \\frac{N-1}{N-K-1} \\right) \\] The adjusted \\(R^2\\) value will only increase when the new variables improve the model performance. 5.3 Generalized Regression The linear model \\(\\phi(x) = a_0 + a_1 x\\) could be generalized in several ways. We can also apply linear regression to more than 1 variable \\(x=\\{x_1, x_2\\}\\). For example, consider modeling temperature as a function of location \\(\\{x_1,x_2\\}\\). \\[ \\phi(x) = a_0 + a_1 x_1 + a_2x_2\\] or even, \\[ \\phi_2(x) =a_0 + a_1x_1 + a_2x_2 + a_3x_1^2\\] Note: it is possible to model non-linear relationships. In general, a simple approach (we could use non-polynomial functions) is considered using polynomial basis functions, where the model has the form \\[\\phi(x)=a_0+a_1x+a_2x^2+ \\ldots+ a_nx^n\\] 5.3.1 Multiple Linear Regression Given a set of variables \\(X_1, X_2, \\dots, X_m\\), multiple linear regression computes the next model: \\[\\phi(x)=a_o+a_1X_1+a_2X_2+\\dots, a_mX_m+\\epsilon\\] In this model a coefficient \\(a_j\\) is interpreted as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed. The ideal scenario is when the predictors are uncorrelated and in this case each coefficient can be estimated and tested separately. Correlations amongst predictors cause problems because the variance of all coefficients tends to increase, sometimes dramatically. Normally the independence is a utopia. Normally, predictors usually change together. Call: lm(formula = HDI.Rank ~ CPI + HDI) Coefficients: (Intercept) CPI HDI 293.991 -2.935 -283.876 (Intercept) CPI HDI 293.991367 -2.935028 -283.876464 1 2 3 4 5 6 -4.605992 -5.108072 8.665987 -2.157349 -13.936740 2.895255 5.3.1.1 About plot(model) The first plot (residuals vs. fitted values) is a simple scatterplot between residuals and predicted values. R shows some outliers. The second plot (normal Q-Q) is a normal probability plot. It will give a straight line if the errors are distributed normally. Outliers deviate from the straight line. The third plot (Scale-Location), like the the first, should look random. No patterns. We have a little V-shaped pattern. The last plot (Cook’s distance) tells us which points have the greatest influence on the regression (leverage points). We see that points 20,36, 116 have great influence on the model. Detection of outliers: Remove these points and repeat. 5.3.2 Evaluating the Regression Coefficients \\[\\phi(x)=a_o+a_1X_1+a_2X_2+\\dots, a_pX_p\\] 5.3.2.1 F-statistic We establish the null hypothesis: \\[H_O: a_o = a_1 = a_2= \\dots= a_p= 0\\] versus the alternative \\[H_a: \\mbox{ at least one } a_i \\mbox{ is non-zero}\\] The hyphotesis test is performed by means of F-statistic: \\[F=\\frac{(TSS-RSS)/p}{RSS/(n-p-1)}\\] where \\[TSS = \\sum (y_i − \\overline{y} )^2\\] When there is no relationship between the real valor and predictors, the F-statistic must to take on a value close to 1. If \\(H_a\\) is true the F-value must to be greater than 1. If F-value is closer to 1, the answer to reject the hyphotesis depends on values of \\(n\\) and \\(p\\). Using F-value, p-value is computed. 5.3.2.2 p-value Using p-value we can determine whether or not to reject \\(H_0\\). p-value is essentially 0: extremely strong evidence that at least one of the media is associated with output variable. R language returns p-value for each coefficient: These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. Call: lm(formula = HDI.Rank ~ CPI + HDI) Residuals: Min 1Q Median 3Q Max -19.9326 -6.5788 -0.5189 5.9769 21.6061 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 293.9914 2.5014 117.529 &lt; 2e-16 CPI -2.9350 0.4153 -7.068 3.89e-11 HDI -283.8765 5.0064 -56.703 &lt; 2e-16 *** — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 8.178 on 170 degrees of freedom Multiple R-squared: 0.9782, Adjusted R-squared: 0.9779 F-statistic: 3806 on 2 and 170 DF, p-value: &lt; 2.2e-16 Pr(&gt;|t|) of the coefficients is close to 0 (significatives) - see *** in each row of the table. F-value is 3806 (must to be greater than 1). p-value is close to 0. degrees of freedom: The number of independent pieces of information used in the estimation of a parameter. From the algebaic point of view is the theal number of equations used from the data. (see Grados de libertad) Multiple R-squared is used for evaluating how well your model fits the data. 97% of the variability in HDI.Rank is explained by CPI+HDI. (Multiple R squared) measures the amount of variation in the response variable that can be explained by the predictor variables. Adjusted Rsquared adds penalties for the number of predictors in the model. Therefore it shows a balance between the most parsimonious model, and the best fitting model ( ratio between the number of observations and the predictors). If you have a large difference between your multiple and your adjusted Rsquared that indicates you may have overfit your model. 5.4 Improving the models Trying a more complicated model Polynomial regression. Polynomial ortogonal regression. Non linear regression. etc. Using the previous example: Call: lm(formula = CPI ~ HDI) Coefficients: (Intercept) HDI -1.540 8.497 Computing errors: We develop a function that receives the model, the input variable, and computes the norm 2 of the vector of errors (eucliean distance): \\[\\mid\\mid \\vec{e} \\mid\\mid= \\sqrt{\\sum (CPI_{i}-\\phi(HDI_{i}))^2} \\] - \\(\\phi(HDI_{i}))\\) will be in the function y_aprox [1] 19.69212 Is this result coherent with residuals? [1] 2.371631e-14 We obtain the same values (taking in account rouning). In any case the norm 2 of the errors in all the points of the dataset is 19.6921162 (computed using mi.verror$norma.error). In the following we will improve the model. Before, we will examine the information computed in the building of the model (write f1$ and R will show all the possible parameters): CPI HDI 1 1.5 0.398 2 3.1 0.739 3 2.9 0.698 4 2.0 0.486 5 3.0 0.797 6 2.6 0.716 1 2 3 4 5 6 1.841949 4.739580 4.391184 2.589725 5.232432 4.544139 [1] 2.407623e-14 5.4.1 Updating the models 5.4.2 Searching the best regression Given a set of variables \\(A, B, X, Y, Z, U\\) we can test the following models, in addition to the combinations of these with the linear and polynomial models previously seen. \\(+\\) para combinaciones de variables \\(A+B\\) \\(.\\) combinación de una de las variables con todas las demás Examples: 5.4.3 Orthogonal polynomials When we approach a function using classical approach - we minimize the norm 2 of the errors - we need to solve this linear equation system: For this proposal we use in R language ``I(x^k)`` to add a term with k degree. In orthogonal approach the system will be: For instance using Legendre family: In R language we will use ``poly(x,k)`` to use orthogonal polynomials of k degree. Trying a polynomial grade of \\(n\\) for \\(\\phi(x)\\). Call: lm(formula = CPI ~ HDI + I(HDI^2)) Coefficients: (Intercept) HDI I(HDI^2) 9.552 -30.051 30.785 Call: lm(formula = CPI ~ HDI + I(HDI^2) + I(HDI^3)) Coefficients: (Intercept) HDI I(HDI^2) I(HDI^3) -7.116 59.605 -119.817 80.060 Call: lm(formula = CPI ~ HDI + I(HDI^2) + I(HDI^3) + I(HDI^4)) Coefficients: (Intercept) HDI I(HDI^2) I(HDI^3) I(HDI^4) -0.08463 7.38593 18.62201 -75.89626 63.32330 Call: lm(formula = CPI ~ poly(HDI, 2)) Coefficients: (Intercept) poly(HDI, 2)1 poly(HDI, 2)2 4.052 19.568 11.859 Call: lm(formula = CPI ~ poly(HDI, 3)) Coefficients: (Intercept) poly(HDI, 3)1 poly(HDI, 3)2 poly(HDI, 3)3 4.052 19.568 11.859 5.159 Call: lm(formula = CPI ~ poly(HDI, 4)) Coefficients: (Intercept) poly(HDI, 4)1 poly(HDI, 4)2 poly(HDI, 4)3 poly(HDI, 4)4 4.0520 19.5681 11.8590 5.1587 0.6239 For each model found, calculate in R the errors made, the accuracy, etc. The parameter $ Pr (&gt;F)$ is the probability that rejecting null hypothesis (the most complex model does not fit better than the simplest model) could be an error. 5.5 Model validation It is important to evaluate how the model adjusts the point cloud. There are many ways to make this assessment. Usually statisticians examine diagnostic plots after constructing the model. Most evaluation models focus on residuals If we call \\(\\phi(x)\\) to the regression linear function, at each point the error (residual) function is \\[e (x_i)=\\mid \\phi(x_i)-y_i\\mid \\] 5.5.1 Residuals We assume these errors: They must have a zero average. If this is not the case, the bias must be measured. More points may need to be included in the data cloud. Errors must be uniformly distributed. We must wait for the residues to be uniformly distributed without patterns that detect anomalies. A first way would be to face the waste with the adjustment and the points should be around \\(y=0\\). Facing residues with the adjustment and points should be around y = 0. 5.5.2 Other study of residuals Another possibility would be to confront the residues with respect to the X-axis values. Points should be around \\(y=0\\). Draw the point cloud next to the adjustment. Useful commands: predict, fitted. Estimators with residuals or draw the residuals 1 2 3 4 5 6 2.468377 4.156869 3.575200 2.218723 5.156484 3.817822 1 2 3 4 5 6 2.468377 4.156869 3.575200 2.218723 5.156484 3.817822 1 2 3 4 5 6 -0.9683773 -1.0568692 -0.6752004 -0.2187232 -2.1564845 -1.2178223 [1] -2.053592e-17 5.6 Proyecto - regresion EconomistData Improving regression with EconomistData dataSet. (Intercept) CPI HDI 293.991367 -2.935028 -283.876464 Call: lm(formula = HDI.Rank ~ CPI + HDI) Coefficients: (Intercept) CPI HDI 293.991 -2.935 -283.876 1 2 3 4 5 6 -4.60599167 -5.10807191 8.66598738 -2.15734861 -13.93673984 2.89525520 7 8 9 10 11 12 -2.44188161 -0.86747447 2.76622608 -0.69690560 -8.21829193 1.87144180 13 14 15 16 17 18 1.01589083 -7.33669193 -2.46410654 3.02896845 11.92180933 10.43680829 19 20 21 22 23 24 -2.51782785 21.60610822 4.98504226 1.88724332 -10.43701937 -10.22317210 25 26 27 28 29 30 -13.70985035 -0.36041659 0.17465976 -4.69679020 16.39312099 -11.16467728 31 32 33 34 35 36 -12.00982993 -0.33860867 12.59786618 4.54001917 -1.02878983 -19.93264142 37 38 39 40 41 42 0.77185089 0.30085876 -3.98371883 -10.28558786 -10.37611145 -6.04445802 43 44 45 46 47 48 -8.52410046 3.66733567 1.88059785 7.79734086 7.23059067 1.32426390 49 50 51 52 53 54 10.33665832 12.32046646 0.02684822 -10.58090997 -4.17033750 -9.01963378 55 56 57 58 59 60 5.97694164 -2.49937369 12.14645508 3.50934743 1.12369775 -4.60293950 61 62 63 64 65 66 6.03241096 -10.59463476 7.87830014 -12.17430366 -11.32591264 10.04000583 67 68 69 70 71 72 -1.82840115 12.06249696 -1.41606338 -10.84704151 -0.70956622 4.38764699 73 74 75 76 77 78 13.96549663 2.63386986 5.95289808 -7.21882433 -7.88590197 -10.43672650 79 80 81 82 83 84 1.07241621 -2.73844536 12.36203288 -6.57882450 -0.04118425 14.24613472 85 86 87 88 89 90 -8.50502517 -1.74412350 12.75621810 -0.78303728 -10.14369399 -5.86908897 91 92 93 94 95 96 4.02564135 -9.20391934 -8.37519745 -9.96329461 2.07726908 2.07442105 97 98 99 100 101 102 -0.63569608 -4.34075557 9.98854683 -8.86163679 -5.36998964 0.64873945 103 104 105 106 107 108 4.63934387 -9.60140437 9.75604064 9.30454080 -9.38249946 11.20383176 109 110 111 112 113 114 -10.65856881 -3.47649222 16.34554816 -0.51888391 -6.54203159 -3.34876745 115 116 117 118 119 120 9.54944141 -16.91023903 -0.64800176 1.11939457 9.22967666 1.41994197 121 122 123 124 125 126 -8.28864876 -2.24787220 8.24354415 1.79816613 8.45614978 -8.05714531 127 128 129 130 131 132 -5.43163410 0.04217940 -11.71774620 -6.62056839 8.46677827 13.79651559 133 134 135 136 137 138 11.57122268 11.76225118 -6.49236455 -0.18048754 -7.85640169 -8.46672378 139 140 141 142 143 144 -11.27130400 4.84791253 -10.49828222 -4.72790498 0.71020644 16.76178084 145 146 147 148 149 150 -3.55065523 8.85286350 -4.47372418 11.84971387 3.29073539 -0.07127899 151 152 153 154 155 156 -0.82266968 12.04963221 12.07221208 -0.89984945 12.59147817 0.57155095 157 158 159 160 161 162 -1.46103690 4.95625185 -6.85316332 9.30751297 8.76540081 7.44393283 163 164 165 166 167 168 0.66160421 -4.29485930 -3.87368502 -10.82508278 -3.17089656 7.66949195 169 170 171 172 173 16.43301085 -6.76561190 -2.67688090 1.46760353 -7.79675397 In the following, we check some models. 5.7 Proyecto - Analysing Boston dataset This dataset stores the Housing Values in Suburbs of Boston. 5.7.1 Fit a simple linear regression \\[medv=\\phi(lstat)=a_0+a_1\\, lstat\\] lstat: lower status of the population (percent). medv: median value of owner-occupied homes in $1000s. The model founded is: \\[medv=\\phi(lstat)=34.55-0.95\\, lstat\\] 5.7.2 Analyzing the model For more detailed information: For more information we can use names with the linear model: Confidence interval for the coeffient estimates: Visualizing the dataset+model: ## Visualizing the linear model # Computing residuals residuals: rstudent:return the studentized residuals Some plots regarding residuals, etc… On the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues function which.max(): function identifies the index of the largest element of a which.max() vector. In this case, it tells us which observation has the largest leverage statistic. 5.7.3 Multiple regression Fitting a model trying the regression with two input variables. lstat: lower status of the population (percent). medv: median value of owner-occupied homes in $1000s. age: age proportion of owner-occupied units built prior to 1940. \\[medv=\\phi(lstat,age)=a_0+a_1\\, lstat+ +a_2\\, age\\] Data set contains 13 variables, how to select the best variables?: Facing the output variable against the rest of variables Analyzing the summary of the model. 5.7.4 Interaction terms 5.7.5 Non-linear transformations of the predictors The p-value associated with the quadratic term suggests that lm.fit5 is an improved model. 5.7.6 Using anova Fist, we compare the model: linear (lm.fit) and quadratic (lm.fit5). If the models are represented, we could extract some important ideas: We can see a non-linearity in the relationship between medv and lstat. Now, we will use anova() (Analysis of Variance Table) performs a hypothesis test comparing the models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Interpretation: The null hypothesis says that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. See the F-statistic and the p-value associated. This provides very clear evidence that the model containing the predictors \\(lstat\\) and \\(lstat^2\\) is better than the model with the predictor \\(lstat\\). 5.8 Poyecto - evaluación de cursos - variables categóricas Regresión múltiple: \\[y=f(x_1,x_2,\\dots)\\] \\(y\\) es una variable de salida numérica \\(x_1\\) es variable 1 de entrada numérica o categórica \\(x_1\\) es variable 2 de entrada numérica o categórica, … 5.8.1 EDA - exploratory data analysis Vamos a explorar las relaciones en un .csv con relaciones entre las puntuaciones de la evaluación de la enseñanza dadas por los estudiantes y las puntuaciones estéticas, de edad, etc. de los profesores. Recordatorio de fases de EDA: Observar los valores Preprocesamiento Estimar estadísticos Visualizaciones ID score bty_avg age gender 1 1 4.7 5 36 female 2 2 4.1 5 36 female 3 3 3.9 5 36 female 4 4 4.8 5 36 female 5 5 4.6 3 59 male 6 6 4.3 3 59 male ID score bty_avg age Min. : 1.0 Min. :2.300 Min. :1.667 Min. :29.00 1st Qu.:116.5 1st Qu.:3.800 1st Qu.:3.167 1st Qu.:42.00 Median :232.0 Median :4.300 Median :4.333 Median :48.00 Mean :232.0 Mean :4.175 Mean :4.418 Mean :48.37 3rd Qu.:347.5 3rd Qu.:4.600 3rd Qu.:5.500 3rd Qu.:57.00 Max. :463.0 Max. :5.000 Max. :8.167 Max. :73.00 gender Length:463 Class :character Mode :character Nota: Nuevas funciones de tidyverse en última sección. Rows: 463 Columns: 5 $ ID 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… $ score 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, 4.5, 4.… $ bty_avg 5.000, 5.000, 5.000, 5.000, 3.000, 3.000, 3.000, 3.333, 3.333,… $ age 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40, 40, 40… $ gender “female”, “female”, “female”, “female”, “male”, “male”, “male”… Table 5.1: Data summary Name Piped data Number of rows 463 Number of columns 5 _______________________ Column type frequency: character 1 numeric 4 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace gender 0 1 4 6 0 2 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ID 0 1 232.00 133.80 1.00 116.50 232.00 347.5 463.00 ▇▇▇▇▇ score 0 1 4.17 0.54 2.30 3.80 4.30 4.6 5.00 ▁▁▅▇▇ bty_avg 0 1 4.42 1.53 1.67 3.17 4.33 5.5 8.17 ▃▇▇▃▂ age 0 1 48.37 9.80 29.00 42.00 48.00 57.0 73.00 ▅▆▇▆▁ ID score bty_avg age gender 1 284 4.0 1.667 34 female 2 336 3.1 1.667 60 male 3 406 5.0 2.833 57 male 4 101 4.4 4.333 48 male 5 111 3.5 4.333 57 female mean_bty_avg mean_score median_bty_avg median_score 1 4.417844 4.17473 4.333 4.3 Table 5.1: Data summary Name Piped data Number of rows 463 Number of columns 2 _______________________ Column type frequency: numeric 2 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist score 0 1 4.17 0.54 2.30 3.80 4.30 4.6 5.00 ▁▁▅▇▇ bty_avg 0 1 4.42 1.53 1.67 3.17 4.33 5.5 8.17 ▃▇▇▃▂ \\(p0\\): 0-percentile: valor al cual el 0% de las observaciones son más pequeñas que él (valor mínimo por tanto) \\(p25\\): 25-percentile: valor al cual el 25% de las observaciones son más pequeñas que él … 5.8.1.1 Correlación Se establece entre variables numéricas El coeficiente de correlación es estimador de la relación lineal entre dos variables numéricas. Su valor oscila entre -1 y 1: -1: perfecta relación negativa 0: no relación +1 perfecta relación positiva correlation 1 0.1871424 relación debilmente positiva 5.8.1.2 Visualización Objetivo: visualizar relaciones entre puntuaciones, edad según el género. 5.8.2 Modelo lineal Construimos un primer modelo lineal univariable (puntuación frente a media de las edades): Call: lm(formula = score ~ bty_avg, data = evals5) Coefficients: (Intercept) bty_avg 3.88034 0.06664 Call: lm(formula = score ~ bty_avg, data = evals5) Residuals: Min 1Q Median 3Q Max -1.9246 -0.3690 0.1420 0.3977 0.9309 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.88034 0.07614 50.96 &lt; 2e-16 bty_avg 0.06664 0.01629 4.09 5.08e-05 — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 0.5348 on 461 degrees of freedom Multiple R-squared: 0.03502, Adjusted R-squared: 0.03293 F-statistic: 16.73 on 1 and 461 DF, p-value: 5.083e-05 5.8.3 Interacción de modelos Se usa el operador de construcción de modelos :. \\[y=f_1(x_1,x_2)\\] En nuestro Call: lm(formula = score ~ age * gender, data = evals5) Coefficients: (Intercept) age gendermale age:gendermale 4.88299 -0.01752 -0.44604 0.01353 Call: lm(formula = score ~ age * gender, data = evals5) Residuals: Min 1Q Median 3Q Max -1.86453 -0.34815 0.09863 0.40661 0.96327 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.882989 0.205210 23.795 &lt; 2e-16 age -0.017523 0.004472 -3.919 0.000103 gendermale -0.446044 0.265407 -1.681 0.093520 . age:gendermale 0.013531 0.005531 2.446 0.014803 * — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 0.5314 on 459 degrees of freedom Multiple R-squared: 0.05138, Adjusted R-squared: 0.04518 F-statistic: 8.288 on 3 and 459 DF, p-value: 2.227e-05 Explicación: intercept y age son las variables para el género femenino (female alfabéticamente antes que male) gendermale y age:gendermale son los desplazamientos del intercept y de slope de los profesores (male) respecto a los valores de intercept y de slope de las profesoras. Regresión obtenida: \\[y=b_0+b_1 \\cdot age + b_2 \\cdot is\\_male(x) + b_3 \\cdot is\\_male(x)\\] - \\(b_0\\) es (Intercept) - \\(b_1\\) es age - \\(b_2\\) es gendermale - \\(b_3\\) es age:gendermale La interacción estudia si el efecto asociado de una variable depende del valor de otra variable. en nuestro caso, la respuesta es que si existe: existe una da diferencia en las pendientes de la edad de los instructores masculinos con respecto a los femeninos 5.8.4 Multiple Regression Table 5.2: Data summary Name Piped data Number of rows 463 Number of columns 3 _______________________ Column type frequency: character 1 numeric 2 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace gender 0 1 4 6 0 2 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist score 0 1 4.17 0.54 2.3 3.8 4.3 4.6 5 ▁▁▅▇▇ age 0 1 48.37 9.80 29.0 42.0 48.0 57.0 73 ▅▆▇▆▁ correlation 1 -0.107032 modelo con la edad Call: lm(formula = score ~ age, data = evals5) Coefficients: (Intercept) age 4.461932 -0.005938 Call: lm(formula = score ~ age + bty_avg, data = evals5) Coefficients: (Intercept) age bty_avg 4.054732 -0.003059 0.060656 Call: lm(formula = score ~ age, data = evals5) Residuals: Min 1Q Median 3Q Max -1.9185 -0.3531 0.1172 0.4172 0.8825 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.461932 0.126778 35.195 &lt;2e-16 ** age -0.005938 0.002569 -2.311 0.0213 — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 0.5413 on 461 degrees of freedom Multiple R-squared: 0.01146, Adjusted R-squared: 0.009311 F-statistic: 5.342 on 1 and 461 DF, p-value: 0.02125 Call: lm(formula = score ~ age + bty_avg, data = evals5) Residuals: Min 1Q Median 3Q Max -1.9427 -0.3474 0.1293 0.3957 0.9478 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 4.054732 0.169865 23.870 &lt; 2e-16 age -0.003059 0.002664 -1.148 0.251396 bty_avg 0.060656 0.017098 3.548 0.000429 — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 0.5347 on 460 degrees of freedom Multiple R-squared: 0.03778, Adjusted R-squared: 0.0336 F-statistic: 9.031 on 2 and 460 DF, p-value: 0.0001422 5.8.5 Validando el modelo Un test de hipótesis consiste en una prueba entre dos hipótesis contrapuestas Hipótesis nula \\(H_0\\) versus una hipótesis alternativa Generalmente, la hipótesis nula es una afirmación de que “no hay efecto” o “no hay diferencia de interés”. En muchos casos, la hipótesis nula representa el statu quo o una situación en la que no ocurre nada interesante. Además, generalmente la hipótesis alternativa es la afirmación que el experimentador o investigador quiere establecer o encontrar pruebas que la respalden. Se considera una hipótesis “contraria” a la hipótesis nula \\[y=a+bx\\] \\(H_0\\): no hay modelo, \\(b=0\\) \\(H_a\\): hay modelo, \\(b\\neq0\\) umbral \\(0.05\\), Un estadístico de prueba (test statistic) es una fórmula de estimación puntual/estadística de muestra que se utiliza para la comprobación de hipótesis: t-test statistic p-value es la probabilidad de obtener un estadístico de prueba igual o más extremo que el estadístico de prueba observado suponiendo que la hipótesis nula \\(H_0\\) es verdadera. 5.9 Anexo: R avanzado 5.9.1 Más de tidyverse [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) [sample_n](https://dplyr.tidyverse.org/reference/sample_n.html?q=sample_n#null) 5.9.2 Skimr package [skim](https://www.datanovia.com/en/blog/display-a-beautiful-summary-statistics-in-r-using-skimr-package/) skim() can handle data that has been grouped using dplyr::group_by. 5.10 Proyecto - Predicción de riesgos Importa en R el dataset riesgos.csv La compañía de seguros quiere tener un modelo para predecir los gastos médicos de los asegurados. Realiza un análisis del dataset y desarrolla uno o varios modelos que contesten a lo siguiente. Entregar en el book. Analiza la estructura del dataset. ¿Qué tipo datos tenemos? Plantea los problemas que podemos tener al trabajar con este dataset. Analiza estadísticamente los atributos. Detecta normalidad, sesgos, outliers, etc. Dibuja el atributo gastos con un histograma. ¿Qué conocimiento extraes de la información visualizada? Obten la matriz de correlación entre los atributos del dataset. ¿Qué atributos parecen estar más y menos relacionados? (cor). Visualiza las relaciones entre los atributos - scatterplot (plot, pairs, pairs.panels). Plantea un modelo lineal m1 de regresión entre gastos y otra variable (la que pienses mejor modela los gastos médicos de los asegurados). Intenta un modelo m2 usando funciones polinómicas. Evalua la eficiencia de los modelos (summary). Extrae toda la información acerca de la validez de los dos modelos creados. Mejora el modelo usando regresión generalizada. Crea un modelo m3 teniendo en cuenta todas las variables. Analiza qué variables son significativas. Mira la eficiencia del nuevo modelo. Usa anova para vez que modelo de los creados es más interesante. "],["series-temporales.html", "Capítulo 6 Series Temporales 6.1 COVID predictions 6.2 Introduction 6.3 Time series analysis 6.4 Time Series in R 6.5 Time Series Components 6.6 Other packages 6.7 Basic forecasting methods 6.8 Example: Temperature forecast in Malaga - Basic forecasting methods 6.9 Example: Temperature forecast in Malaga - Decomposing Time Series 6.10 Forecasting using forecast package 6.11 Example: Temperature forecast in Malaga - Exponential Smoothing 6.12 ARIMA methods 6.13 Autoregressive models - AR(p) 6.14 Moving average models 6.15 Seasonal differencing 6.16 Non-seasonal ARIMA models 6.17 Auto-ARIMA model 6.18 Example: Temperature forecast in Malaga - Modelos ARIMA 6.19 References", " Capítulo 6 Series Temporales 6.1 COVID predictions P A G T A G N A: Philippine COVID-19 Case Forecasting Web Application Andre Calero covid19forecast Note: Install TTR, forecast packages 6.2 Introduction Forecasting is one of the principal applications in data science. See Forecasting: Principles and Practice Tell us what the future holds, so we may know that you are gods. (Isaiah 41:23) Forecasting is the set of techniques modelling how to predict the future as accurately as possible, given all of the information available, including historical data and any future data. These methods try to extrapolate trend and seasonal patterns Applications of Forecasting Medicine, epidemiology, planning for the economy features of a country, financial institutions, pocily organizations, weather, global temperature changes scheduling in bussiness, marketing, predicting sales in a ship, finance and risk management, prediction of population in a country, in a region, seismic recordings planning th stock of product in a online shop, deciding whether to build another power generation plant, scheduling personal in a call centre depending of the number of calls, deciding capital investments, See The statistical forecasting perspective 6.3 Time series analysis The study of adjacent points using conventional statistical methods have problems. Conventional statistical methods are dependent on the assumption that these adjacent observations are independent and identically distributed. The temporal data appears temporal correlations and ad-hoc techniques have been developed to treat these data. Basic steps Problem study Data collection Data analysis Model selection and fitting Model validation Forecasting model deployment Monitoring forecasting model performance 6.4 Time Series in R 6.4.1 Importing time series To analyse your time series data we need to read it into R, and to plot the time series. You can read data into R using scan(), which assumes that your data for successive time points is in a simple text file with one column. The file ts_k1.dat contains data about average business sales of a product. [1] 60 43 67 50 56 42 The next step is to store the data in a time series object in R. A time series is a list of numbers with temporal information stored as a ts object in R There is a lot of packages and functions to deal with time series. Firstly, we will use ts function, which is used to create time-series objects. See ts-objects. Time Series: Start = 1 End = 42 Frequency = 1 [1] 60 43 67 50 56 42 50 65 68 43 65 34 47 34 49 41 13 35 53 56 16 43 69 59 48 [26] 59 86 55 68 51 33 49 67 77 81 67 71 81 68 70 77 56 If the data set has values for periods less than one year, for example, monthly or quarterly, one can specify the number of times that data was collected per year by using the ‘frequency’ parameter in the ts function. Dataset of unemployment in a city: Jan Feb Mar Apr May Jun Jul Aug Sep Oct 1980 26.663 23.598 26.931 24.740 25.806 24.364 24.477 23.901 23.175 23.227 1981 21.439 21.089 23.709 21.669 21.752 20.761 23.479 23.824 23.105 23.110 1982 21.937 20.035 23.590 21.672 22.222 22.123 23.950 23.504 22.238 23.142 1983 21.548 20.000 22.424 20.615 21.761 22.874 24.104 23.748 23.262 22.907 1984 22.604 20.894 24.677 23.673 25.320 23.583 24.671 24.454 24.122 24.252 1985 23.287 23.049 25.076 24.037 24.430 24.667 26.451 25.618 25.014 25.110 1986 23.798 22.270 24.775 22.646 23.988 24.737 26.276 25.816 25.210 25.199 1987 24.364 22.644 25.565 24.062 25.431 24.635 27.009 26.606 26.268 26.462 1988 24.657 23.304 26.982 26.199 27.210 26.122 26.706 26.878 26.152 26.379 1989 24.990 24.239 26.721 23.475 24.767 26.219 28.361 28.599 27.914 27.784 1990 26.217 24.218 27.914 26.975 28.527 27.139 28.982 28.169 28.056 29.136 1991 26.589 24.848 27.543 26.896 28.878 27.390 28.065 28.141 29.048 28.484 1992 27.132 24.924 28.963 26.589 27.931 28.009 29.229 28.759 28.405 27.945 1993 26.076 25.286 27.660 25.951 26.398 25.565 28.865 30.000 29.261 29.012 Nov Dec 1980 21.672 21.870 1981 21.759 22.073 1982 21.059 21.573 1983 21.519 22.025 1984 22.084 22.991 1985 22.964 23.981 1986 23.162 24.707 1987 25.246 25.180 1988 24.712 25.688 1989 25.693 26.881 1990 26.291 26.987 1991 26.634 27.735 1992 25.912 26.619 1993 26.992 27.897 ts_f1.dat dataset contains monthly unemployment rates in a city. Jan Feb Mar Apr May Jun Jul 1987 1664.81 2397.53 2840.71 3547.29 3752.96 3714.74 4349.61 1988 2499.81 5198.24 7225.14 4806.03 5900.88 4951.34 6179.12 1989 4717.02 5702.63 9957.58 5304.78 6492.43 6630.80 7349.62 1990 5921.10 5814.58 12421.25 6369.77 7609.12 7224.75 8121.22 1991 4826.64 6470.23 9638.77 8821.17 8722.37 10209.48 11276.55 1992 7615.03 9849.69 14558.40 11587.33 9332.56 13082.09 16732.78 1993 10243.24 11266.88 21826.84 17357.33 15997.79 18601.53 26155.15 Aug Sep Oct Nov Dec 1987 3566.34 5021.82 6423.48 7600.60 19756.21 1988 4752.15 5496.43 5835.10 12600.08 28541.72 1989 8176.62 8573.17 9690.50 15151.84 34061.01 1990 7979.25 8093.06 8476.70 17914.66 30114.41 1991 12552.22 11637.39 13606.89 21822.11 45060.69 1992 19888.61 23933.38 25391.35 36024.80 80721.71 1993 28586.52 30505.41 30821.33 46634.38 104660.67 6.4.2 Displaying time series Let us make a basic plot of the time series data (for the two first datasets): In the second one, there seems to be seasonal variation in the number of unemployees per month: There is a peak every summer, and a trough every winter. The plot for the third dataset is the following: The size of the seasonal fluctuations and random fluctuations seem to increase with the level of the time series. In these cases it is reasonable to transform the time series by calculating the natural logarithm of the original data. See time plots. 6.5 Time Series Components A time series has usually a trend component and an irregular component and, if it turns out to be a seasonal time series, a seasonal component as well. See Time series patterns: Trend, Seasonal, Cyclic. 6.5.1 Seasonal plot 6.5.2 Subseries plot The mean is indicated by horizontal lines. See Time series graphics: Scatterplots, lag plots, etc.. 6.6 Other packages base R: times series class named ts. too restrictive, functiones associated limited zoo, xts packages: to represent time series special structure for time series many functions interestings Exercise Make an .rmd with a mini-tutorial on the use of the zoo and xts packages, applying them to a time series (dataset with data from Spain) that you search on the web about electricity consumption, product consumption, etc. Hand in the homework of the time series topic. 6.7 Basic forecasting methods We denote \\(\\hat{y}_{T+h\\mid T}\\) the value of the variable \\(y\\) in the time \\(T+h\\) based on \\(y_1, \\ldots, y_T\\). The residual will be computed by: \\[e_T=y_T-\\hat{y}_{T}\\] The forecast error is: \\[e_t(T)=y_t-\\hat{y}_{t}(t-T)\\] 6.7.1 Average method \\[\\hat{y}_{T+h\\mid T}=\\frac{(y_1, \\ldots, y_T)}{T}\\] 6.7.2 Naïve method The prediction will be the last \\(Y_T\\). \\[\\hat{y}_{T+h\\mid T}= y_T\\] 6.7.3 Seasonal naïve method The prediction will be the same that the value of the variable in the last year in the same time. \\[\\hat{y}_{T+h\\mid T}= y_{T+h-m(k+1)}\\] (where \\(m\\) is the season in the year and \\(k\\) is the truncated value of \\((h-1)/m\\)) 6.7.4 Drift method It is based on naïve method where the amount of change in time (the drift) is set as the average change in the historical data: \\[\\hat{y}_{_{T+h\\mid T}}=y_{_{T}}+\\frac{h}{T-1}\\displaystyle\\sum_{t=2}^T(y_t-t_{t-1})=y_{_{T}}+h\\frac{y_{_T}-y_{_{1}}}{T-1}\\] See all the methods in the same picture - Forecasts for quarterly beer production. Note: It is necessary the package fpp2 to execute the code in this page. 6.8 Example: Temperature forecast in Malaga - Basic forecasting methods To analyse the evolution of minimum temperatures since 2001 in Malaga. First of all we obtain a time-series object with the function ts, as our data are divided into months, we set the parameter frecuency to 12 and we indicate that we start in January 2001. After this, we draw the object obtained: We decompose the data: Seasonal component, Trend component, Cycle component y residual. Let’s see if it is seasonal, let’s look at the data by year: As we can see, all the years are identical, which indicates that the temperatures repeat annual and monthly cycles. With these graphs we can see that our series is seasonal, at the beginning of the year it is down, during the year it rises and from August onwards it starts to fall. This cycle is repeated every year. With this last graph we see the subseries by months. From it we see how each month moves in the same range. With this we confirm that it is a seasonal series. 6.8.1 Average method With this method we see that the prediction is basically an average. 6.8.2 Naïve method This method uses the last value of \\(y_t\\). 6.8.3 Seasonal naïve method This method uses the last year to predict. As we will see in the graph, it generates what has happened in the last year. 6.8.4 Drift method This method is based on the previous one and that the amount of change over time is set as the average change in the historical data. 6.8.5 Conclusion My data series as I have mentioned is for minimum temperatures, so these methods are too simple to predict well what will happen with the temperature in the following years. The only model that could be used is the Seasonal naïve method which uses the last year to see what will happen next year. But we still need to use better methods. 6.8.6 Decomposing Time Series Decomposing a time series means separating it into its constituent components: trend, cycles, irregular, a seasonal component and a remainder component. If \\(S_t\\) is the seasonal component, \\(T_t\\) is the trend-cycle component, and \\(R_t\\) is the remainder component, we can consider an additive decomposition or a multiplicative decomposition as follows: Additive decomposition: \\[y_t=S_t+T_t+R_t\\] Multiplicative decomposition: \\[y_t=S_t\\times T_t\\times R_t\\] Addditve decomposition is adequate when the seasonal part and the trend-cycle does not vary too much. Multiplicative decomposition is adequate when the amplitude of the seasonal part increases or decreases with the average level of the time series. Multiplicative decomposition can be transformed into an additive one just by applying logarithms: \\[\\log y_t= \\log S_t + \\log T_t +\\log R_t\\] 6.8.7 Additive decomposition We describe how decompose a time series considering additive decomposition. Note: Multiplicative decomposition is similar, except that the subtractions are replaced by divisions. 6.8.8 Non-Seasonal Data A non-seasonal time series consists of a trend component and an irregular component. To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a smoothing method, such as calculating the Simple Moving Average (SMA) of the time series. The SMA() function in the TTR package is used to smooth time series data using a simple moving average: \\[\\hat{y_{_t}}=\\frac{1}{m}\\displaystyle\\sum_{j=-k}^k y_{_{t+j}}\\] To compute the average eliminates randomness in the data. To specify the order of the simple moving average, the parameter \\(n\\) will be used. For example, to calculate a simple moving average of order 2, we set \\(n=2\\) in the SMA() function. There still appears to be quite a lot of random fluctuations in the time series smoothed using a simple moving average of order 2. Thus, to estimate the trend component more accurately, we might want to try smoothing the data with a simple moving average of a higher order. This takes a little bit of trial-and-error, to find the right amount of smoothing. With MA of order 5, and order 9 the trend starts to be clearly identified in the plots. 6.8.9 Seasonal Data A seasonal time series consists of a trend component, a seasonal component and an irregular component. We can use the decompose function in R which estimates: the trend-cycle component using MA \\(\\hat{y_t}\\), the detrended serie \\(y_t-\\hat{y_t}\\) the seasonal component \\(S_t\\), computing the average of the detrended values for that season, the random components \\(R_t=y_t-\\hat{y_t}-S_t\\). To compute the detrended serie in additive decomposition: \\(y_t-T_t =S_t+R_t\\) Given the time series ts.n1, we will decompose the time serie using the following: The estimated seasonal factors are given for the months January-December, and are the same for each year. The different parts of the decomposed series can be managed as usual: 6.8.10 Seasonal Adjusting As we have explained, if you have a seasonal time series that can be described using an additive model, you can seasonally adjust the time series by estimating the seasonal component, and subtracting the estimated seasonal component from the original time series. Jan Feb Mar Apr May Jun Jul Aug 1980 27.34019 25.68096 26.06848 25.54168 25.55435 24.51726 23.02095 22.73641 1981 22.11619 23.17196 22.84648 22.47068 21.50035 20.91426 22.02295 22.65941 1982 22.61419 22.11796 22.72748 22.47368 21.97035 22.27626 22.49395 22.33941 1983 22.22519 22.08296 21.56148 21.41668 21.50935 23.02726 22.64795 22.58341 1984 23.28119 22.97696 23.81448 24.47468 25.06835 23.73626 23.21495 23.28941 1985 23.96419 25.13196 24.21348 24.83868 24.17835 24.82026 24.99495 24.45341 1986 24.47519 24.35296 23.91248 23.44768 23.73635 24.89026 24.81995 24.65141 1987 25.04119 24.72696 24.70248 24.86368 25.17935 24.78826 25.55295 25.44141 1988 25.33419 25.38696 26.11948 27.00068 26.95835 26.27526 25.24995 25.71341 1989 25.66719 26.32196 25.85848 24.27668 24.51535 26.37226 26.90495 27.43441 1990 26.89419 26.30096 27.05148 27.77668 28.27535 27.29226 27.52595 27.00441 1991 27.26619 26.93096 26.68048 27.69768 28.62635 27.54326 26.60895 26.97641 1992 27.80919 27.00696 28.10048 27.39068 27.67935 28.16226 27.77295 27.59441 1993 26.75319 27.36896 26.79748 26.75268 26.14635 25.71826 27.40895 28.83541 Sep Oct Nov Dec 1980 22.48338 22.45176 22.78177 22.24682 1981 22.41338 22.33476 22.86877 22.44982 1982 21.54638 22.36676 22.16877 21.94982 1983 22.57038 22.13176 22.62877 22.40182 1984 23.43038 23.47676 23.19377 23.36782 1985 24.32238 24.33476 24.07377 24.35782 1986 24.51838 24.42376 24.27177 25.08382 1987 25.57638 25.68676 26.35577 25.55682 1988 25.46038 25.60376 25.82177 26.06482 1989 27.22238 27.00876 26.80277 27.25782 1990 27.36438 28.36076 27.40077 27.36382 1991 28.35638 27.70876 27.74377 28.11182 1992 27.71338 27.16976 27.02177 26.99582 1993 28.56938 28.23676 28.10177 28.27382 The seasonal variation has been removed from the seasonally adjusted time series. The seasonally adjusted time series now just contains the trend component and an irregular component. 6.9 Example: Temperature forecast in Malaga - Decomposing Time Series We have seen that the time series we use is seasonal so we are going to use the method of Additive decomposition. 6.9.1 Additive decomposition A seasonal time series consists of a trend component, a seasonal component and an irregular component. To decompose it we are going to use decompose. 6.9.2 Seasonal Adjusting If we have a seasonal time series that can be described using an additive model, we can seasonally adjust the time series by estimating the seasonal component and subtracting the estimated seasonal component from the original time series. Jan Feb Mar Apr May Jun Jul Aug 2001 15.21772 14.62312 16.29322 15.00204 13.99887 15.21996 13.84395 14.84199 2002 16.01772 15.02312 14.69322 13.70204 13.59887 14.21996 13.74395 13.04199 2003 14.01772 14.12312 15.39322 14.10204 14.89887 15.81996 14.34395 14.64199 2004 16.31772 15.32312 14.59322 13.20204 12.79887 14.71996 14.54395 14.44199 2005 12.51772 11.82312 14.29322 14.50204 15.29887 14.71996 15.34395 13.94199 2006 14.51772 17.42312 17.59322 16.30204 16.09887 14.61996 14.84395 14.04199 2007 13.31772 16.52312 14.19322 14.40204 14.89887 14.71996 14.74395 14.64199 2008 15.31772 17.12312 15.59322 14.70204 14.99887 14.61996 16.04395 15.84199 2009 13.91772 14.82312 14.49322 13.80204 15.34887 15.91996 16.04395 14.84199 2010 15.41772 15.92312 14.89322 15.40204 14.39887 14.51996 15.74395 15.94199 2011 15.71772 14.42312 15.09322 16.20204 15.79887 14.41996 14.34395 14.94199 2012 13.91772 11.82312 13.89322 14.60204 14.99887 16.41996 15.14395 15.84199 2013 16.11772 13.62312 14.89322 15.00204 13.99887 13.11996 13.54395 14.74199 2014 16.01772 15.42312 14.89322 16.60204 15.59887 15.31996 14.44395 14.94199 2015 14.86772 14.22312 15.09322 16.30204 16.19887 14.71996 16.74395 16.54199 2016 16.11772 15.92312 14.39322 15.60204 14.99887 15.01996 15.54395 15.74199 2017 14.21772 16.22312 14.79322 15.20204 15.19887 16.21996 14.84395 14.84199 2018 15.21772 13.82312 14.89322 14.10204 14.09887 14.41996 14.34395 15.34199 Sep Oct Nov Dec 2001 14.85424 14.82508 13.62900 14.95081 2002 13.85424 14.52508 16.72900 16.05081 2003 14.95424 14.42508 14.92900 14.25081 2004 14.25424 13.92508 13.82900 14.35081 2005 14.05424 14.62508 13.72900 15.05081 2006 14.55424 15.72508 16.52900 13.95081 2007 14.85424 14.82508 14.82900 15.35081 2008 14.95424 14.72508 12.82900 13.65081 2009 15.15424 15.82508 16.72900 15.65081 2010 15.25424 13.82508 14.52900 15.55081 2011 14.85424 14.42508 14.12900 14.05081 2012 15.05424 14.22508 15.62900 15.25081 2013 15.45424 16.12508 14.62900 14.75081 2014 16.05424 15.92508 16.32900 14.15081 2015 15.05424 15.52508 15.52900 16.35081 2016 15.35424 15.92508 14.72900 16.25081 2017 15.15424 14.22508 14.32900 13.95081 2018 16.55424 14.32508 6.9.3 Forecasts using Exponential Smoothing Exponential Smoothing considers weigths to the values of the variables: weighted averages of past observations. The weights decrease exponentially for distant values. Taxonomy 6.9.4 Simple Exponential Smoothing The idea is to consider a weighted average between the value of the variable and the previous one: \\[\\hat{y}_{T+1\\mid T}=\\alpha y_T + (1-\\alpha)\\hat{y}_{T\\mid T-1}\\] If we have into account more past values of the variable, after some computations, the forecast will be: \\[\\hat{y}_{T+1\\mid T}=\\alpha y_T + \\alpha(1-\\alpha){y}_{T-1}+\\alpha(1-\\alpha)^2{y}_{T-2}+\\ldots \\] The parameter \\(\\alpha\\) controls the rates at which the weights decrease. The weights decrease exponentially and this is the reason of the name of the method. \\(\\alpha\\) has values between 0 and 1. \\(\\alpha\\) close to 0, more important are the longer observations -  slow learning (past observations have a large influence on forecasts). \\(\\alpha\\) close to 1, more important the more recent observation s- fast learning (that is, only the most recent values influence the forecasts). With \\(\\alpha=0\\) or \\(\\alpha=1\\) we have the extreme cases. If you have a time series that can be described using an additive model with constant level (no clear trend) and no seasonality, you can use simple exponential smoothing to make short-term forecasts. The simple exponential smoothing method provides a way of estimating the level at the current time point. The file ts_p1.dat contains total annual snowfall for a city. The mean stays constant at about 25 - roughly constant level. The random fluctuations seem to be roughly constant. Thus, we can make forecasts using simple exponential smoothing. In R, we can fit a simple exponential smoothing predictive model using the HoltWinters function. Main parameters: alpha: Smoothing is controlled by this parameter; the value of alpha lies between 0 and 1 (close to 0 mean that little weight is placed on the most recent observations when making forecasts of future values). beta=FALSE: the function will do exponential smoothing. gamma=FALSE: a non-seasonal model is fitted. seasonal: additive by default or multiplicative seasonal model. Holt-Winters exponential smoothing without trend and without seasonal component. Call: HoltWinters(x = ts.p1, beta = FALSE, gamma = FALSE) Smoothing parameters: alpha: 0.02412151 beta : FALSE gamma: FALSE Coefficients: [,1] a 24.67819 The output of HoltWinters() tells us that the estimated value of the alpha parameter is about 0.024. This is very close to zero, telling us that past observations have more influence on forecasts. The plot shows the original time series in black, and the forecasts of these values as a red line. The time series of forecasts is much smoother than the time series of the original data here. The function forecast predicts the value as the previous plot shows. As a measure of the accuracy of the forecasts, we can calculate the sum of squared errors for the in-sample forecast errors, that is, the forecast errors for the time period covered by our original time series. The sum-of-squared- errors is stored in a named element of the list variable ts.p1.forecasts called “SSE”, so we can get its value by typing: [1] 1828.855 Finally, we try with different values of \\(\\alpha\\) and the errors are computed: Holt-Winters exponential smoothing without trend and without seasonal component. Call: HoltWinters(x = ts.p1, alpha = 0.5, beta = FALSE, gamma = FALSE) Smoothing parameters: alpha: 0.5 beta : FALSE gamma: FALSE Coefficients: [,1] a 26.45644 [1] 2428.339 Holt-Winters exponential smoothing without trend and without seasonal component. Call: HoltWinters(x = ts.p1, alpha = 0.9, beta = FALSE, gamma = FALSE) Smoothing parameters: alpha: 0.9 beta : FALSE gamma: FALSE Coefficients: [,1] a 27.57778 [1] 3399.419 Note: The forecasts made by HoltWinters are stored in the list called fitted. Time Series: Start = 1901 End = 1999 Frequency = 1 xhat level 1901 23.56000 23.56000 1902 25.81900 25.81900 1903 22.25590 22.25590 1904 30.34159 30.34159 1905 24.31916 24.31916 1906 23.92392 23.92392 1907 26.16139 26.16139 1908 23.01914 23.01914 1909 30.82291 30.82291 1910 24.55629 24.55629 1911 24.15463 24.15463 1912 31.60246 31.60246 1913 24.09425 24.09425 1914 22.72242 22.72242 1915 22.97224 22.97224 1916 27.38922 27.38922 1917 25.52692 25.52692 1918 25.12469 25.12469 1919 27.49647 27.49647 1920 20.58765 20.58765 1921 24.36076 24.36076 1922 20.54408 20.54408 1923 23.96041 23.96041 1924 27.07404 27.07404 1925 20.20340 20.20340 1926 21.48734 21.48734 1927 26.88973 26.88973 1928 20.17597 20.17597 1929 30.03460 30.03460 1930 23.78446 23.78446 1931 25.64345 25.64345 1932 22.94934 22.94934 1933 22.76993 22.76993 1934 26.00099 26.00099 1935 18.53010 18.53010 1936 28.68201 28.68201 1937 23.50520 23.50520 1938 19.64852 19.64852 1939 20.53185 20.53185 1940 33.85919 33.85919 1941 26.68692 26.68692 1942 19.45369 19.45369 1943 22.69937 22.69937 1944 22.25894 22.25894 1945 22.18789 22.18789 1946 19.11179 19.11179 1947 27.30018 27.30018 1948 31.74602 31.74602 1949 23.21760 23.21760 1950 27.13476 27.13476 1951 22.14448 22.14448 1952 17.45145 17.45145 1953 28.27714 28.27714 1954 31.26771 31.26771 1955 26.75177 26.75177 1956 23.73518 23.73518 1957 25.25152 25.25152 1958 21.71315 21.71315 1959 24.68932 24.68932 1960 32.94293 32.94293 1961 23.69729 23.69729 1962 19.30773 19.30773 1963 27.52677 27.52677 1964 26.29668 26.29668 1965 27.98267 27.98267 1966 33.47027 33.47027 1967 33.78503 33.78503 1968 30.63050 30.63050 1969 28.19105 28.19105 1970 27.24511 27.24511 1971 24.68451 24.68451 1972 20.78345 20.78345 1973 26.05435 26.05435 1974 26.91443 26.91443 1975 19.98044 19.98044 1976 26.96404 26.96404 1977 24.16140 24.16140 1978 21.52314 21.52314 1979 27.48731 27.48731 1980 23.09773 23.09773 1981 20.12977 20.12977 1982 27.15898 27.15898 1983 22.03890 22.03890 1984 23.37189 23.37189 1985 22.91119 22.91119 1986 18.21212 18.21212 1987 22.10721 22.10721 1988 23.16272 23.16272 1989 22.26927 22.26927 1990 20.98293 20.98293 1991 36.38829 36.38829 1992 22.22383 22.22383 1993 22.89538 22.89538 1994 24.12354 24.12354 1995 23.12135 23.12135 1996 23.61514 23.61514 1997 26.43651 26.43651 1998 25.46765 25.46765 1999 24.85777 24.85777 6.10 Forecasting using forecast package We can make forecasts for further time points by using the forecast.HoltWinters function in the R forecast package. When using the forecast.HoltWinters function, as its first argument (input), you pass it the predictive model that you have already fitted using the HoltWinters function. You specify how many further time points you want to make forecasts for by using the “h” parameter in forecast.HoltWinters(). The output for the next 20 years is: Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 2000 24.67819 19.17493 30.18145 16.26169 33.09470 2001 24.67819 19.17333 30.18305 16.25924 33.09715 2002 24.67819 19.17173 30.18465 16.25679 33.09960 2003 24.67819 19.17013 30.18625 16.25434 33.10204 2004 24.67819 19.16853 30.18785 16.25190 33.10449 2005 24.67819 19.16694 30.18945 16.24945 33.10694 2006 24.67819 19.16534 30.19105 16.24701 33.10938 2007 24.67819 19.16374 30.19265 16.24456 33.11182 2008 24.67819 19.16214 30.19425 16.24212 33.11427 2009 24.67819 19.16054 30.19584 16.23968 33.11671 2010 24.67819 19.15895 30.19744 16.23724 33.11915 2011 24.67819 19.15735 30.19904 16.23479 33.12159 2012 24.67819 19.15576 30.20063 16.23235 33.12403 2013 24.67819 19.15416 30.20223 16.22991 33.12647 2014 24.67819 19.15257 30.20382 16.22748 33.12891 2015 24.67819 19.15097 30.20542 16.22504 33.13135 2016 24.67819 19.14938 30.20701 16.22260 33.13379 2017 24.67819 19.14778 30.20860 16.22016 33.13623 2018 24.67819 19.14619 30.21020 16.21773 33.13866 2019 24.67819 19.14460 30.21179 16.21529 33.14110 The forecast for a year, a 80% prediction interval for the forecast, and a 95% prediction interval for the forecast. Plotting the predictions: The forecasts for 2000-2020 are plotted as a blue line, the 80% prediction interval as a dark blue shaded area, and the 95% prediction interval as a grey shaded area. Forecast errors Forecast errors are stored in the element residuals. If the predictive model cannot be improved upon, there should be no correlations between forecast errors for successive predictions. If there are correlations between forecast errors for successive predictions, it is likely that the simple exponential smoothing forecasts could be improved upon by another forecasting technique. To figure out whether this is the case, we can obtain a correlogram of the in-sample forecast errors. We can calculate a correlogram of the forecast errors using the acf function in R. acf computes (and plots, by default) the estimation of the autocovariance or AutoCorrelation Function (ACF). The autocorrelation at lag 3 is just touching the significance bounds (significant evidence for non-zero correlations). To test whether there is significant evidence for non-zero correlations at lags, we can carry out a Ljung- Box test. This can be done in R using the Box.test function. Box-Ljung test data: ts.p1.forecasts2$residuals X-squared = 17.401, df = 20, p-value = 0.6268 The p-value is 0.6, so there is little evidence of non-zero autocorrelations in the in-sample forecast errors at lags 1-20. 6.11 Example: Temperature forecast in Malaga - Exponential Smoothing Analyse the evolution of minimum temperatures since 2001 in Malaga with the methods seen in this document. Malaga with the methods seen in this document**. We obtain a time-series object with the ts function. data are divided into months, the parameter frequency is set to 12 and we indicate that we start in January 2001. After this, we draw the object: Forecasts using Exponential Smoothing Exponential Smoothing considers weights on variable values: weighted averages of past observations. The weights decrease exponentially for distant values. 6.11.1 Simple Exponential Smoothing alpha parameter of Holt-Winters Filter. beta parameter of Holt-Winters Filter. If set to FALSE, the function will do exponential smoothing. gamma parameter used for the seasonal component. If set to FALSE, an non-seasonal model is fitted. Looking at these parameters we have to assemble a model with beta=FALSE as we want exponential smoothing. Holt-Winters exponential smoothing without trend and with additive seasonal component. Call: HoltWinters(x = ts.p1, beta = FALSE) Smoothing parameters: alpha: 0.1412013 beta : FALSE gamma: 0.2045694 Coefficients: [,1] a 14.8002193 s1 -3.0179721 s2 -5.3483612 s3 -6.0676567 s4 -5.8300885 s5 -4.2971166 s6 -2.0597869 s7 0.5443399 s8 4.3512303 s9 6.9081866 s10 7.9327353 s11 5.3361482 s12 1.4610327 The graph shows the original time series in black, and forecasts of these values as a red line. The forecast time series is very close to the original data. We now draw the forecast using forecast. As we can see it fits quite well with what is happening in recent years. Let’s try a higher alpha value: Holt-Winters exponential smoothing without trend and with additive seasonal component. Call: HoltWinters(x = ts.p1, alpha = 0.5, beta = FALSE) Smoothing parameters: alpha: 0.5 beta : FALSE gamma: 0.3927417 Coefficients: [,1] a 15.0512589 s1 -3.1697282 s2 -5.4838631 s3 -6.2075694 s4 -5.9219038 s5 -4.2667725 s6 -1.8548302 s7 0.8149946 s8 4.5957651 s9 7.0279317 s10 8.0279557 s11 5.3161216 s12 1.1929326 Holt-Winters exponential smoothing without trend and with additive seasonal component. Call: HoltWinters(x = ts.p1, alpha = 0.9, beta = FALSE) Smoothing parameters: alpha: 0.9 beta : FALSE gamma: 0.9999496 Coefficients: [,1] a 14.7654331 s1 -3.0613814 s2 -5.3688209 s3 -5.8712039 s4 -5.2291009 s5 -3.4451379 s6 -1.4794315 s7 0.5904564 s8 4.0760361 s9 6.5257969 s10 7.4170932 s11 4.6994542 s12 1.1345783 As we can see there is not much difference between the first model and the other 2 created later, so the alpha value generated by the first model is the optimal one. We can see that the higher the alpha value, the narrower the fixed values, and the gamma values increase along with alpha. 6.11.2 Forecasting using forecast package When we use the forecast.HoltWinters function, as the first argument, it is passed the predictive model that we have already set using the HoltWinters function. To specify how many additional time points we want to predict, we use the parameter “h”. The prediction for the next 2 years is: Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 Nov 2018 11.782247 10.448881 13.11561 9.743039 13.82146 Dec 2018 9.451858 8.105265 10.79845 7.392421 11.51129 Jan 2019 8.732563 7.372871 10.09225 6.653094 10.81203 Feb 2019 8.970131 7.597467 10.34279 6.870822 11.06944 Mar 2019 10.503103 9.117587 11.88862 8.384139 12.62207 Apr 2019 12.740432 11.342183 14.13868 10.601995 14.87887 May 2019 15.344559 13.933692 16.75543 13.186823 17.50229 Jun 2019 19.151450 17.728075 20.57482 16.974587 21.32831 Jul 2019 21.708406 20.272634 23.14418 19.512582 23.90423 Aug 2019 22.732955 21.284891 24.18102 20.518333 24.94758 Sep 2019 20.136368 18.676116 21.59662 17.903106 22.36963 Oct 2019 16.261252 14.788913 17.73359 14.009504 18.51300 Nov 2019 11.782247 10.250481 13.31401 9.439613 14.12488 Dec 2019 9.451858 7.908565 10.99515 7.091594 11.81212 Jan 2020 8.732563 7.177827 10.28730 6.354800 11.11032 Feb 2020 8.970131 7.404038 10.53622 6.574998 11.36526 Mar 2020 10.503103 8.925733 12.08047 8.090724 12.91548 Apr 2020 12.740432 11.151866 14.32900 10.310930 15.16993 May 2020 15.344559 13.744875 16.94424 12.898054 17.79106 Jun 2020 19.151450 17.540725 20.76217 16.688058 21.61484 Jul 2020 21.708406 20.086715 23.33010 19.228243 24.18857 Aug 2020 22.732955 21.100371 24.36554 20.236134 25.22978 Sep 2020 20.136368 18.492964 21.77977 17.622999 22.64974 Oct 2020 16.261252 14.607099 17.91541 13.731443 18.79106 The forecasts for the next 2 years are represented as a blue line, the 80% prediction interval as a dark grey shaded area and the 95% prediction interval as a grey shaded area. Forecast errors Errors are stored in residuals. If the model cannot be improved, there should be no correlations between forecast errors for successive predictions. If there are correlations between forecast errors for successive predictions, it is likely that the predictions can be improved with another technique. To find out which is the case, we can obtain a correlation of the errors. We can calculate it using the acf function. acf calculates the estimate of the autocovariance or autocorrelation function (ACF). Let’s see what happens, we use acf with residuals and lag max 24. We performed a Ljung Box text to see if there is significant evidence of non-zero correlations in the lag. Box-Ljung test data: ts.p1.forecasts2$residuals X-squared = 41.723, df = 24, p-value = 0.01386 As we can see, the p-value is 0.01, which tells us that there is evidence of autocorrelations other than 0. Note: The ACF (AutoCorrelation Function) chart is used to identify non-stationary time series. Stationary: if the ACF is rapidly declining 6.12 ARIMA methods The difference between exponential smoothing models and ARIMA models is that the former are based on a description of the trend and seasonality of the data, while ARIMA models study the autocorrelations of the data. As we have already seen, the name ARIMA comes from AutoRegressive Integrated Moving Average. AR: the study variable is analysed using the above values (lag). MA: the regression error is a linear combination of current and past values of the signal. I: replaces the value of the variable by the difference between the values and the previous values (one or more times). ARIMA models can be obtained following the Box-Jenkins method. 6.12.1 Stationarity A time series is said to be stationary when it does not depend on the instant of observation (white noise series). The autocovariance function is defined as \\[Cov(y_t,y_{t+k}) = \\gamma(k), \\forall k\\] That is, the covariance does not depend on time, only on the parameter \\(k\\) (lag). If a series has a trend or stationarity it is not stationary. A time series that has a cyclical component is stationary. 6.13 Autoregressive models - AR(p) The current value of the \\(x_t\\) series can be explained as a function of \\(p\\) past values, \\(x_{t-1},x_{t-2},x_{t-p}\\), where \\(p\\) determines the number of past steps needed for the prediction. \\[ y_t=c+\\phi_1y_{t-1}+\\phi_2y_{t-2}+\\cdots+\\phi_py_{t-p}+\\epsilon_t \\] \\(\\epsilon_t\\): Gaussian white noise with mean 0 and variance \\(sigma^2_epsilon\\). If the mean is non-zero, \\(c\\) takes the value \\(c = \\mu(1-\\phi_1-\\cdots-\\phi_p)\\). This is an autoregressive model of order \\(p\\) that applies to stationary models. 6.14 Moving average models As an alternative to the autoregressive representation where \\(x_t\\) (on the left-hand side of the equation) are assumed to be linear combinations using moving averages of order \\(q\\), \\(MA(q)\\). We assume white noise \\(epsilon_t\\). Model MA(\\(q\\)): \\[ y_t=c+\\epsilon_t+\\theta_1\\epsilon_{t-1}+\\theta_2\\epsilon_{t-2}+\\cdots+\\theta_q\\epsilon_{t-q} \\] As with autoregressive models, changing the parameters affects the time series patterns and changing the error term changes the scale. 6.14.1 Differentiation To convert a non-stationary time series into a stationary time series, the differences between consecutive signal values are calculated (trend and seasonality are removed or reduced). The following equation allows to calculate the differences between consecutive signal values: \\[ y&#39;_t=y_t-y_{t-1} \\] If the differentiated series is white noise: \\[ y_t-y_{t-1}=\\epsilon_t \\] (\\(\\epsilon_t\\) denotes white noise) A closely related model allows the difference to have a non-zero mean. \\[ y_t-y_{t-1}=c+\\epsilon_t \\] where \\(c\\) is the average of the changes between consecutive observations. 6.14.2 Second order differencing The difference of the data may have to be recalculated a second time to obtain a stationary series (the change in changes). 6.15 Seasonal differencing A seasonal difference is the difference between one observation and the previous observation: \\[ y&#39;_{_t}=y_{_t}-y_{_{t-m}} \\] where \\(m\\) is the number of seasons. lynx &lt;- read.csv(file = &quot;data/annual-number-of-lynx-trapped-ma.csv&quot;, header = TRUE, quote = &#39;\\&quot;&#39;) lynx.ts &lt;- ts(lynx$Lynx, frequency = 1, start = c(1821)) electricity &lt;- read.csv(file = &quot;data/monthly-electricity-production-i.csv&quot;, header = TRUE, quote = &#39;\\&quot;&#39;) electricity.ts &lt;- ts(electricity$Million.kilowatt.hours, frequency = 12, start = c(1956, 1)) plot.ts(lynx.ts, xlab = &quot;Year&quot;, ylab = &quot;Lynx Trapped&quot;) Figure 6.1: Although there are cycles, they are not fixed in time. plot.ts(electricity.ts, xlab = &quot;Year&quot;, ylab = &quot;kW/h&quot;) Figure 6.2: Clear trend indicates this is not a stationary series. par(mfrow = c(1, 2)) acf(lynx.ts) acf(electricity.ts) Figure 6.3: Lynx trapped ACF plot (left) and kilowatts per hour in Australia ACF plot (right). 6.15.1 Parameters in ARIMA methods. Therefore, in an ARIMA model we will use three parameters: (p, d, q). p: refers to the use of past values of the series. It specifies the number of lags used in the model. For example, AR(2) or, ARIMA(2,0,0) specifies the use of two lags of the series. d: degree of differentiation. Differencing your current and previous values d times. q: model error as a combination of q previous error terms. ARIMA works best for long and stable series. 6.16 Non-seasonal ARIMA models If we combine differencing and autoregression we obtain a non-seasonal ARIMA model: \\[ y&#39;_t = c + \\phi_1y&#39;_{t-1}+\\cdots+\\phi_py&#39;_{t-p}+\\theta_1\\epsilon_{t-1}+\\cdots+\\theta_q\\epsilon_{t-q}+\\epsilon_t \\] where \\(y&#39;_t\\) is the series to which differencing has been applied. These models are represented as ARIMA(\\(p,d,q\\)), where \\(d\\) represents the degree of differencing. 6.17 Auto-ARIMA model In R we have functions that automatically calculate the parameter values of an ARIMA model. The auto.arima() function is a modification of the Hyndman-Khandakar algorithm. R’s auto.arima() function combines tests, and minimisations to obtain an ARIMA model. By default, it can use initial approximations to speed up the model search (approximation=TRUE/FALSE). If this does not allow to find a good model (minimum AICc) a larger set of models can be searched with the argument stepwise=FALSE. See (forecasting?) for more on this topic. To choose a custom model for a non-seasonal series, we will use the Arima() function as follows: Draw the dataset and identify outliers. Transform the data to stabilise the variance. If the data is not stationary use differencing until it is. Analyse ACF/PACF for an ARIMA(\\(p,d,0\\)) or ARIMA(\\(0,d,q\\)) model. Try to pick your model and use AICc to find the best one. Check that the residuals behave like white noise: if this is not satisfied look for another model if this is true, we can use the model to predict 6.18 Example: Temperature forecast in Malaga - Modelos ARIMA Analyse the evolution of the minimum temperatures since 2001 in Malaga with the methods seen in this document. We obtain a time-series object with the ts function, as our data are divided into months, we set the parameter frequency to 12 and we indicate that we start in January 2001. Then we draw the object obtained: ARIMA models We decompose the data: Seasonal component, Trend component, Cycle component y residual. Previously we have seen that the series is stationary, but to fit an ARIMA model requires the series to be stationary. Augmented Dickey-Fuller Test data: obj.ts Dickey-Fuller = -14.33, Lag order = 5, p-value = 0.01 alternative hypothesis: stationary We see that it is stationary. We fix an ARIMA model using auto.arima: Series: deseasonal_cnt ARIMA(1,1,1) Coefficients: ar1 ma1 0.3684 -0.9799 s.e. 0.0668 0.0175 sigma^2 = 0.7702: log likelihood = -274.66 AIC=555.32 AICc=555.44 BIC=565.41 As we can see, even lag=23 is equal, so p or q is 23. Call: arima(x = deseasonal_cnt, order = c(1, 1, 23)) Coefficients: ar1 ma1 ma2 ma3 ma4 ma5 ma6 ma7 -0.7624 0.1627 -0.6937 -0.1923 -0.0753 0.0409 -0.1322 -0.0993 s.e. 0.2475 0.2563 0.1611 0.1147 0.0880 0.0930 0.1067 0.1014 ma8 ma9 ma10 ma11 ma12 ma13 ma14 ma15 ma16 0.0485 0.2009 0.0295 -0.2316 -0.1224 -0.1785 0.0240 0.3970 0.1431 s.e. 0.0936 0.0982 0.0963 0.1006 0.0939 0.0922 0.0978 0.1027 0.1168 ma17 ma18 ma19 ma20 ma21 ma22 ma23 -0.1927 -0.2143 0.0691 0.1088 0.1423 -0.0907 -0.0931 s.e. 0.0902 0.1099 0.0969 0.1216 0.1053 0.1051 0.1117 sigma^2 estimated as 0.6437: log likelihood = -261.09, aic = 572.19 We forecast for the next 30 months: 6.19 References Information and images obtained from: Forecasting: Principles and Practice. Rob J Hyndman and George Athanasopoulos. Monash University, Australia Introducion to Forecasting with ARIMA in R AEMET "],["clustering.html", "Capítulo 7 Clustering 7.1 Fundamentos 7.2 Clustering en R 7.3 Número optimo de clusters 7.4 Clustering jerárquico", " Capítulo 7 Clustering 7.1 Fundamentos 7.1.1 Medidas de similaridad Los métodos matemáticos en la que descansan las técnicas de clustering están basados en la búsqueda de patrones de similaridad/disimilaridad en los datos. Dado un conjunto de valores de un dominio, las medidas de disimilariad son funciones que asignan valores reales a pares de instancias del dominio. Estas funciones se usan o bien en la formación del cluster (grupos iniciales) o el proceso de modelización del cluster (grupos evolucionan). Estos algoritmos se denominan algoritmos de clustering basados en similaridad o más bien disimilaridad. Las funciones de similaridad/disimilaridad miden y expresan numericamente el grado en el que dos instancias del mismo dominio (descritas por un conjunto de atributos) sin similares/disimilares unas de otras. Se podrían clasisificar en: Basadas en diferencias: transforman y agregan de alguna forma las diferencias para cada dos instancias comparadas. Basadas en correlaciones: detectan patrones comunes de los valores inferiores y superiores de los atributos para cada dos instancias de valores. 7.1.2 Disimilaridad basada en diferencias Dadas dos instancias \\(x_1,x_2\\) con un conjunto de atributos \\(a_i\\), estas medidas devuelven una matriz con la distancia existente entre cada par de atributos en cada una de las instancias. Distancias a usar para calcular la matriz: Distancia euclidea: \\(d_{euc}=\\mid \\mid x-y \\mid\\mid_2\\) Distancia de Manhattan: \\(d_{euc}=\\displaystyle \\sum_{i=1}^n \\mid (x_i-y_i)\\mid\\) Distancia de Minkowski: Distancia de Canberra Distancia de Chebysehv Distancia de Hamming 7.1.3 Matriz de distancias library(cluster) library(factoextra) data(&quot;USArrests&quot;) usa_arrests &lt;- na.omit(USArrests) dist_euc &lt;- get_dist(usa_arrests) fviz_dist(dist_euc) La función get_dist por defecto calcula la distancia euclídea pero soporta otras funciones distancia. 7.1.4 Similaridad basada en correlaciones A veces debemos tener en cuenta que dos instancias son similares según el conocimiento del dominio más que en los valores de los atributos. Si atributos representan frecuencias de eventos, cuentas de frecuencias de palabras en textos. Las diferencias que importan más que cuantitativas son discretas: alto, bajo, etc. Medidas: Correlación de Pearson Correlación de Spearman Similaridad del coseno 7.1.5 Correlación de Pearson Establece el grado de relación entre dos atributos Normalmente es la usada \\[\\displaystyle d_{cor}=1-\\frac{\\sum ( x_i - \\bar{x} )( y_i - \\bar{y} ) }{\\sqrt{\\sum ( x_i - \\bar{x} )^2\\sum ( y_i - \\bar{y} )^2}} \\] 7.2 Clustering en R Clustering es una técnica de aprendizaje no supervisado. Se introduce el número de grupos (clusters) a identificar. Algoritmo: El algoritmo debe garantizar la convergencia. 7.2.1 Centros Clustering es muy sensible a la elección inicial de los centros. Y es un algoritmo muy exigente en recursos. Para calcular las distancias de puntos a centros se usa: Métodos de clustering 7.2.2 Objetivo Dado un dataset \\(D=\\{x_1,x_2, \\ldots,x_n\\}\\) donde \\(x_i\\) denota las filas y con \\(m\\) atributos \\(A_i\\) en las columnas tal que cada fila \\(x_i=\\{a_1,a_2,\\ldots,a_m\\}\\) con \\(a_k\\) una valor del atributo \\(A_k\\). Los métodos de clustering agrupan la población del dataset (las filas) en \\(k\\) grupos (clusters) con \\(k \\leq n\\). 7.2.3 k-means Características: Algoritmo: Algoritmo debe garantizar la convergencia. El método k-means agrupa el dataset en \\(k\\) grupos distintos \\(S=\\{S_1,\\ldots,S_k\\}\\) minimizando la media interna de la suma de cuadrados. \\[min_{S}\\sum_{j=1}^k\\sum_{x_j\\in S_j}\\mid\\mid x_i -\\overline{m_j}\\mid\\mid^2\\] donde \\(\\overline{m_j}\\) es el vector media de los \\(m\\) atributos promediado en todas las filas del cluster y \\(\\mid\\mid x_i -\\overline{m_j}\\mid\\mid^2= \\sum_{r=1}(x_r - m_r)^2\\) (suma de las diferencias al cuadrado de los \\(m\\) atributos - distancia euclidea entre los vectores de cada grupo y la media) 7.3 Número optimo de clusters 7.3.1 Proyecto - dataset USArrests We can compute k-means in R with the kmeans function. Here will group the data into two clusters (centers = 2). The kmeans function also has an nstart option that attempts multiple initial configurations and reports on the best one. For example, adding nstart = 25 will generate 25 initial configurations. This approach is often recommended. List of 9 $ cluster : Named int [1:50] 1 1 1 1 1 1 2 1 1 1 … ..- attr(, “names”)= chr [1:50] “Alabama” “Alaska” “Arizona” “Arkansas” … $ centers : num [1:2, 1:4] 11.86 4.84 255 109.76 67.62 … ..- attr(, “dimnames”)=List of 2 .. ..$ : chr [1:2] “1” “2” .. ..$ : chr [1:4] “Murder” “Assault” “UrbanPop” “Rape” $ totss : num 355808 $ withinss : num [1:2] 41637 54762 $ tot.withinss: num 96399 $ betweenss : num 259409 $ size : int [1:2] 21 29 $ iter : int 1 $ ifault : int 0 - attr(*, “class”)= chr “kmeans” We can also view our results by using fviz_cluster. This provides a nice illustration of the clusters. If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance. Alternatively, you can use standard pairwise scatter plots to illustrate the clusters compared to the original variables. 7.3.2 Tamaño optimal de clusters Una forma sencilla de estimar el número K óptimo de clusters cuando no se dispone de información adicional en la que basarse es aplicar el algoritmo para un rango de valores de K, identificando aquel a partir del cual la reducción en la suma total de varianza intra-cluster deja de ser sustancial (en las siguientes secciones se describen otras alternativas). La función fviz_nbclust() automatiza este proceso. En este caso, dado que se sospecha de la presencia de outliers, se emplea la distancia de Manhattan como medida de similitud. A partir de 4 clusters la reducción en la suma total de cuadrados internos parece estabilizarse, indicando que K = 4 es una buena opción. 7.4 Clustering jerárquico 7.4.1 Proyecto dataset protein Country RedMeat WhiteMeat Eggs Milk Fish Cereals Starch Nuts Fr.Veg 1 Albania 10.1 1.4 0.5 8.9 0.2 42.3 0.6 5.5 1.7 2 Austria 8.9 14.0 4.3 19.9 2.1 28.0 3.6 1.3 4.3 3 Belgium 13.5 9.3 4.1 17.5 4.5 26.6 5.7 2.1 4.0 4 Bulgaria 7.8 6.0 1.6 8.3 1.2 56.7 1.1 3.7 4.2 5 Czechoslovakia 9.7 11.4 2.8 12.5 2.0 34.3 5.0 1.1 4.0 6 Denmark 10.6 10.8 3.7 25.0 9.9 21.9 4.8 0.7 2.4 [1] 25 10 RedMeat WhiteMeat Eggs Milk Fish Cereals Starch Nuts Fr.Veg 10.1 1.4 0.5 8.9 0.2 42.3 0.6 5.5 1.7 8.9 14.0 4.3 19.9 2.1 28.0 3.6 1.3 4.3 13.5 9.3 4.1 17.5 4.5 26.6 5.7 2.1 4.0 7.8 6.0 1.6 8.3 1.2 56.7 1.1 3.7 4.2 9.7 11.4 2.8 12.5 2.0 34.3 5.0 1.1 4.0 10.6 10.8 3.7 25.0 9.9 21.9 4.8 0.7 2.4 8.4 11.6 3.7 11.1 5.4 24.6 6.5 0.8 3.6 9.5 4.9 2.7 33.7 5.8 26.3 5.1 1.0 1.4 18.0 9.9 3.3 19.5 5.7 28.1 4.8 2.4 6.5 10.2 3.0 2.8 17.6 5.9 41.7 2.2 7.8 6.5 5.3 12.4 2.9 9.7 0.3 40.1 4.0 5.4 4.2 13.9 10.0 4.7 25.8 2.2 24.0 6.2 1.6 2.9 9.0 5.1 2.9 13.7 3.4 36.8 2.1 4.3 6.7 9.5 13.6 3.6 23.4 2.5 22.4 4.2 1.8 3.7 9.4 4.7 2.7 23.3 9.7 23.0 4.6 1.6 2.7 6.9 10.2 2.7 19.3 3.0 36.1 5.9 2.0 6.6 6.2 3.7 1.1 4.9 14.2 27.0 5.9 4.7 7.9 6.2 6.3 1.5 11.1 1.0 49.6 3.1 5.3 2.8 7.1 3.4 3.1 8.6 7.0 29.2 5.7 5.9 7.2 9.9 7.8 3.5 24.7 7.5 19.5 3.7 1.4 2.0 13.1 10.1 3.1 23.8 2.3 25.6 2.8 2.4 4.9 17.4 5.7 4.7 20.6 4.3 24.3 4.7 3.4 3.3 9.3 4.6 2.1 16.6 3.0 43.6 6.4 3.4 2.9 11.4 12.5 4.1 18.8 3.4 18.6 5.2 1.5 3.8 4.4 5.0 1.2 9.5 0.6 55.9 3.0 5.7 3.2 x Eggs 3 Nuts 3 Fish 4 Starch 4 Fr.Veg 4 WhiteMeat 8 RedMeat 10 Milk 17 Cereals 32 x Eggs 1.1 Starch 1.6 Fr.Veg 1.8 Nuts 2.0 RedMeat 3.3 Fish 3.4 WhiteMeat 3.7 Milk 7.1 Cereals 11.0 List of 9 $ cluster : int [1:25] 3 2 1 3 2 2 2 3 1 3 … $ centers : num [1:3, 1:2] 9 12.06 4.66 15.18 8.84 … ..- attr(, “dimnames”)=List of 2 .. ..$ : chr [1:3] “1” “2” “3” .. ..$ : chr [1:2] “WhiteMeat” “RedMeat” $ totss : num 596 $ withinss : num [1:3] 35.7 39.5 69.9 $ tot.withinss: num 145 $ betweenss : num 451 $ size : int [1:3] 5 8 12 $ iter : int 3 $ ifault : int 0 - attr(, “class”)= chr “kmeans” K-means clustering with 3 clusters of sizes 5, 8, 12 Cluster means: WhiteMeat RedMeat 1 9.000000 15.180000 2 12.062500 8.837500 3 4.658333 8.258333 Clustering vector: [1] 3 2 1 3 2 2 2 3 1 3 2 1 3 2 3 2 3 3 3 3 1 1 3 2 3 Within cluster sum of squares by cluster: [1] 35.66800 39.45750 69.85833 (between_SS / total_SS = 75.7 %) Available components: [1] “cluster” “centers” “totss” “withinss” “tot.withinss” [6] “betweenss” “size” “iter” “ifault” [1] 3 9 12 21 22 2 5 6 7 11 14 16 24 1 4 8 10 13 15 17 18 19 20 23 25 food.Country.Gr1. grpMeat.cluster.Gr1. 1 Belgium 1 2 France 1 3 Ireland 1 4 Switzerland 1 5 UK 1 6 Austria 2 7 Czechoslovakia 2 8 Denmark 2 9 E Germany 2 10 Hungary 2 11 Netherlands 2 12 Poland 2 13 W Germany 2 14 Albania 3 15 Bulgaria 3 16 Finland 3 17 Greece 3 18 Italy 3 19 Norway 3 20 Portugal 3 21 Romania 3 22 Spain 3 23 Sweden 3 24 USSR 3 25 Yugoslavia 3 food.Country.Gr1. grpProtein.cluster.Gr1. 1 Belgium 1 2 France 1 3 Ireland 1 4 Switzerland 1 5 UK 1 6 Bulgaria 2 7 Romania 2 8 Yugoslavia 2 9 Denmark 3 10 Finland 3 11 Norway 3 12 Sweden 3 13 Czechoslovakia 4 14 Hungary 4 15 Poland 4 16 Portugal 5 17 Spain 5 18 Albania 6 19 Greece 6 20 Italy 6 21 USSR 6 22 Austria 7 23 E Germany 7 24 Netherlands 7 25 W Germany 7 [1] “dist” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0.00000 23.176281 21.650173 15.688212 15.154537 30.157586 22.865914 30.99097 23.174124 12.136309 13.157127 27.902509 10.624500 28.302120 26.805410 17.643979 23.111036 10.319399 17.149927 29.987998 24.928899 24.308435 11.034038 29.143953 15.474818 23.17628 0.000000 7.868291 32.304489 10.305338 11.956588 10.742905 17.42125 11.010904 19.529721 16.974982 10.039422 14.688771 6.763875 13.684663 9.942334 22.931202 25.261235 17.440183 13.032268 7.583535 12.920526 19.042846 10.131634 31.946987 21.65017 7.868291 0.000000 32.786125 10.609901 11.119802 8.928606 17.60312 6.007495 18.254862 18.784036 9.146584 13.568714 9.675743 10.803703 12.201639 19.200781 25.878563 13.916537 11.632283 7.525955 6.830081 18.416840 9.066422 32.684859 15.68821 32.304489 32.786125 0.000000 24.005416 40.334105 33.614729 40.33510 33.263494 19.315538 18.399185 38.359744 21.013329 38.526744 38.174730 24.493877 33.293843 8.334867 28.887367 41.480598 35.509013 36.419500 16.675431 40.619823 4.875449 15.15454 10.305338 10.609901 24.005416 0.000000 19.420608 10.613670 24.01999 13.435029 15.025645 9.179869 17.582093 8.705171 16.352676 18.727253 8.255301 19.060430 17.311557 13.070578 20.430370 14.971640 16.497273 12.635268 17.151385 23.977698 30.15759 11.956588 11.119802 40.334105 19.420608 0.000000 15.184532 12.24990 12.718490 24.466099 26.734809 8.938121 21.595833 8.360024 6.688049 17.813759 23.926972 33.292191 21.187732 4.795832 9.650389 11.740528 25.334759 9.901010 39.874177 22.86591 10.742905 8.928606 33.614729 10.613670 15.184532 0.000000 23.82751 13.855324 22.109274 17.517991 16.177763 15.622740 13.268760 14.985326 14.874811 15.156187 26.731629 11.784312 15.584287 14.647184 14.780054 21.369137 10.558409 33.279423 30.99097 17.421251 17.603125 40.335096 24.019992 12.249898 23.827505 0.00000 18.181309 24.107053 29.983829 11.569356 23.755420 14.677534 11.688028 19.358977 31.176273 33.398054 26.571413 11.908820 13.054118 15.951175 24.692712 18.878824 39.330014 23.17412 11.010904 6.007495 33.263494 13.435029 12.718490 13.855324 18.18131 0.000000 18.254589 21.256293 10.153325 15.157836 12.350304 13.150285 14.013208 21.854061 27.116969 17.215110 14.027117 8.182298 6.987131 19.434248 12.459936 33.723137 12.13631 19.529721 18.254862 19.315538 15.025645 24.466099 22.109274 24.10705 18.254589 0.000000 14.933184 22.976510 7.976841 24.062834 21.419617 12.358802 22.156489 13.341664 16.268682 25.157305 20.039960 20.320187 8.175573 26.280411 18.747800 13.15713 16.974982 18.784036 18.399185 9.179869 26.734809 17.517991 29.98383 21.256293 14.933184 0.000000 25.019193 10.702803 23.208619 25.669437 11.992498 22.031568 11.638728 16.165704 27.655198 22.139557 24.172091 12.493198 24.701012 17.615902 27.90251 10.039422 9.146584 38.359744 17.582093 8.938121 16.177763 11.56936 10.153325 22.976510 25.019193 0.000000 20.037215 6.827884 10.883014 16.008748 27.137612 31.295687 21.806192 8.884256 5.097058 8.241966 23.017385 9.709789 37.953919 10.62450 14.688771 13.568714 21.013329 8.705171 21.595833 15.622740 23.75542 15.157836 7.976841 10.702803 20.037215 0.000000 19.872343 18.766726 9.066973 17.866169 14.311184 10.910545 21.877386 16.668833 17.261518 9.462029 21.096682 20.721245 28.30212 6.763875 9.675743 38.526744 16.352676 8.360024 13.268760 14.67753 12.350304 24.062834 23.208619 6.827884 19.872343 0.000000 11.551623 15.336884 25.384838 31.244840 20.661317 8.497647 6.297619 11.987076 24.233448 6.525335 37.969725 26.80541 13.684663 10.803703 38.174730 18.727253 6.688049 14.985326 11.68803 13.150285 21.419617 25.669437 10.883014 18.766726 11.551623 0.000000 16.913308 20.662768 31.020638 17.575836 5.557877 10.653638 10.523783 22.826301 12.152366 37.451569 17.64398 9.942334 12.201639 24.493877 8.255301 17.813759 14.874811 19.35898 14.013208 12.358802 11.992498 16.008748 9.066973 15.336884 16.913308 0.000000 21.676946 17.429859 15.496774 19.150196 13.500370 17.070735 10.812955 18.513238 23.728464 23.11104 22.931202 19.200781 33.293843 19.060430 23.926972 15.156187 31.17627 21.854061 22.156489 22.031568 27.137612 17.866169 25.384838 20.662768 21.676946 0.000000 27.650859 8.788629 24.082774 24.844517 22.770156 24.005208 22.834842 32.829408 10.31940 25.261235 25.878563 8.334867 17.311557 33.292191 26.731629 33.39805 27.116969 13.341664 11.638728 31.295687 14.311184 31.244840 31.020638 17.429859 27.650859 0.000000 22.286094 34.196052 28.574464 29.726924 9.880283 33.438600 6.910861 17.14993 17.440183 13.916537 28.887367 13.070578 21.187732 11.784312 26.57141 17.215110 16.268682 16.165704 21.806192 10.910545 20.661317 17.575836 15.496774 8.788629 22.286094 0.000000 20.789420 19.311396 17.655028 17.884910 19.048097 28.132721 29.98800 13.032268 11.632283 41.480598 20.430370 4.795832 15.584287 11.90882 14.027117 25.157305 27.655198 8.884256 21.877386 8.497647 5.557877 19.150196 24.082774 34.196052 20.789420 0.000000 9.537819 10.903211 26.293155 9.090105 40.833932 24.92890 7.583535 7.525955 35.509013 14.971640 9.650389 14.647184 13.05412 8.182298 20.039960 22.139557 5.097058 16.668833 6.297619 10.653638 13.500370 24.844517 28.574464 19.311396 9.537819 0.000000 7.969316 20.975700 9.624968 35.278889 24.30843 12.920526 6.830081 36.419500 16.497273 11.740528 14.780054 15.95118 6.987131 20.320187 24.172091 8.241966 17.261518 11.987076 10.523783 17.070735 22.770156 29.726924 17.655028 10.903211 7.969316 0.000000 21.605786 11.101802 36.405906 11.03404 19.042846 18.416840 16.675431 12.635268 25.334759 21.369137 24.69271 19.434248 8.175573 12.493198 23.017385 9.462029 24.233448 22.826301 10.812955 24.005208 9.880283 17.884910 26.293155 20.975700 21.605786 0.000000 26.583454 15.791770 29.14395 10.131634 9.066422 40.619823 17.151385 9.901010 10.558409 18.87882 12.459936 26.280411 24.701012 9.709789 21.096682 6.525335 12.152366 18.513238 22.834842 33.438600 19.048097 9.090105 9.624968 11.101802 26.583454 0.000000 40.275551 15.47482 31.946987 32.684859 4.875449 23.977698 39.874177 33.279423 39.33001 33.723137 18.747800 17.615902 37.953919 20.721245 37.969725 37.451569 23.728464 32.829408 6.910861 28.132721 40.833932 35.278889 36.405906 15.791770 40.275551 0.000000 id country 1 Albania 2 Austria 3 Belgium 4 Bulgaria 5 Czechoslovakia 6 Denmark 7 E Germany 8 Finland 9 France 10 Greece 11 Hungary 12 Ireland 13 Italy 14 Netherlands 15 Norway 16 Poland 17 Portugal 18 Romania 19 Spain 20 Sweden 21 Switzerland 22 UK 23 USSR 24 W Germany 25 Yugoslavia [1] 13.07058 [1] 13.07058 [,1] [,2] [1,] -6 -20 [2,] -4 -25 [3,] -12 -21 [4,] -3 -9 [5,] -14 -24 [6,] -15 1 [7,] -22 4 [8,] -10 -13 [9,] -5 -16 [10,] -18 2 [11,] -17 -19 [12,] -2 5 [13,] -23 8 [14,] 3 12 [15,] -11 9 [16,] -1 13 [17,] -7 11 [18,] -8 6 [19,] 7 14 [20,] 15 16 [21,] 18 19 [22,] 17 20 [23,] 10 22 [24,] 21 23 Country RedMeat WhiteMeat Eggs Milk Fish Cereals Starch Nuts Fr.Veg 1 Albania 10.1 1.4 0.5 8.9 0.2 42.3 0.6 5.5 1.7 2 Austria 8.9 14.0 4.3 19.9 2.1 28.0 3.6 1.3 4.3 3 Belgium 13.5 9.3 4.1 17.5 4.5 26.6 5.7 2.1 4.0 4 Bulgaria 7.8 6.0 1.6 8.3 1.2 56.7 1.1 3.7 4.2 5 Czechoslovakia 9.7 11.4 2.8 12.5 2.0 34.3 5.0 1.1 4.0 6 Denmark 10.6 10.8 3.7 25.0 9.9 21.9 4.8 0.7 2.4 cluster 1 1 2 2 3 2 4 3 5 1 6 2 1 2 3 4 7 12 3 3 cluster RedMeat WhiteMeat Eggs Milk Fish Cereals Starch 1 1 8.642857 6.871429 2.385714 14.042857 2.5428571 39.27143 3.742857 2 2 12.091667 9.441667 3.708333 23.000000 4.9916667 24.02500 4.616667 3 3 6.133333 5.766667 1.433333 9.633333 0.9333333 54.06667 2.400000 4 4 7.233333 6.233333 2.633333 8.200000 8.8666667 26.93333 6.033333 Nuts Fr.Veg 1 4.214286 4.657143 2 1.766667 3.491667 3 4.900000 3.400000 4 3.800000 6.233333 [1] RedMeat WhiteMeat Eggs Milk Fish Cereals Starch [8] Nuts Fr.Veg &lt;0 rows&gt; (or 0-length row.names) [1] RedMeat WhiteMeat Eggs Milk Fish Cereals Starch [8] Nuts Fr.Veg &lt;0 rows&gt; (or 0-length row.names) [1] RedMeat WhiteMeat Eggs Milk Fish Cereals Starch [8] Nuts Fr.Veg &lt;0 rows&gt; (or 0-length row.names) [1] RedMeat WhiteMeat Eggs Milk Fish Cereals Starch [8] Nuts Fr.Veg &lt;0 rows&gt; (or 0-length row.names) "],["reglas-de-asociación.html", "Capítulo 8 Reglas de Asociación 8.1 Introducción 8.2 Reglas de Asociación de transacciones 8.3 Reglas de Asociación - preprocesamiento 8.4 Ejercicio en Laboratorio 8.5 arulesViz 8.6 Construyendo un sistema recomendador", " Capítulo 8 Reglas de Asociación 8.1 Introducción Aprendizaje no supervisado: técnica que permite un aprendizaje a partir de observaciones que permite extraer patrones, tendencias y realizar predicciones - sin entrenamiento. Entre los problemas más relevantes - predición de comportamiento de clientes, tendencias de compras: tiendas online, servicios online que ofrecen música, películas, etc. La realidad actual es que hay mucha competencia, muchos servicios similares, mercados online, etc. Objetivo: Atraer clientes es la clave. Medios: datos recolectados de los consumidores, de los usuarios de los servicios, características de los consumidores extraidas de los datos recolectados, patrones previos de compra o de uso, etc. Herramientas: Métodos de Ciencia de Datos - búsqueda de patrones frecuentes, reglas, etc. Los métodos usados en esta área deben analizr los datos almacenados para diseñar sistemas recomendadores con los que: Ayudar a personalizar los servicios y la experiencia de compra, adivinar y sugerir tendencias de compra a partir de likes, dislikes. Controlar las horas punta de los servicios. Analizar combinaciones de productos que la gente suele comprar juntas. Analizar revisiones y precios que la competencia ofrece por los mismos productos Los problemas en este área surgen del llamado Market Basket Analysis que se enfrenta a cómo realizar recomendaciones basadas en productos. Las soluciones para este problema pueden usarse en otros problemas similares: recomendaciones de intereses de las personas, etc. Las técnicas más importantes son: Evaluación de la matriz de contingenica de los productos. Generación de itemsets frecuentes. Extracción de reglas de asociación. 8.1.1 Detectando y prediciendo tendencias Tendencia: patrón específico o comportamiento de compra-venta que aparece en un periodo de tiempo en una tienda. ¿Cómo detectar?: Almacenar todas las transacciones que se realicen en la tienda. items comprados stocks combinaciones de items comprados juntos transacción de cada venta realizada ¿Cómo tratar los datos?: pre-procesar normalizar agregar Aplicar algoritmos: localizar patrones y tendencias Recomendar: usar patrones y tendencias para sugerir nuevas compras. El principal método para conseguir todo esto está basado en el problema denominado Market Basket Analysis. Estos métodos están fundamentados en estadística basado en probabilidad y nociones probabilísticas como: * soporte * confianza * lift, etc. Objetivo: ¿Qué items (productos) se han comprado más frecuentemente? 8.2 Reglas de Asociación de transacciones Explorando el dataset Adult #install.packages(arules) library(arules) data(&quot;Adult&quot;) length(Adult) [1] 48842 dim(Adult) [1] 48842 115 Adult transactions in sparse format with 48842 transactions (rows) and 115 items (columns) inspect(Adult[1:2]) items transactionID [1] {age=Middle-aged, workclass=State-gov, education=Bachelors, marital-status=Never-married, occupation=Adm-clerical, relationship=Not-in-family, race=White, sex=Male, capital-gain=Low, capital-loss=None, hours-per-week=Full-time, native-country=United-States, income=small} 1 [2] {age=Senior, workclass=Self-emp-not-inc, education=Bachelors, marital-status=Married-civ-spouse, occupation=Exec-managerial, relationship=Husband, race=White, sex=Male, capital-gain=None, capital-loss=None, hours-per-week=Part-time, native-country=United-States, income=small} 2 Para calcular las reglas de asociación: data(&quot;Adult&quot;) rules &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9, target = &quot;rules&quot;)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.9 0.1 1 none FALSE TRUE 5 0.5 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[0 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.02s]. sorting and recoding items … [9 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. writing … [52 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. summary(rules) set of 52 rules rule length distribution (lhs + rhs):sizes 1 2 3 4 2 13 24 13 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 3.000 2.923 3.250 4.000 summary of quality measures: support confidence coverage lift Min. :0.5084 Min. :0.9031 Min. :0.5406 Min. :0.9844 1st Qu.:0.5415 1st Qu.:0.9155 1st Qu.:0.5875 1st Qu.:0.9937 Median :0.5974 Median :0.9229 Median :0.6293 Median :0.9997 Mean :0.6436 Mean :0.9308 Mean :0.6915 Mean :1.0036 3rd Qu.:0.7426 3rd Qu.:0.9494 3rd Qu.:0.7945 3rd Qu.:1.0057 Max. :0.9533 Max. :0.9583 Max. :1.0000 Max. :1.0586 count Min. :24832 1st Qu.:26447 Median :29178 Mean :31433 3rd Qu.:36269 Max. :46560 mining info: data ntransactions support confidence Adult 48842 0.5 0.9 call apriori(data = Adult, parameter = list(supp = 0.5, conf = 0.9, target = “rules”)) inspect(head(rules)) lhs rhs support confidence [1] {} =&gt; {capital-gain=None} 0.9173867 0.9173867 [2] {} =&gt; {capital-loss=None} 0.9532779 0.9532779 [3] {hours-per-week=Full-time} =&gt; {capital-gain=None} 0.5435895 0.9290688 [4] {hours-per-week=Full-time} =&gt; {capital-loss=None} 0.5606650 0.9582531 [5] {sex=Male} =&gt; {capital-gain=None} 0.6050735 0.9051455 [6] {sex=Male} =&gt; {capital-loss=None} 0.6331027 0.9470750 coverage lift count [1] 1.0000000 1.0000000 44807 [2] 1.0000000 1.0000000 46560 [3] 0.5850907 1.0127342 26550 [4] 0.5850907 1.0052191 27384 [5] 0.6684820 0.9866565 29553 [6] 0.6684820 0.9934931 30922 8.2.1 Parámetros de apriori parameter: lista con las restricciones en el proceso de extracción supp o support conf o confidence minlen - máximo número de items en itemset maxlen - máximo número de items en itemset maxtime - límite de tiempo target - indicar qué tipo de asociaciones queremos extraer: “rules”, “frequent itemsets”, “maximally frequent itemsets”, “closed frequent itemsets”. rules &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9,minlen=2)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.9 0.1 1 none FALSE TRUE 5 0.5 2 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[0 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.02s]. sorting and recoding items … [9 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. writing … [50 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. inspect(tail(rules)) lhs rhs support confidence coverage lift count [1] {workclass=Private, race=White, capital-loss=None} =&gt; {capital-gain=None} 0.5204742 0.9171628 0.5674829 0.9997559 25421 [2] {workclass=Private, capital-gain=None, native-country=United-States} =&gt; {capital-loss=None} 0.5414807 0.9517075 0.5689570 0.9983526 26447 [3] {workclass=Private, capital-loss=None, native-country=United-States} =&gt; {capital-gain=None} 0.5414807 0.9182030 0.5897179 1.0008898 26447 [4] {race=White, capital-gain=None, native-country=United-States} =&gt; {capital-loss=None} 0.6803980 0.9457029 0.7194628 0.9920537 33232 [5] {race=White, capital-loss=None, native-country=United-States} =&gt; {capital-gain=None} 0.6803980 0.9083504 0.7490480 0.9901500 33232 [6] {race=White, capital-gain=None, capital-loss=None} =&gt; {native-country=United-States} 0.6803980 0.9189249 0.7404283 1.0239581 33232 patterns &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9,maxlen=10, target=&quot;frequent itemsets&quot;)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen NA 0.1 1 none FALSE TRUE 5 0.5 1 maxlen target ext 10 frequent itemsets TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[0 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.02s]. sorting and recoding items … [9 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. sorting transactions … done [0.01s]. writing … [49 set(s)] done [0.00s]. creating S4 object … done [0.00s]. summary(patterns) set of 49 itemsets most frequent items: capital-loss=None capital-gain=None 23 21 native-country=United-States race=White 21 19 workclass=Private (Other) 14 20 element (itemset/transaction) length distribution:sizes 1 2 3 4 9 17 17 6 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 2.000 2.408 3.000 4.000 summary of quality measures: support count Min. :0.5051 Min. :24671 1st Qu.:0.5434 1st Qu.:26540 Median :0.5942 Median :29024 Mean :0.6449 Mean :31497 3rd Qu.:0.7404 3rd Qu.:36164 Max. :0.9533 Max. :46560 includes transaction ID lists: FALSE mining info: data ntransactions support confidence Adult 48842 0.5 1 call apriori(data = Adult, parameter = list(supp = 0.5, conf = 0.9, maxlen = 10, target = “frequent itemsets”)) inspect(tail(patterns)) items support count [1] {race=White, sex=Male, capital-loss=None, native-country=United-States} 0.5113632 24976 [2] {sex=Male, capital-gain=None, capital-loss=None, native-country=United-States} 0.5084149 24832 [3] {workclass=Private, race=White, capital-loss=None, native-country=United-States} 0.5181401 25307 [4] {workclass=Private, race=White, capital-gain=None, capital-loss=None} 0.5204742 25421 [5] {workclass=Private, capital-gain=None, capital-loss=None, native-country=United-States} 0.5414807 26447 [6] {race=White, capital-gain=None, capital-loss=None, native-country=United-States} 0.6803980 33232 appearance: especificar restricciones en la extracción de las asociaciones. rules1 &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9), appearance = list(items = c(&quot;income=small&quot;, &quot;sex=Male&quot;))) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.9 0.1 1 none FALSE TRUE 5 0.5 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[2 item(s)] done [0.00s]. set transactions …[2 item(s), 48842 transaction(s)] done [0.01s]. sorting and recoding items … [2 item(s)] done [0.00s]. creating transaction tree … done [0.00s]. checking subsets of size 1 2 done [0.00s]. writing … [0 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. inspect(head(rules1)) rules2 &lt;- apriori(Adult, parameter = list(supp = 0.5, conf = 0.9), appearance = list(none = c(&quot;income=small&quot;, &quot;sex=Male&quot;))) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.9 0.1 1 none FALSE TRUE 5 0.5 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 24421 set item appearances …[2 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.02s]. sorting and recoding items … [7 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 done [0.00s]. writing … [39 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. inspect(head(rules2)) lhs rhs support confidence [1] {} =&gt; {capital-gain=None} 0.9173867 0.9173867 [2] {} =&gt; {capital-loss=None} 0.9532779 0.9532779 [3] {hours-per-week=Full-time} =&gt; {capital-gain=None} 0.5435895 0.9290688 [4] {hours-per-week=Full-time} =&gt; {capital-loss=None} 0.5606650 0.9582531 [5] {workclass=Private} =&gt; {capital-gain=None} 0.6413742 0.9239073 [6] {workclass=Private} =&gt; {capital-loss=None} 0.6639982 0.9564974 coverage lift count [1] 1.0000000 1.000000 44807 [2] 1.0000000 1.000000 46560 [3] 0.5850907 1.012734 26550 [4] 0.5850907 1.005219 27384 [5] 0.6941976 1.007108 31326 [6] 0.6941976 1.003377 32431 8.3 Reglas de Asociación - preprocesamiento Explorando el dataset AdultUCI Contiene los datos del dataset que originalmente se llamó ‘Census Income’ Database en formato data.frame. El dataset Adult del apartado anterior tiene los datos preparados para el cómputo de las reglas. El tipo de datos de este dataset es transactions que es adecuado para el paquete arules. El dataset AdultUCI: library(arules) data(&quot;AdultUCI&quot;) #View(AdultUCI) str(AdultUCI) ‘data.frame’: 48842 obs. of 15 variables: $ age : int 39 50 38 53 28 37 49 52 31 42 … $ workclass : Factor w/ 8 levels “Federal-gov”,..: 7 6 4 4 4 4 4 6 4 4 … $ fnlwgt : int 77516 83311 215646 234721 338409 284582 160187 209642 45781 159449 … $ education : Ord.factor w/ 16 levels “Preschool”&lt;“1st-4th”&lt;..: 14 14 9 7 14 15 5 9 15 14 … $ education-num : int 13 13 9 7 13 14 5 9 14 13 … $ marital-status: Factor w/ 7 levels “Divorced”,“Married-AF-spouse”,..: 5 3 1 3 3 3 4 3 5 3 … $ occupation : Factor w/ 14 levels “Adm-clerical”,..: 1 4 6 6 10 4 8 4 10 4 … $ relationship : Factor w/ 6 levels “Husband”,“Not-in-family”,..: 2 1 2 1 6 6 2 1 2 1 … $ race : Factor w/ 5 levels “Amer-Indian-Eskimo”,..: 5 5 5 3 3 5 3 5 5 5 … $ sex : Factor w/ 2 levels “Female”,“Male”: 2 2 2 2 1 1 1 2 1 2 … $ capital-gain : int 2174 0 0 0 0 0 0 0 14084 5178 … $ capital-loss : int 0 0 0 0 0 0 0 0 0 0 … $ hours-per-week: int 40 13 40 40 40 40 16 45 50 40 … $ native-country: Factor w/ 41 levels “Cambodia”,“Canada”,..: 39 39 39 39 5 39 23 39 39 39 … $ income : Ord.factor w/ 2 levels “small”&lt;“large”: 1 1 1 1 1 1 1 2 2 2 … En la mayoría de los datasets es necesario un primer paso de preprocesamiento. A continuación aplicaremos el preprocesamiento a AdultUCI hasta convertirlo en transacciones que arules maneja adecuadamente. 8.3.1 Discretización de items Borrar algunas columnas que no son interesantes: {} AdultUCI$fnlwgt &lt;-NULL ## o AdultUCI[[&quot;fnlwgt&quot;]] &lt;- NULL AdultUCI$`education-num` &lt;- NULL Convertir a discretos valores numéricos: age, hours-per-week, capital-gain, capital-loss}. Usaremos comandos {} para hacerlo. A continuación un ejemplo de cómo funcionan estos dos comandos: # ejemplo de funcionamiento de cut y ordered v &lt;- 1:100 v2 &lt;- cut(v,c(0,25,50,75,100),labels=c(&quot;bajo&quot;,&quot;medio&quot;,&quot;alto&quot;,&quot;muyalto&quot;)) v3 &lt;- ordered(v2) Aplicamos estas funciones a AdultUCI: AdultUCI$age &lt;- ordered(cut(AdultUCI[[ &quot;age&quot;]], c(15,25,45,65,100)), labels = c(&quot;Young&quot;, &quot;Middle-aged&quot;, &quot;Senior&quot;, &quot;Old&quot;)) AdultUCI[[ &quot;hours-per-week&quot;]] &lt;- ordered(cut(AdultUCI[[ &quot;hours-per-week&quot;]], c(0,25,40,60,168)), labels = c(&quot;Part-time&quot;, &quot;Full-time&quot;, &quot;Over-time&quot;, &quot;Workaholic&quot;)) AdultUCI[[ &quot;capital-gain&quot;]] &lt;- ordered(cut(AdultUCI[[ &quot;capital-gain&quot;]], c(-Inf,0,median(AdultUCI[[ &quot;capital-gain&quot;]][AdultUCI[[ &quot;capital-gain&quot;]]&gt;0]), Inf)), labels = c(&quot;None&quot;, &quot;Low&quot;, &quot;High&quot;)) AdultUCI[[ &quot;capital-loss&quot;]] &lt;- ordered(cut(AdultUCI[[ &quot;capital-loss&quot;]], c(-Inf,0, median(AdultUCI[[ &quot;capital-loss&quot;]][AdultUCI[[ &quot;capital-loss&quot;]]&gt;0]), Inf)), labels = c(&quot;None&quot;, &quot;Low&quot;, &quot;High&quot;)) Llamamos a apriori: reg &lt;- apriori(AdultUCI) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.8 0.1 1 none FALSE TRUE 5 0.1 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 4884 set item appearances …[0 item(s)] done [0.00s]. set transactions …[115 item(s), 48842 transaction(s)] done [0.02s]. sorting and recoding items … [31 item(s)] done [0.00s]. creating transaction tree … done [0.01s]. checking subsets of size 1 2 3 4 5 6 7 8 9 done [0.06s]. writing … [6137 rule(s)] done [0.00s]. creating S4 object … done [0.01s]. inspect(head(reg)) lhs rhs support [1] {} =&gt; {race=White} 0.8550428 [2] {} =&gt; {native-country=United-States} 0.8974243 [3] {} =&gt; {capital-gain=None} 0.9173867 [4] {} =&gt; {capital-loss=None} 0.9532779 [5] {relationship=Unmarried} =&gt; {capital-loss=None} 0.1019819 [6] {occupation=Sales} =&gt; {race=White} 0.1005282 confidence coverage lift count [1] 0.8550428 1.0000000 1.000000 41762 [2] 0.8974243 1.0000000 1.000000 43832 [3] 0.9173867 1.0000000 1.000000 44807 [4] 0.9532779 1.0000000 1.000000 46560 [5] 0.9719024 0.1049302 1.019537 4981 [6] 0.8920785 0.1126899 1.043314 4910 8.3.2 Tipo de dato transactions Ver https://www.r-bloggers.com/data-frames-and-transactions/} Comparamos AdultUCI que hemos procesado con Adult. Adult1 &lt;- as(AdultUCI, &quot;transactions&quot;) class(Adult1) [1] “transactions” attr(,“package”) [1] “arules” length(Adult1) [1] 48842 dim(Adult1) [1] 48842 115 Adult1 transactions in sparse format with 48842 transactions (rows) and 115 items (columns) inspect(Adult1[1:2]) items transactionID [1] {age=Middle-aged, workclass=State-gov, education=Bachelors, marital-status=Never-married, occupation=Adm-clerical, relationship=Not-in-family, race=White, sex=Male, capital-gain=Low, capital-loss=None, hours-per-week=Full-time, native-country=United-States, income=small} 1 [2] {age=Senior, workclass=Self-emp-not-inc, education=Bachelors, marital-status=Married-civ-spouse, occupation=Exec-managerial, relationship=Husband, race=White, sex=Male, capital-gain=None, capital-loss=None, hours-per-week=Part-time, native-country=United-States, income=small} 2 data(&quot;Adult&quot;) class(Adult) [1] “transactions” attr(,“package”) [1] “arules” length(Adult) [1] 48842 dim(Adult) [1] 48842 115 Adult transactions in sparse format with 48842 transactions (rows) and 115 items (columns) inspect(Adult[1:2]) items transactionID [1] {age=Middle-aged, workclass=State-gov, education=Bachelors, marital-status=Never-married, occupation=Adm-clerical, relationship=Not-in-family, race=White, sex=Male, capital-gain=Low, capital-loss=None, hours-per-week=Full-time, native-country=United-States, income=small} 1 [2] {age=Senior, workclass=Self-emp-not-inc, education=Bachelors, marital-status=Married-civ-spouse, occupation=Exec-managerial, relationship=Husband, race=White, sex=Male, capital-gain=None, capital-loss=None, hours-per-week=Part-time, native-country=United-States, income=small} 2 8.3.3 Métodos de arules summary(): Visión del conjunto de reglas. length(): Número de reglas. items(): Elementos involucrados. sort(): Ordenar. subset(): Elementos involucrados. (see help(subset). Seleccionar reglas que cumplan ciertos criterios. union(), intersect(), setequal(), match() (usar ayuda help(xxx) ). write(): Escribir reglas con formato más adequado. data(&quot;Adult&quot;) r1 &lt;- apriori(Adult[1:1000], parameter = list(support = 0.5)) r2 &lt;- apriori(Adult[1001:2000], parameter = list(support = 0.5)) #Convertir en un dataframe dfr1 &lt;-DATAFRAME(r1) r_comb &lt;- c(r1, r2) duplicated(r_comb) intersect(r1,r2) union(r1,r2) lhs(reglas1) rhs(reglas1) class(lhs(reglas1)) 8.4 Ejercicio en Laboratorio Utilizar con el dataset Adult los métodos y operaciones vistos en el presente documento. 8.5 arulesViz Vamos a usar el dataset Groceries para ver los comandos de visualización de reglas de asociación más interesantes. Extraemos las reglas: Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.5 0.1 1 none FALSE TRUE 5 0.001 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 9 set item appearances …[0 item(s)] done [0.00s]. set transactions …[169 item(s), 9835 transaction(s)] done [0.00s]. sorting and recoding items … [157 item(s)] done [0.00s]. creating transaction tree … done [0.00s]. checking subsets of size 1 2 3 4 5 6 done [0.01s]. writing … [5668 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. set of 5668 rules lhs rhs support confidence coverage [1] {honey} =&gt; {whole milk} 0.001118454 0.7333333 0.001525165 [2] {tidbits} =&gt; {rolls/buns} 0.001220132 0.5217391 0.002338587 [3] {cocoa drinks} =&gt; {whole milk} 0.001321810 0.5909091 0.002236909 [4] {pudding powder} =&gt; {whole milk} 0.001321810 0.5652174 0.002338587 [5] {cooking chocolate} =&gt; {whole milk} 0.001321810 0.5200000 0.002541942 [6] {cereals} =&gt; {whole milk} 0.003660397 0.6428571 0.005693950 lift count [1] 2.870009 11 [2] 2.836542 12 [3] 2.312611 13 [4] 2.212062 13 [5] 2.035097 13 [6] 2.515917 36 El comando básico de visualización de reglas es plot. Mostramos distintas opciones de uso de plot (colores). Opción de visualización interactiva: Representación matricial de las reglas: set of 52 rules Itemsets in Antecedent (LHS) [1] “{Instant food products,soda}” [2] “{soda,popcorn}” [3] “{flour,baking powder}” [4] “{ham,processed cheese}” [5] “{whole milk,Instant food products}” [6] “{other vegetables,curd,yogurt,whipped/sour cream}” [7] “{processed cheese,domestic eggs}” [8] “{tropical fruit,other vegetables,yogurt,white bread}” [9] “{hamburger meat,yogurt,whipped/sour cream}” [10] “{tropical fruit,other vegetables,whole milk,yogurt,domestic eggs}” [11] “{liquor,red/blush wine}” [12] “{other vegetables,yogurt,whipped/sour cream,cream cheese }” [13] “{yogurt,whipped/sour cream,hard cheese}” [14] “{tropical fruit,root vegetables,other vegetables,whole milk,rolls/buns}” [15] “{tropical fruit,whole milk,yogurt,sliced cheese}” [16] “{other vegetables,butter,sugar}” [17] “{whole milk,whipped/sour cream,hard cheese}” [18] “{other vegetables,hard cheese,domestic eggs}” [19] “{tropical fruit,other vegetables,whipped/sour cream,fruit/vegetable juice}” [20] “{tropical fruit,onions,yogurt}” [21] “{tropical fruit,other vegetables,yogurt,domestic eggs}” [22] “{butter,yogurt,pastry}” [23] “{whole milk,butter,hard cheese}” [24] “{tropical fruit,other vegetables,butter,fruit/vegetable juice}” [25] “{whole milk,curd,yogurt,cream cheese }” [26] “{tropical fruit,other vegetables,hard cheese}” [27] “{other vegetables,whole milk,whipped/sour cream,napkins}” [28] “{citrus fruit,whole milk,cream cheese }” [29] “{tropical fruit,other vegetables,frozen fish}” [30] “{butter,yogurt,hard cheese}” [31] “{curd,yogurt,sugar}” [32] “{other vegetables,whole milk,butter,soda}” [33] “{whole milk,cream cheese ,sugar}” [34] “{frozen vegetables,specialty chocolate}” [35] “{citrus fruit,other vegetables,whole milk,cream cheese }” [36] “{tropical fruit,whipped/sour cream,shopping bags}” [37] “{citrus fruit,tropical fruit,grapes}” [38] “{other vegetables,butter,hard cheese}” [39] “{whole milk,butter,sliced cheese}” [40] “{citrus fruit,other vegetables,soda,fruit/vegetable juice}” [41] “{tropical fruit,other vegetables,whole milk,yogurt,oil}” [42] “{tropical fruit,grapes,fruit/vegetable juice}” [43] “{frankfurter,tropical fruit,domestic eggs}” [44] “{tropical fruit,whole milk,yogurt,frozen meals}” [45] “{other vegetables,curd,yogurt,cream cheese }” [46] “{root vegetables,whole milk,flour}” [47] “{citrus fruit,whole milk,sugar}” [48] “{tropical fruit,other vegetables,misc. beverages}” [49] “{ham,tropical fruit,other vegetables}” [50] “{citrus fruit,grapes,fruit/vegetable juice}” [51] “{whole milk,whipped/sour cream,rolls/buns,pastry}” Itemsets in Consequent (RHS) [1] “{tropical fruit}” “{citrus fruit}” [3] “{root vegetables}” “{pip fruit}” [5] “{fruit/vegetable juice}” “{domestic eggs}” [7] “{whipped/sour cream}” “{butter}” [9] “{curd}” “{beef}” [11] “{bottled beer}” “{white bread}” [13] “{cream cheese }” “{sugar}” [15] “{salty snack}” “{hamburger meat}” E interactiva: Representación matricial mostrando los items: Representación mediante grafos de las reglas (solo para conjuntos pequeños de reglas): Available control parameters (with default values): layout = stress circular = FALSE ggraphdots = NULL edges = nodes = nodetext = colors = c(“#EE0000FF”, “#EEEEEEFF”) engine = ggplot2 max = 100 verbose = FALSE El paquete Graphviz permite una mejor visualización: Permite visualización dinámica: 8.6 Construyendo un sistema recomendador El dataset lastfm.csv del CV incluye las transacciones recogidas en una radio online que almacena el identificador del usuario, artista, sexo del usuario y el país. Objetivo: Construir un sistema de recomendación de grupos de música a los usuarios a partir de dataset anterior. library(arules) lastfm &lt;- read.csv(&quot;data/lastfm.csv&quot;) lastfm[1:20,] user artist sex country 1 1 red hot chili peppers f Germany 2 1 the black dahlia murder f Germany 3 1 goldfrapp f Germany 4 1 dropkick murphys f Germany 5 1 le tigre f Germany 6 1 schandmaul f Germany 7 1 edguy f Germany 8 1 jack johnson f Germany 9 1 eluveitie f Germany 10 1 the killers f Germany 11 1 judas priest f Germany 12 1 rob zombie f Germany 13 1 john mayer f Germany 14 1 the who f Germany 15 1 guano apes f Germany 16 1 the rolling stones f Germany 17 3 devendra banhart m United States 18 3 boards of canada m United States 19 3 cocorosie m United States 20 3 aphex twin m United States length(lastfm$user) ## 289,955 filas [1] 289955 class(lastfm$user) [1] “integer” # Necesitamos convertir este atributo a factor #para poder analizarlo con paquete {\\tt arules} lastfm$user &lt;- factor(lastfm$user) # Ejecuta en tu ordenador # levels(lastfm$user) ## 15,000 users # levels(lastfm$artist) ## 1,004 artists Llamamos a apriori: reglas1 &lt;- apriori(lastfm,parameter=list(support=.01, confidence=.5)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.5 0.1 1 none FALSE TRUE 5 0.01 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 2899 set item appearances …[0 item(s)] done [0.00s]. set transactions …[16165 item(s), 289955 transaction(s)] done [0.14s]. sorting and recoding items … [21 item(s)] done [0.00s]. creating transaction tree … done [0.03s]. checking subsets of size 1 2 done [0.00s]. writing … [19 rule(s)] done [0.00s]. creating S4 object … done [0.01s]. inspect(reglas1) lhs rhs support confidence coverage [1] {} =&gt; {sex=m} 0.73053750 0.7305375 1.00000000 [2] {country=Czech Republic} =&gt; {sex=m} 0.01039472 0.8033049 0.01293994 [3] {country=Mexico} =&gt; {sex=m} 0.01023607 0.7804365 0.01311583 [4] {country=Norway} =&gt; {sex=m} 0.01239503 0.7744021 0.01600593 [5] {country=Turkey} =&gt; {sex=m} 0.01088445 0.6627467 0.01642324 [6] {country=Italy} =&gt; {sex=m} 0.01501612 0.7615882 0.01971685 [7] {country=France} =&gt; {sex=m} 0.01707161 0.8302583 0.02056181 [8] {country=Australia} =&gt; {sex=m} 0.01497474 0.6776963 0.02209653 [9] {country=Canada} =&gt; {sex=m} 0.01568174 0.6563222 0.02389336 [10] {country=Spain} =&gt; {sex=m} 0.02482109 0.7720446 0.03214982 [11] {country=Netherlands} =&gt; {sex=m} 0.02690417 0.8064716 0.03336035 [12] {country=Finland} =&gt; {sex=m} 0.02661103 0.7596731 0.03502957 [13] {country=Russian Federation} =&gt; {sex=m} 0.03062544 0.7605344 0.04026832 [14] {country=Brazil} =&gt; {sex=m} 0.03001845 0.7300788 0.04111673 [15] {country=Sweden} =&gt; {sex=m} 0.03193254 0.7479603 0.04269283 [16] {country=Poland} =&gt; {sex=m} 0.03854391 0.6531471 0.05901261 [17] {country=Germany} =&gt; {sex=m} 0.06069907 0.7257433 0.08363712 [18] {country=United Kingdom} =&gt; {sex=m} 0.07730510 0.8110211 0.09531824 [19] {country=United States} =&gt; {sex=m} 0.13852839 0.6744182 0.20540429 lift count [1] 1.0000000 211823 [2] 1.0996080 3014 [3] 1.0683045 2968 [4] 1.0600442 3594 [5] 0.9072043 3156 [6] 1.0425040 4354 [7] 1.1365033 4950 [8] 0.9276680 4342 [9] 0.8984100 4547 [10] 1.0568172 7197 [11] 1.1039428 7801 [12] 1.0398825 7716 [13] 1.0410615 8880 [14] 0.9993722 8704 [15] 1.0238492 9259 [16] 0.8940638 11176 [17] 0.9934374 17600 [18] 1.1101703 22415 [19] 0.9231808 40167 Comentario: En versiones anteriores de arules el anterior comando daba error. Teníamos que convertir a factor las variables discretas. Es un paquete vivo que va evolucionando día a día. ¿Cual es la recomendación que podemos obtener con estas reglas? No es el tipo de reglas que queremos obtener para nuestro sistema de recomendación. Los datos deben ser manipulados para poder encontrar lo que nos interesa. Usaremos los Comandos: split, lapply. Primero me quedo con una lista de lo que escucha cada usuario: lista.musica.por.usuario &lt;- split(x=lastfm[,&quot;artist&quot;],f=lastfm$user) lista.musica.por.usuario[1:2] $1 [1] “red hot chili peppers” “the black dahlia murder” [3] “goldfrapp” “dropkick murphys” [5] “le tigre” “schandmaul” [7] “edguy” “jack johnson” [9] “eluveitie” “the killers” [11] “judas priest” “rob zombie” [13] “john mayer” “the who” [15] “guano apes” “the rolling stones” $3 [1] “devendra banhart” “boards of canada” “cocorosie” [4] “aphex twin” “animal collective” “atmosphere” [7] “joanna newsom” “air” “portishead” [10] “massive attack” “broken social scene” “arcade fire” [13] “plaid” “prefuse 73” “m83” [16] “the flashbulb” “pavement” “goldfrapp” [19] “amon tobin” “sage francis” “four tet” [22] “max richter” “autechre” “radiohead” [25] “neutral milk hotel” “beastie boys” “aesop rock” [28] “mf doom” “the books” A continuación: Un grupo/cantante podría estar dos veces en un usuario: eliminar repeticiones Convertir a formato transacciones Mirar la música escuchada por los primeros usuarios ## Eliminar duplicados lista.musica.por.usuario &lt;- lapply(lista.musica.por.usuario,unique) # Convertimos en transacciones la lista de música. lista.musica.por.usuario1 &lt;- as(lista.musica.por.usuario,&quot;transactions&quot;) lista.musica.por.usuario[1:5] $1 [1] “red hot chili peppers” “the black dahlia murder” [3] “goldfrapp” “dropkick murphys” [5] “le tigre” “schandmaul” [7] “edguy” “jack johnson” [9] “eluveitie” “the killers” [11] “judas priest” “rob zombie” [13] “john mayer” “the who” [15] “guano apes” “the rolling stones” $3 [1] “devendra banhart” “boards of canada” “cocorosie” [4] “aphex twin” “animal collective” “atmosphere” [7] “joanna newsom” “air” “portishead” [10] “massive attack” “broken social scene” “arcade fire” [13] “plaid” “prefuse 73” “m83” [16] “the flashbulb” “pavement” “goldfrapp” [19] “amon tobin” “sage francis” “four tet” [22] “max richter” “autechre” “radiohead” [25] “neutral milk hotel” “beastie boys” “aesop rock” [28] “mf doom” “the books” $4 [1] “tv on the radio” “tool” [3] “kyuss” “dj shadow” [5] “air” “a tribe called quest” [7] “the cinematic orchestra” “beck” [9] “bon iver” “röyksopp” [11] “bonobo” “the decemberists” [13] “snow patrol” “battles” [15] “the prodigy” “pink floyd” [17] “rjd2” “the flaming lips” [19] “michael jackson” “mgmt” [21] “the rolling stones” “late of the pier” [23] “flight of the conchords” “simian mobile disco” [25] “muse” “fleetwood mac” [27] “led zeppelin” $5 [1] “dream theater” “ac/dc” [3] “metallica” “iron maiden” [5] “bob marley &amp; the wailers” “megadeth” [7] “children of bodom” “trivium” [9] “nightwish” “sublime” [11] “volbeat” $6 [1] “lily allen” “kanye west” “sigur rós” [4] “pink floyd” “stevie wonder” “metallica” [7] “thievery corporation” “iron maiden” “the streets” [10] “muse” “faith no more” “manu chao” [13] “tenacious d” “depeche mode” “justin timberlake” [16] “green day” “snow patrol” “dream theater” [19] “u2” “jay-z” “type o negative” [22] “pearl jam” “queen” # en la versión actual de R esto va bien #error ¿? (en versiones anteriores de R daba error, si os pasa intentar siguientes comandos) #lista.musica.por.usuario2 &lt;- as(lapply(lista.musica.por.usuario, &quot;[[&quot;, 1), &quot;transactions&quot;) #lista.musica.por.usuario2 Visualizamos lo que hemos conseguido hasta el momento: str(lista.musica.por.usuario1) Formal class ‘transactions’ [package “arules”] with 3 slots ..@ data :Formal class ‘ngCMatrix’ [package “Matrix”] with 5 slots .. .. ..@ i : int [1:289953] 280 288 299 373 383 429 457 468 512 714 … .. .. ..@ p : int [1:15001] 0 16 45 72 83 106 128 147 177 184 … .. .. ..@ Dim : int [1:2] 1004 15000 .. .. ..@ Dimnames:List of 2 .. .. .. ..$ : NULL .. .. .. ..$ : NULL .. .. ..@ factors : list() ..@ itemInfo :‘data.frame’: 1004 obs. of 1 variable: .. ..$ labels: chr [1:1004] “…and you will know us by the trail of dead” “[unknown]” “2pac” “3 doors down” … ..@ itemsetInfo:‘data.frame’: 15000 obs. of 1 variable: .. ..$ transactionID: chr [1:15000] “1” “3” “4” “5” … write(head(lista.musica.por.usuario1)) “dropkick murphys” “edguy” “eluveitie” “goldfrapp” “guano apes” “jack johnson” “john mayer” “judas priest” “le tigre” “red hot chili peppers” “rob zombie” “schandmaul” “the black dahlia murder” “the killers” “the rolling stones” “the who” “aesop rock” “air” “amon tobin” “animal collective” “aphex twin” “arcade fire” “atmosphere” “autechre” “beastie boys” “boards of canada” “broken social scene” “cocorosie” “devendra banhart” “four tet” “goldfrapp” “joanna newsom” “m83” “massive attack” “max richter” “mf doom” “neutral milk hotel” “pavement” “plaid” “portishead” “prefuse 73” “radiohead” “sage francis” “the books” “the flashbulb” “a tribe called quest” “air” “battles” “beck” “bon iver” “bonobo” “dj shadow” “fleetwood mac” “flight of the conchords” “kyuss” “late of the pier” “led zeppelin” “mgmt” “michael jackson” “muse” “pink floyd” “rjd2” “röyksopp” “simian mobile disco” “snow patrol” “the cinematic orchestra” “the decemberists” “the flaming lips” “the prodigy” “the rolling stones” “tool” “tv on the radio” “ac/dc” “bob marley &amp; the wailers” “children of bodom” “dream theater” “iron maiden” “megadeth” “metallica” “nightwish” “sublime” “trivium” “volbeat” “depeche mode” “dream theater” “faith no more” “green day” “iron maiden” “jay-z” “justin timberlake” “kanye west” “lily allen” “manu chao” “metallica” “muse” “pearl jam” “pink floyd” “queen” “sigur rós” “snow patrol” “stevie wonder” “tenacious d” “the streets” “thievery corporation” “type o negative” “u2” “ac/dc” “aerosmith” “alice in chains” “audioslave” “buckethead” “camel” “disturbed” “dream theater” “jethro tull” “king crimson” “led zeppelin” “oasis” “pearl jam” “pink floyd” “porcupine tree” “rammstein” “rush” “soundgarden” “stone temple pilots” “the verve” “tool” “type o negative” write(head(lista.musica.por.usuario1),format=&quot;single&quot;) “1” “dropkick murphys” “1” “edguy” “1” “eluveitie” “1” “goldfrapp” “1” “guano apes” “1” “jack johnson” “1” “john mayer” “1” “judas priest” “1” “le tigre” “1” “red hot chili peppers” “1” “rob zombie” “1” “schandmaul” “1” “the black dahlia murder” “1” “the killers” “1” “the rolling stones” “1” “the who” “3” “aesop rock” “3” “air” “3” “amon tobin” “3” “animal collective” “3” “aphex twin” “3” “arcade fire” “3” “atmosphere” “3” “autechre” “3” “beastie boys” “3” “boards of canada” “3” “broken social scene” “3” “cocorosie” “3” “devendra banhart” “3” “four tet” “3” “goldfrapp” “3” “joanna newsom” “3” “m83” “3” “massive attack” “3” “max richter” “3” “mf doom” “3” “neutral milk hotel” “3” “pavement” “3” “plaid” “3” “portishead” “3” “prefuse 73” “3” “radiohead” “3” “sage francis” “3” “the books” “3” “the flashbulb” “4” “a tribe called quest” “4” “air” “4” “battles” “4” “beck” “4” “bon iver” “4” “bonobo” “4” “dj shadow” “4” “fleetwood mac” “4” “flight of the conchords” “4” “kyuss” “4” “late of the pier” “4” “led zeppelin” “4” “mgmt” “4” “michael jackson” “4” “muse” “4” “pink floyd” “4” “rjd2” “4” “röyksopp” “4” “simian mobile disco” “4” “snow patrol” “4” “the cinematic orchestra” “4” “the decemberists” “4” “the flaming lips” “4” “the prodigy” “4” “the rolling stones” “4” “tool” “4” “tv on the radio” “5” “ac/dc” “5” “bob marley &amp; the wailers” “5” “children of bodom” “5” “dream theater” “5” “iron maiden” “5” “megadeth” “5” “metallica” “5” “nightwish” “5” “sublime” “5” “trivium” “5” “volbeat” “6” “depeche mode” “6” “dream theater” “6” “faith no more” “6” “green day” “6” “iron maiden” “6” “jay-z” “6” “justin timberlake” “6” “kanye west” “6” “lily allen” “6” “manu chao” “6” “metallica” “6” “muse” “6” “pearl jam” “6” “pink floyd” “6” “queen” “6” “sigur rós” “6” “snow patrol” “6” “stevie wonder” “6” “tenacious d” “6” “the streets” “6” “thievery corporation” “6” “type o negative” “6” “u2” “7” “ac/dc” “7” “aerosmith” “7” “alice in chains” “7” “audioslave” “7” “buckethead” “7” “camel” “7” “disturbed” “7” “dream theater” “7” “jethro tull” “7” “king crimson” “7” “led zeppelin” “7” “oasis” “7” “pearl jam” “7” “pink floyd” “7” “porcupine tree” “7” “rammstein” “7” “rush” “7” “soundgarden” “7” “stone temple pilots” “7” “the verve” “7” “tool” “7” “type o negative” Es una lista de transacciones - clase de datos definida en arules. Calculamos la frecuencia relativa de las canciones escuchadas: itfreq1 &lt;-itemFrequency(lista.musica.por.usuario1) head(itfreq1) …and you will know us by the trail of dead 0.009800000 [unknown] 0.036866667 2pac 0.022733333 3 doors down 0.030933333 30 seconds to mars 0.032800000 311 0.008333333 itfreq1: es una vector numérico los nombres de la lista (names(itfreq), los nombres de cada grupo ) cada posición por tanto es la frecuencia del grupo de esa posición Dibujar las frecuencias usando la lista de transacciones obtenida: itemFrequencyPlot(lista.musica.por.usuario1,support=.08,cex.names=1) Y obtenemos las reglas de asociación con soporte 0.1 y confianza 0.5: reglas2 &lt;- apriori(lista.musica.por.usuario1,parameter= list(support=.01, confidence=.5)) Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 0.5 0.1 1 none FALSE TRUE 5 0.01 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 150 set item appearances …[0 item(s)] done [0.00s]. set transactions …[1004 item(s), 15000 transaction(s)] done [0.03s]. sorting and recoding items … [655 item(s)] done [0.00s]. creating transaction tree … done [0.00s]. checking subsets of size 1 2 3 4 done [0.01s]. writing … [50 rule(s)] done [0.00s]. creating S4 object … done [0.00s]. reglas2 set of 50 rules inspect(reglas2) lhs rhs support [1] {t.i.} =&gt; {kanye west} 0.01040000 [2] {the pussycat dolls} =&gt; {rihanna} 0.01040000 [3] {the fray} =&gt; {coldplay} 0.01126667 [4] {sonata arctica} =&gt; {nightwish} 0.01346667 [5] {judas priest} =&gt; {iron maiden} 0.01353333 [6] {the kinks} =&gt; {the beatles} 0.01360000 [7] {travis} =&gt; {coldplay} 0.01373333 [8] {the flaming lips} =&gt; {radiohead} 0.01306667 [9] {megadeth} =&gt; {metallica} 0.01626667 [10] {simon &amp; garfunkel} =&gt; {the beatles} 0.01540000 [11] {broken social scene} =&gt; {radiohead} 0.01506667 [12] {blur} =&gt; {radiohead} 0.01753333 [13] {keane} =&gt; {coldplay} 0.02226667 [14] {snow patrol} =&gt; {coldplay} 0.02646667 [15] {beck} =&gt; {radiohead} 0.02926667 [16] {snow patrol, the killers} =&gt; {coldplay} 0.01040000 [17] {radiohead, snow patrol} =&gt; {coldplay} 0.01006667 [18] {death cab for cutie, the shins} =&gt; {radiohead} 0.01006667 [19] {the beatles, the shins} =&gt; {radiohead} 0.01066667 [20] {led zeppelin, the doors} =&gt; {pink floyd} 0.01066667 [21] {pink floyd, the doors} =&gt; {led zeppelin} 0.01066667 [22] {pink floyd, the doors} =&gt; {the beatles} 0.01000000 [23] {the beatles, the strokes} =&gt; {radiohead} 0.01046667 [24] {oasis, the killers} =&gt; {coldplay} 0.01113333 [25] {oasis, the beatles} =&gt; {coldplay} 0.01060000 [26] {oasis, radiohead} =&gt; {coldplay} 0.01273333 [27] {beck, the beatles} =&gt; {radiohead} 0.01300000 [28] {bob dylan, the rolling stones} =&gt; {the beatles} 0.01146667 [29] {david bowie, the rolling stones} =&gt; {the beatles} 0.01000000 [30] {led zeppelin, the rolling stones} =&gt; {the beatles} 0.01066667 [31] {radiohead, the rolling stones} =&gt; {the beatles} 0.01060000 [32] {coldplay, the smashing pumpkins} =&gt; {radiohead} 0.01093333 [33] {the beatles, the smashing pumpkins} =&gt; {radiohead} 0.01146667 [34] {radiohead, u2} =&gt; {coldplay} 0.01140000 [35] {coldplay, sigur rós} =&gt; {radiohead} 0.01206667 [36] {sigur rós, the beatles} =&gt; {radiohead} 0.01046667 [37] {bob dylan, pink floyd} =&gt; {the beatles} 0.01033333 [38] {bob dylan, radiohead} =&gt; {the beatles} 0.01386667 [39] {bloc party, the killers} =&gt; {coldplay} 0.01106667 [40] {david bowie, pink floyd} =&gt; {the beatles} 0.01006667 [41] {david bowie, radiohead} =&gt; {the beatles} 0.01393333 [42] {placebo, radiohead} =&gt; {muse} 0.01366667 [43] {led zeppelin, radiohead} =&gt; {the beatles} 0.01306667 [44] {death cab for cutie, the killers} =&gt; {coldplay} 0.01086667 [45] {death cab for cutie, the beatles} =&gt; {radiohead} 0.01246667 [46] {muse, the killers} =&gt; {coldplay} 0.01513333 [47] {red hot chili peppers, the killers} =&gt; {coldplay} 0.01086667 [48] {the beatles, the killers} =&gt; {coldplay} 0.01253333 [49] {radiohead, the killers} =&gt; {coldplay} 0.01506667 [50] {muse, the beatles} =&gt; {radiohead} 0.01380000 confidence coverage lift count [1] 0.5672727 0.01833333 8.854413 156 [2] 0.5777778 0.01800000 13.415893 156 [3] 0.5168196 0.02180000 3.260006 169 [4] 0.5101010 0.02640000 8.236292 202 [5] 0.5075000 0.02666667 8.562992 203 [6] 0.5298701 0.02566667 2.979030 204 [7] 0.5628415 0.02440000 3.550304 206 [8] 0.5297297 0.02466667 2.938589 196 [9] 0.5281385 0.03080000 4.743759 244 [10] 0.5238095 0.02940000 2.944956 231 [11] 0.5472155 0.02753333 3.035589 226 [12] 0.5228628 0.03353333 2.900496 263 [13] 0.6374046 0.03493333 4.020634 334 [14] 0.5251323 0.05040000 3.312441 397 [15] 0.5092807 0.05746667 2.825152 439 [16] 0.5954198 0.01746667 3.755802 156 [17] 0.6344538 0.01586667 4.002021 151 [18] 0.5033333 0.02000000 2.792160 151 [19] 0.5673759 0.01880000 3.147425 160 [20] 0.5970149 0.01786667 5.689469 160 [21] 0.5387205 0.01980000 6.802027 160 [22] 0.5050505 0.01980000 2.839489 150 [23] 0.5607143 0.01866667 3.110471 157 [24] 0.6626984 0.01680000 4.180183 167 [25] 0.5196078 0.02040000 3.277594 159 [26] 0.5876923 0.02166667 3.707058 191 [27] 0.5909091 0.02200000 3.277972 195 [28] 0.5910653 0.01940000 3.323081 172 [29] 0.5703422 0.01753333 3.206572 150 [30] 0.5776173 0.01846667 3.247474 160 [31] 0.5638298 0.01880000 3.169958 159 [32] 0.6283525 0.01740000 3.485683 164 [33] 0.6209386 0.01846667 3.444556 172 [34] 0.5213415 0.02186667 3.288529 171 [35] 0.5801282 0.02080000 3.218167 181 [36] 0.6434426 0.01626667 3.569393 157 [37] 0.6150794 0.01680000 3.458092 155 [38] 0.5730028 0.02420000 3.221530 208 [39] 0.5236593 0.02113333 3.303150 166 [40] 0.5741445 0.01753333 3.227949 151 [41] 0.5225000 0.02666667 2.937594 209 [42] 0.5137845 0.02660000 4.504247 205 [43] 0.5283019 0.02473333 2.970213 196 [44] 0.5884477 0.01846667 3.711823 163 [45] 0.5013405 0.02486667 2.781105 187 [46] 0.5089686 0.02973333 3.210483 227 [47] 0.5093750 0.02133333 3.213047 163 [48] 0.5340909 0.02346667 3.368950 188 [49] 0.5243619 0.02873333 3.307582 226 [50] 0.5073529 0.02720000 2.814458 207 Primero nos quedamos con las reglas más interesantes. Filtramos aquellas con lift mayor que 1: inspect(subset(reglas2, subset=lift &gt; 1)) lhs rhs support [1] {t.i.} =&gt; {kanye west} 0.01040000 [2] {the pussycat dolls} =&gt; {rihanna} 0.01040000 [3] {the fray} =&gt; {coldplay} 0.01126667 [4] {sonata arctica} =&gt; {nightwish} 0.01346667 [5] {judas priest} =&gt; {iron maiden} 0.01353333 [6] {the kinks} =&gt; {the beatles} 0.01360000 [7] {travis} =&gt; {coldplay} 0.01373333 [8] {the flaming lips} =&gt; {radiohead} 0.01306667 [9] {megadeth} =&gt; {metallica} 0.01626667 [10] {simon &amp; garfunkel} =&gt; {the beatles} 0.01540000 [11] {broken social scene} =&gt; {radiohead} 0.01506667 [12] {blur} =&gt; {radiohead} 0.01753333 [13] {keane} =&gt; {coldplay} 0.02226667 [14] {snow patrol} =&gt; {coldplay} 0.02646667 [15] {beck} =&gt; {radiohead} 0.02926667 [16] {snow patrol, the killers} =&gt; {coldplay} 0.01040000 [17] {radiohead, snow patrol} =&gt; {coldplay} 0.01006667 [18] {death cab for cutie, the shins} =&gt; {radiohead} 0.01006667 [19] {the beatles, the shins} =&gt; {radiohead} 0.01066667 [20] {led zeppelin, the doors} =&gt; {pink floyd} 0.01066667 [21] {pink floyd, the doors} =&gt; {led zeppelin} 0.01066667 [22] {pink floyd, the doors} =&gt; {the beatles} 0.01000000 [23] {the beatles, the strokes} =&gt; {radiohead} 0.01046667 [24] {oasis, the killers} =&gt; {coldplay} 0.01113333 [25] {oasis, the beatles} =&gt; {coldplay} 0.01060000 [26] {oasis, radiohead} =&gt; {coldplay} 0.01273333 [27] {beck, the beatles} =&gt; {radiohead} 0.01300000 [28] {bob dylan, the rolling stones} =&gt; {the beatles} 0.01146667 [29] {david bowie, the rolling stones} =&gt; {the beatles} 0.01000000 [30] {led zeppelin, the rolling stones} =&gt; {the beatles} 0.01066667 [31] {radiohead, the rolling stones} =&gt; {the beatles} 0.01060000 [32] {coldplay, the smashing pumpkins} =&gt; {radiohead} 0.01093333 [33] {the beatles, the smashing pumpkins} =&gt; {radiohead} 0.01146667 [34] {radiohead, u2} =&gt; {coldplay} 0.01140000 [35] {coldplay, sigur rós} =&gt; {radiohead} 0.01206667 [36] {sigur rós, the beatles} =&gt; {radiohead} 0.01046667 [37] {bob dylan, pink floyd} =&gt; {the beatles} 0.01033333 [38] {bob dylan, radiohead} =&gt; {the beatles} 0.01386667 [39] {bloc party, the killers} =&gt; {coldplay} 0.01106667 [40] {david bowie, pink floyd} =&gt; {the beatles} 0.01006667 [41] {david bowie, radiohead} =&gt; {the beatles} 0.01393333 [42] {placebo, radiohead} =&gt; {muse} 0.01366667 [43] {led zeppelin, radiohead} =&gt; {the beatles} 0.01306667 [44] {death cab for cutie, the killers} =&gt; {coldplay} 0.01086667 [45] {death cab for cutie, the beatles} =&gt; {radiohead} 0.01246667 [46] {muse, the killers} =&gt; {coldplay} 0.01513333 [47] {red hot chili peppers, the killers} =&gt; {coldplay} 0.01086667 [48] {the beatles, the killers} =&gt; {coldplay} 0.01253333 [49] {radiohead, the killers} =&gt; {coldplay} 0.01506667 [50] {muse, the beatles} =&gt; {radiohead} 0.01380000 confidence coverage lift count [1] 0.5672727 0.01833333 8.854413 156 [2] 0.5777778 0.01800000 13.415893 156 [3] 0.5168196 0.02180000 3.260006 169 [4] 0.5101010 0.02640000 8.236292 202 [5] 0.5075000 0.02666667 8.562992 203 [6] 0.5298701 0.02566667 2.979030 204 [7] 0.5628415 0.02440000 3.550304 206 [8] 0.5297297 0.02466667 2.938589 196 [9] 0.5281385 0.03080000 4.743759 244 [10] 0.5238095 0.02940000 2.944956 231 [11] 0.5472155 0.02753333 3.035589 226 [12] 0.5228628 0.03353333 2.900496 263 [13] 0.6374046 0.03493333 4.020634 334 [14] 0.5251323 0.05040000 3.312441 397 [15] 0.5092807 0.05746667 2.825152 439 [16] 0.5954198 0.01746667 3.755802 156 [17] 0.6344538 0.01586667 4.002021 151 [18] 0.5033333 0.02000000 2.792160 151 [19] 0.5673759 0.01880000 3.147425 160 [20] 0.5970149 0.01786667 5.689469 160 [21] 0.5387205 0.01980000 6.802027 160 [22] 0.5050505 0.01980000 2.839489 150 [23] 0.5607143 0.01866667 3.110471 157 [24] 0.6626984 0.01680000 4.180183 167 [25] 0.5196078 0.02040000 3.277594 159 [26] 0.5876923 0.02166667 3.707058 191 [27] 0.5909091 0.02200000 3.277972 195 [28] 0.5910653 0.01940000 3.323081 172 [29] 0.5703422 0.01753333 3.206572 150 [30] 0.5776173 0.01846667 3.247474 160 [31] 0.5638298 0.01880000 3.169958 159 [32] 0.6283525 0.01740000 3.485683 164 [33] 0.6209386 0.01846667 3.444556 172 [34] 0.5213415 0.02186667 3.288529 171 [35] 0.5801282 0.02080000 3.218167 181 [36] 0.6434426 0.01626667 3.569393 157 [37] 0.6150794 0.01680000 3.458092 155 [38] 0.5730028 0.02420000 3.221530 208 [39] 0.5236593 0.02113333 3.303150 166 [40] 0.5741445 0.01753333 3.227949 151 [41] 0.5225000 0.02666667 2.937594 209 [42] 0.5137845 0.02660000 4.504247 205 [43] 0.5283019 0.02473333 2.970213 196 [44] 0.5884477 0.01846667 3.711823 163 [45] 0.5013405 0.02486667 2.781105 187 [46] 0.5089686 0.02973333 3.210483 227 [47] 0.5093750 0.02133333 3.213047 163 [48] 0.5340909 0.02346667 3.368950 188 [49] 0.5243619 0.02873333 3.307582 226 [50] 0.5073529 0.02720000 2.814458 207 Ordenamos por confianza estas reglas anteriores: inspect(sort(subset(reglas2, subset=lift &gt; 1), by=&quot;confidence&quot;)) lhs rhs support [1] {oasis, the killers} =&gt; {coldplay} 0.01113333 [2] {sigur rós, the beatles} =&gt; {radiohead} 0.01046667 [3] {keane} =&gt; {coldplay} 0.02226667 [4] {radiohead, snow patrol} =&gt; {coldplay} 0.01006667 [5] {coldplay, the smashing pumpkins} =&gt; {radiohead} 0.01093333 [6] {the beatles, the smashing pumpkins} =&gt; {radiohead} 0.01146667 [7] {bob dylan, pink floyd} =&gt; {the beatles} 0.01033333 [8] {led zeppelin, the doors} =&gt; {pink floyd} 0.01066667 [9] {snow patrol, the killers} =&gt; {coldplay} 0.01040000 [10] {bob dylan, the rolling stones} =&gt; {the beatles} 0.01146667 [11] {beck, the beatles} =&gt; {radiohead} 0.01300000 [12] {death cab for cutie, the killers} =&gt; {coldplay} 0.01086667 [13] {oasis, radiohead} =&gt; {coldplay} 0.01273333 [14] {coldplay, sigur rós} =&gt; {radiohead} 0.01206667 [15] {the pussycat dolls} =&gt; {rihanna} 0.01040000 [16] {led zeppelin, the rolling stones} =&gt; {the beatles} 0.01066667 [17] {david bowie, pink floyd} =&gt; {the beatles} 0.01006667 [18] {bob dylan, radiohead} =&gt; {the beatles} 0.01386667 [19] {david bowie, the rolling stones} =&gt; {the beatles} 0.01000000 [20] {the beatles, the shins} =&gt; {radiohead} 0.01066667 [21] {t.i.} =&gt; {kanye west} 0.01040000 [22] {radiohead, the rolling stones} =&gt; {the beatles} 0.01060000 [23] {travis} =&gt; {coldplay} 0.01373333 [24] {the beatles, the strokes} =&gt; {radiohead} 0.01046667 [25] {broken social scene} =&gt; {radiohead} 0.01506667 [26] {pink floyd, the doors} =&gt; {led zeppelin} 0.01066667 [27] {the beatles, the killers} =&gt; {coldplay} 0.01253333 [28] {the kinks} =&gt; {the beatles} 0.01360000 [29] {the flaming lips} =&gt; {radiohead} 0.01306667 [30] {led zeppelin, radiohead} =&gt; {the beatles} 0.01306667 [31] {megadeth} =&gt; {metallica} 0.01626667 [32] {snow patrol} =&gt; {coldplay} 0.02646667 [33] {radiohead, the killers} =&gt; {coldplay} 0.01506667 [34] {simon &amp; garfunkel} =&gt; {the beatles} 0.01540000 [35] {bloc party, the killers} =&gt; {coldplay} 0.01106667 [36] {blur} =&gt; {radiohead} 0.01753333 [37] {david bowie, radiohead} =&gt; {the beatles} 0.01393333 [38] {radiohead, u2} =&gt; {coldplay} 0.01140000 [39] {oasis, the beatles} =&gt; {coldplay} 0.01060000 [40] {the fray} =&gt; {coldplay} 0.01126667 [41] {placebo, radiohead} =&gt; {muse} 0.01366667 [42] {sonata arctica} =&gt; {nightwish} 0.01346667 [43] {red hot chili peppers, the killers} =&gt; {coldplay} 0.01086667 [44] {beck} =&gt; {radiohead} 0.02926667 [45] {muse, the killers} =&gt; {coldplay} 0.01513333 [46] {judas priest} =&gt; {iron maiden} 0.01353333 [47] {muse, the beatles} =&gt; {radiohead} 0.01380000 [48] {pink floyd, the doors} =&gt; {the beatles} 0.01000000 [49] {death cab for cutie, the shins} =&gt; {radiohead} 0.01006667 [50] {death cab for cutie, the beatles} =&gt; {radiohead} 0.01246667 confidence coverage lift count [1] 0.6626984 0.01680000 4.180183 167 [2] 0.6434426 0.01626667 3.569393 157 [3] 0.6374046 0.03493333 4.020634 334 [4] 0.6344538 0.01586667 4.002021 151 [5] 0.6283525 0.01740000 3.485683 164 [6] 0.6209386 0.01846667 3.444556 172 [7] 0.6150794 0.01680000 3.458092 155 [8] 0.5970149 0.01786667 5.689469 160 [9] 0.5954198 0.01746667 3.755802 156 [10] 0.5910653 0.01940000 3.323081 172 [11] 0.5909091 0.02200000 3.277972 195 [12] 0.5884477 0.01846667 3.711823 163 [13] 0.5876923 0.02166667 3.707058 191 [14] 0.5801282 0.02080000 3.218167 181 [15] 0.5777778 0.01800000 13.415893 156 [16] 0.5776173 0.01846667 3.247474 160 [17] 0.5741445 0.01753333 3.227949 151 [18] 0.5730028 0.02420000 3.221530 208 [19] 0.5703422 0.01753333 3.206572 150 [20] 0.5673759 0.01880000 3.147425 160 [21] 0.5672727 0.01833333 8.854413 156 [22] 0.5638298 0.01880000 3.169958 159 [23] 0.5628415 0.02440000 3.550304 206 [24] 0.5607143 0.01866667 3.110471 157 [25] 0.5472155 0.02753333 3.035589 226 [26] 0.5387205 0.01980000 6.802027 160 [27] 0.5340909 0.02346667 3.368950 188 [28] 0.5298701 0.02566667 2.979030 204 [29] 0.5297297 0.02466667 2.938589 196 [30] 0.5283019 0.02473333 2.970213 196 [31] 0.5281385 0.03080000 4.743759 244 [32] 0.5251323 0.05040000 3.312441 397 [33] 0.5243619 0.02873333 3.307582 226 [34] 0.5238095 0.02940000 2.944956 231 [35] 0.5236593 0.02113333 3.303150 166 [36] 0.5228628 0.03353333 2.900496 263 [37] 0.5225000 0.02666667 2.937594 209 [38] 0.5213415 0.02186667 3.288529 171 [39] 0.5196078 0.02040000 3.277594 159 [40] 0.5168196 0.02180000 3.260006 169 [41] 0.5137845 0.02660000 4.504247 205 [42] 0.5101010 0.02640000 8.236292 202 [43] 0.5093750 0.02133333 3.213047 163 [44] 0.5092807 0.05746667 2.825152 439 [45] 0.5089686 0.02973333 3.210483 227 [46] 0.5075000 0.02666667 8.562992 203 [47] 0.5073529 0.02720000 2.814458 207 [48] 0.5050505 0.01980000 2.839489 150 [49] 0.5033333 0.02000000 2.792160 151 [50] 0.5013405 0.02486667 2.781105 187 ¿Recomendación a usuarios que escuchan Coldplay?: r1 &lt;-subset(reglas2, subset = lhs %ain% c(&quot;coldplay&quot;)) inspect(r1) lhs rhs support confidence [1] {coldplay, the smashing pumpkins} =&gt; {radiohead} 0.01093333 0.6283525 [2] {coldplay, sigur rós} =&gt; {radiohead} 0.01206667 0.5801282 coverage lift count [1] 0.0174 3.485683 164 [2] 0.0208 3.218167 181 Probad con otros grupos. "],["formal-concept-analysis.html", "Capítulo 9 Formal Concept Analysis 9.1 Background in FCA 9.2 Working with Formal Contexts - datasets 9.3 Concept Lattice 9.4 Exercises 9.5 Implications in FCA 9.6 Exercise 9.7 Simplification Logic for Mushroom Dataset”", " Capítulo 9 Formal Concept Analysis Port-Royal logic (traditional logic): formal notion of concept, Arnauld A., Nicole P.: La logique ou l’art de penser, 1662 (Logic Or The Art Of Thinking, CUP, 2003): concept = extent (objects) + intent (attributes) G. Birkhoff (1940s): work on lattices and related mathematical structures, emphasizes applicational aspects of lattices in data analysis. Barbut M., Monjardet B.: Ordre et classiffication, algebre et combinatoire. Hachette, Paris, 1970. Wille R.: Restructuring lattice theory: an approach based on hierarchies of concepts. In: I. Rival (Ed.): Ordered Sets. Reidel, Dordrecht, 1982, pp. 445-470. Ganter B., Wille R.: Formal Concept Analysis. Springer, 1999. Application of FCA: knowledge extraction clustering and classification machine learning concepts, ontologies rules, association rules, attribute implications 9.1 Background in FCA FCA provides methods to describe the relationship between a set of objects \\(G\\) and a set of attributes \\(M\\). We show the main methods of FCA using the main functionalities and data structures of the fcaR package. We load the fcaR package by: Formal Context, \\(\\mathbf{ K} := (G, M, I)\\) objects &lt;- c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;, &quot;Pluto&quot;) attributes &lt;- c(&quot;small&quot;, &quot;medium&quot;, &quot;large&quot;, &quot;near&quot;, &quot;far&quot;, &quot;moon&quot;, &quot;no_moon&quot;) planets &lt;- matrix(0, nrow = length(objects), ncol = length(attributes)) rownames(planets) &lt;- objects colnames(planets) &lt;- attributes planets[&quot;Mercury&quot;, c(&quot;small&quot;, &quot;near&quot;, &quot;no_moon&quot;)] &lt;- 1 planets[&quot;Venus&quot;, c(&quot;small&quot;, &quot;near&quot;, &quot;no_moon&quot;)] &lt;- 1 planets[&quot;Earth&quot;, c(&quot;small&quot;, &quot;near&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Mars&quot;, c(&quot;small&quot;, &quot;near&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Jupiter&quot;, c(&quot;large&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Saturn&quot;, c(&quot;large&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Uranus&quot;, c(&quot;medium&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Neptune&quot;, c(&quot;medium&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 planets[&quot;Pluto&quot;, c(&quot;small&quot;, &quot;far&quot;, &quot;moon&quot;)] &lt;- 1 fc_planets &lt;- FormalContext$new(planets) knitr::kable(planets, format = &quot;html&quot;, booktabs = TRUE) small medium large near far moon no_moon Mercury 1 0 0 1 0 0 1 Venus 1 0 0 1 0 0 1 Earth 1 0 0 1 0 1 0 Mars 1 0 0 1 0 1 0 Jupiter 0 0 1 0 1 1 0 Saturn 0 0 1 0 1 1 0 Uranus 0 1 0 0 1 1 0 Neptune 0 1 0 0 1 1 0 Pluto 1 0 0 0 1 1 0 Two mappings can be defined: intent: \\((\\ )&#39;\\colon 2^G \\to 2^M\\) with, for all \\(A\\subseteq G\\), \\(A&#39; = \\{m \\in M \\mid g\\, I\\, m \\mbox{ for all } g \\in A\\}\\) for all \\(A\\subseteq G\\). extent: \\((\\ )&#39;\\colon 2^M \\to 2^G\\) with, for all \\(B\\subseteq M\\), \\(B&#39; = \\{g \\in G \\mid g\\, I\\, m \\mbox{ for all } m \\in B\\}\\). That is, the intent of a set of objects is the set of their common attributes: # Define a set of objects S &lt;- Set$new(attributes = fc_planets$objects) S$assign(Earth = 1, Mars = 1) cat(&quot;Given the set of objects:&quot;) Given the set of objects: S {Earth, Mars} cat(&quot;The intent is:&quot;) The intent is: # Compute the intent of S fc_planets$intent(S) {small, near, moon} Analogously, the extent of a set of attributes is the set of objects which possess all the attributes in the given set: # Define a set of objects S &lt;- Set$new(attributes = fc_planets$attributes) S$assign(moon = 1, large = 1) cat(&quot;Given the set of attributes:&quot;) Given the set of attributes: S {large, moon} cat(&quot;The extent is:&quot;) The extent is: # Compute the extent of S fc_planets$extent(S) {Jupiter, Saturn} This pair of mappings is a Galois connection. The composition of intent and extent is the closure of a set of attributes: # Compute the closure of S print(&quot;El conjunto de objetos &quot;) [1] “El conjunto de objetos” S {large, moon} print(&quot;tiene como cerrado&quot;) [1] “tiene como cerrado” Sc &lt;- fc_planets$closure(S) Sc {large, far, moon} This means that all planets which have the attributes moon and large also have far in common. Definition: A formal concept is a pair \\((A,B)\\) such that \\(A \\subseteq G\\), \\(B \\subseteq M\\), \\(A&#39; = B\\) and \\(B&#39; = A\\). Consequently, \\(A\\) and \\(B\\) are closed sets of objects and attributes, respectively. # Define a set of objects S &lt;- Set$new(attributes = fc_planets$attributes) S$assign(moon = 1, large = 1, far= 1) print(&quot;Given the set of attributes:&quot;) [1] “Given the set of attributes:” S {large, far, moon} print(&quot;The extent is:&quot;) [1] “The extent is:” # Compute the extent of S extent &lt;- fc_planets$extent(S) extent {Jupiter, Saturn} print(&quot;And the intent of this one is:&quot;) [1] “And the intent of this one is:” fc_planets$intent(extent) {large, far, moon} \\(\\big(\\{Jupiter, Saturn\\},\\{large, far, moon\\}\\big)\\) is a concept. It is a maximal cluster. 9.1.1 Datasets We are going to work with two datasets, a crisp one and a fuzzy one. The classical (binary) dataset is the well-known planets formal context, presented in Wille R (1982). “Restructuring Lattice Theory: An Approach Based on Hierarchies of Concepts.” In Ordered Sets, pp. 445–470. Springer. small medium large near far moon no_moon Mercury 1 0 0 1 0 0 1 Venus 1 0 0 1 0 0 1 Earth 1 0 0 1 0 1 0 Mars 1 0 0 1 0 1 0 Jupiter 0 0 1 0 1 1 0 Saturn 0 0 1 0 1 1 0 Uranus 0 1 0 0 1 1 0 Neptune 0 1 0 0 1 1 0 Pluto 1 0 0 0 1 1 0 The other formal context is fuzzy and is defined by the following matrix I: P1 P2 P3 P4 P5 P6 O1 0.0 0.0 0.5 0.5 1.0 0 O2 1.0 1.0 1.0 0.0 0.0 0 O3 0.5 0.5 0.0 0.0 0.0 1 O4 0.0 0.0 0.0 1.0 0.5 0 O5 0.0 0.0 1.0 0.5 0.0 0 O6 0.5 0.5 0.0 0.0 0.0 1 9.2 Working with Formal Contexts - datasets The first step when using the fcaR package to analyse a formal context is to create an object of class FormalContext which will store all the information related to the context. In our examples, we create two objects: Internally, the object stores information about whether the context is binary or the names of objects and attributes, which are taken from the rownames and colnames of the provided matrix. 9.2.1 Plotting, printing and latex-ing the FormalContext Once created the FormalContext objects, we can print them or plot them as heatmaps (with functions print() and plot()): FormalContext with 9 objects and 7 attributes. small medium large near far moon no_moon Mercury X X X Venus X X X Earth X X X Mars X X X Jupiter X X X Saturn X X X Uranus X X X Neptune X X X Pluto X X X FormalContext with 6 objects and 6 attributes. P1 P2 P3 P4 P5 P6 O1 0 0 0.5 0.5 1 0 O2 1 1 1 0 0 0 O3 0.5 0.5 0 0 0 1 O4 0 0 0 1 0.5 0 O5 0 0 1 0.5 0 0 O6 0.5 0.5 0 0 0 1 Also, we can export the formal context as a LaTeX table: 9.2.2 Closures The basic operation in FCA is the computation of closures given an attribute set, by using the two derivation operators, extent and intent. The intent of a (probably fuzzy) set of objects is the set of their common attributes: {Earth, Mars} {small, near, moon} Analogously, the extent of a set of attributes is the set of objects which possess all the attributes in the given set: {large, moon} {Jupiter, Saturn} The composition of intent and extent is the closure of a set of attributes: {large, far, moon} This means that all planets which have the attributes moon and large also have far in common. We can check whether a set is closed (that is, it is equal to its closure), using is_closed(): [1] FALSE [1] TRUE 9.2.3 Clarification and Reduction An interesting point when managing formal contexts is the ability to reduce the context, removing redundancies, while retaining all the knowledge. This is accomplished by two functions: clarify(), which removes duplicated attributes and objects (columns and rows in the original matrix); and reduce(), which uses closures to remove dependent attributes, but only on binary formal contexts. The resulting FormalContext is equivalent to the original one in both cases. FormalContext with 5 objects and 7 attributes. small medium large near far moon no_moon Pluto X X X [Mercury, Venus] X X X [Earth, Mars] X X X [Jupiter, Saturn] X X X [Uranus, Neptune] X X X FormalContext with 5 objects and 5 attributes. P3 P4 P5 P6 [P1, P2] O1 0.5 0.5 1 0 0 O2 1 0 0 0 1 O4 0 1 0.5 0 0 O5 1 0.5 0 0 0 [O3, O6] 0 0 0 1 0.5 Note that merged attributes or objects are stored in the new formal context by using squared brackets to unify them, e.g. [Mercury, Venus]. 9.2.4 Extracting Implications and Concepts The function to extract the canonical basis of implications and the concept lattice is find_implications(). Its use is to store a ConceptLattice and an ImplicationSet objects internally in the FormalContext object after running the NextClosure algorithm. It can be used both for binary and fuzzy formal contexts, resulting in binary or fuzzy concepts and implications: We can inspect the results as: A set of 12 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter}, {}) 2: ({Venus, Earth, Mars, Jupiter}, {moon}) 3: ({Earth, Mars, Jupiter}, {far, moon}) 4: ({Earth}, {large, far, moon}) 5: ({Mars}, {medium, far, moon}) 6: ({Mercury, Venus, Jupiter}, {small}) 7: ({Venus, Jupiter}, {small, moon}) 8: ({Jupiter}, {small, far, moon}) 9: ({Mercury, Venus}, {small, near}) 10: ({Mercury}, {small, near, no_moon}) 11: ({Venus}, {small, near, moon}) 12: ({}, {small, medium, large, near, far, moon, no_moon}) Implication set with 10 implications. Rule 1: {no_moon} -&gt; {small, near} Rule 2: {far} -&gt; {moon} Rule 3: {near} -&gt; {small} Rule 4: {large} -&gt; {far, moon} Rule 5: {medium} -&gt; {far, moon} Rule 6: {medium, large, far, moon} -&gt; {small, near, no_moon} Rule 7: {small, near, moon, no_moon} -&gt; {medium, large, far} Rule 8: {small, near, far, moon} -&gt; {medium, large, no_moon} Rule 9: {small, large, far, moon} -&gt; {medium, near, no_moon} Rule 10: {small, medium, far, moon} -&gt; {large, near, no_moon} 9.2.5 Saving and loading A FormalContext is saved and loaded (in RDS format) using its own methods save() and load(), which are more efficient than the base saveRDS() and readRDS(). 9.3 Concept Lattice We are going to use the previously computed concept lattices for the two FormalContext objects. 9.3.1 Plot, print and LaTeX The concept lattice can be plotted using a Hasse diagram and the function plot() inside the ConceptLattice component: If one desires to get the list of concepts printed, or in \\(\\LaTeX\\) format, just: A set of 12 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter}, {}) 2: ({Venus, Earth, Mars, Jupiter}, {moon}) 3: ({Earth, Mars, Jupiter}, {far, moon}) 4: ({Earth}, {large, far, moon}) 5: ({Mars}, {medium, far, moon}) 6: ({Mercury, Venus, Jupiter}, {small}) 7: ({Venus, Jupiter}, {small, moon}) 8: ({Jupiter}, {small, far, moon}) 9: ({Mercury, Venus}, {small, near}) 10: ({Mercury}, {small, near, no_moon}) 11: ({Venus}, {small, near, moon}) 12: ({}, {small, medium, large, near, far, moon, no_moon}) 9.3.2 Getting all extents, intents and retrieving concepts For a ConceptLattice, one may want to retrieve particular concepts, using a subsetting as in R: A set of 2 concepts: 1: ({Venus, Earth, Mars, Jupiter}, {moon}) 2: ({Earth, Mars, Jupiter}, {far, moon}) Or get all the extents and all the intents of all concepts, as sparse matrices: 5 x 12 sparse Matrix of class “dgCMatrix” [1,] 1 . . . . 1 . . 1 1 . . [2,] 1 1 . . . 1 1 . 1 . 1 . [3,] 1 1 1 1 . . . . . . . . [4,] 1 1 1 . 1 . . . . . . . [5,] 1 1 1 . . 1 1 1 . . . . 7 x 12 sparse Matrix of class “dgCMatrix” [1,] . . . . . 1 1 1 1 1 1 1 [2,] . . . . 1 . . . . . . 1 [3,] . . . 1 . . . . . . . 1 [4,] . . . . . . . . 1 1 1 1 [5,] . . 1 1 1 . . 1 . . . 1 [6,] . 1 1 1 1 . 1 1 . . 1 1 [7,] . . . . . . . . . 1 . 1 9.3.3 Concept support First, the support of an itemset is: \\[ supp(X)=\\frac{X^\\prime}{G} \\] The support of a concept $\\langle A, B\\rangle$ (A is the extent of the concept and B is the intent) is the cardinality (relative) of the extent - number of objects of the extent. The support of concepts can be computed using the function support(): [1] 1.0000000 0.7777778 0.5555556 0.2222222 0.2222222 0.5555556 0.3333333 [8] 0.1111111 0.4444444 0.2222222 0.2222222 0.0000000 The support of itemsets and concepts is used to mine all the knowledge: Algorithm Titanic (computing iceberg concept lattices) 9.3.4 Sublattices When the concept lattice is too large, it can be useful in certain occasions to just work with a sublattice of the complete lattice. To this end, we use the sublattice() function. For instance, to build the sublattice of those concepts with support greater than 0.5, we can do: A set of 13 concepts: 1: ({O1, O2, O3, O4, O5}, {}) 2: ({O1, O4, O5}, {P4 [0.5]}) 3: ({O1, O4}, {P4 [0.5], P5 [0.5]}) 4: ({O1, O2, O5}, {P3 [0.5]}) 5: ({O1, O5}, {P3 [0.5], P4 [0.5]}) 6: ({O1}, {P3 [0.5], P4 [0.5], P5}) 7: ({O1 [0.5], O2, O5}, {P3}) 8: ({O1 [0.5], O5}, {P3, P4 [0.5]}) 9: ({O1 [0.5]}, {P3, P4, P5}) 10: ({O2, O3}, {P1 [0.5], P2 [0.5]}) 11: ({O3}, {P1 [0.5], P2 [0.5], P6}) 12: ({O2}, {P1, P2, P3}) 13: ({}, {P1, P2, P3, P4, P5, P6}) And we can plot just the sublattice: 9.3.5 Subconcepts, superconcepts, infimum and supremum It may be interesting to use the notions of subconcept and superconcept. Given a concept, we can compute all its subconcepts and all its superconcepts: A set of 1 concepts: 1: ({Mars}, {medium, far, moon}) A set of 2 concepts: 1: ({Mars}, {medium, far, moon}) 2: ({}, {small, medium, large, near, far, moon, no_moon}) A set of 4 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter}, {}) 2: ({Venus, Earth, Mars, Jupiter}, {moon}) 3: ({Earth, Mars, Jupiter}, {far, moon}) 4: ({Mars}, {medium, far, moon}) Also, we can define infimum and supremum of a set of concepts as the greatest common subconcept of all the given concepts, and the lowest common superconcept of them, and can be computed by: A set of 3 concepts: 1: ({Mars}, {medium, far, moon}) 2: ({Mercury, Venus, Jupiter}, {small}) 3: ({Venus, Jupiter}, {small, moon}) ({Mercury, Venus, Earth, Mars, Jupiter}, {}) ({}, {small, medium, large, near, far, moon, no_moon}) 9.3.6 Join- and meet- irreducible elements Theorem: In a complete lattice, an element is called supremum-irreducible or join-irreducible if it cannot be written as the supremum of other elements and infimum-irreducible or meet-irreducible if it can not be expressed as the infimum of other elements. The irreducible elements with respect to join (supremum) and meet (infimum) can be computed for a given concept lattice: A set of 5 concepts: 1: ({Earth}, {large, far, moon}) 2: ({Mars}, {medium, far, moon}) 3: ({Jupiter}, {small, far, moon}) 4: ({Mercury}, {small, near, no_moon}) 5: ({Venus}, {small, near, moon}) A set of 7 concepts: 1: ({Venus, Earth, Mars, Jupiter}, {moon}) 2: ({Earth, Mars, Jupiter}, {far, moon}) 3: ({Earth}, {large, far, moon}) 4: ({Mars}, {medium, far, moon}) 5: ({Mercury, Venus, Jupiter}, {small}) 6: ({Mercury, Venus}, {small, near}) 7: ({Mercury}, {small, near, no_moon}) This are the concepts used to build the standard context, mentioned above. 9.4 Exercises Compute the intent of Earth and Earth,Mars, Mercury (use the argument attributes in the class Set). {Mercury, Earth, Mars} Given the set of objects:{Earth} The intent is:{small, near, moon} {small, near} Compute the extent of large and far,large (use the argument attributes in the class Set) and save the result in a variable e1, e2. Given the set of objects:{large} {large, far} The extent is:{Jupiter, Saturn} {Jupiter, Saturn} Compute the intent of variables e1 and also of e2. {large, far, moon} {large, far, moon} With the information from the above questions tell me a concept. Check with any command of fcaR package. Compute the closure of no_moon {small, near, no_moon} ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) Compute all the concepts and plot them. How many are there? Show the fist and the last (use subsetting). A set of 2 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({}, {small, medium, large, near, far, moon, no_moon}) Compute the major concept (in lattice) that has moon. The same with no_moon. Locate both in the lattice to understand the meaning. ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) ({Mercury, Venus}, {small, near, no_moon}) Compute the minor concept (in lattice) that has Pluto The same with Earth. Locate both in the lattice to understand the meaning. ({Pluto}, {small, far, moon}) ({Earth, Mars}, {small, near, moon}) Compute the meet irreducible elements in the lattice. A set of 7 concepts: 1: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 2: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 3: ({Jupiter, Saturn}, {large, far, moon}) 4: ({Uranus, Neptune}, {medium, far, moon}) 5: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 6: ({Mercury, Venus, Earth, Mars}, {small, near}) 7: ({Mercury, Venus}, {small, near, no_moon}) Compute the sublattice of the concept in the irreducible elements A set of 12 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {moon}) 3: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 4: ({Jupiter, Saturn}, {large, far, moon}) 5: ({Uranus, Neptune}, {medium, far, moon}) 6: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 7: ({Earth, Mars, Pluto}, {small, moon}) 8: ({Pluto}, {small, far, moon}) 9: ({Mercury, Venus, Earth, Mars}, {small, near}) 10: ({Mercury, Venus}, {small, near, no_moon}) 11: ({Earth, Mars}, {small, near, moon}) 12: ({}, {small, medium, large, near, far, moon, no_moon}) Compute the sublattice of the concept in the irreducible elements removing the first element in the list of irreducible elements. Plot this sublattice. A set of 9 concepts: 1: ({Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune, Pluto}, {}) 2: ({Jupiter, Saturn, Uranus, Neptune, Pluto}, {far, moon}) 3: ({Jupiter, Saturn}, {large, far, moon}) 4: ({Uranus, Neptune}, {medium, far, moon}) 5: ({Mercury, Venus, Earth, Mars, Pluto}, {small}) 6: ({Pluto}, {small, far, moon}) 7: ({Mercury, Venus, Earth, Mars}, {small, near}) 8: ({Mercury, Venus}, {small, near, no_moon}) 9: ({}, {small, medium, large, near, far, moon, no_moon}) Develop a function returning the index and also the labels in all the concepts (inside the formal context) having a vector with attributes. HOMEWORK. 9.5 Implications in FCA This is a summary of some of the functionalities introduced in package fcaR: Computing implications and concepts using Ganter’s algorithm. Visualization of the concept lattice. Removal of redundancies in implications. Computation of closures. 9.5.0.1 Data The datasets in this vignette come from this paper. 9.5.1 Crisp Version The crisp version of the data appears in Table 3 in the mentioned paper. 9.5.2 Computing Implications and Concepts Once we create the formal context object, with the previous data matrix I, we can compute all concepts and implications using Ganter’s algorithm: [1] 7 LHS RHS 2.142857 1.857143 The obtained implications are: Implication set with 7 implications. Rule 1: {P6} -&gt; {P1, P2} Rule 2: {P5} -&gt; {P4} Rule 3: {P3, P4, P5} -&gt; {P2} Rule 4: {P2, P4} -&gt; {P3, P5} Rule 5: {P1, P4} -&gt; {P2, P3, P5, P6} Rule 6: {P1, P3} -&gt; {P2} Rule 7: {P1, P2, P3, P6} -&gt; {P4, P5} 9.5.2.1 Visualization We provide functions to plot both the concept lattice and the formal context: 9.5.3 Redudancy Removal Let us apply some simplifcation rules: [1] 7 LHS RHS 1.714286 1.857143 The transformed ruleset: Implication set with 7 implications. Rule 1: {P6} -&gt; {P1, P2} Rule 2: {P5} -&gt; {P4} Rule 3: {P3, P5} -&gt; {P2} Rule 4: {P2, P4} -&gt; {P3, P5} Rule 5: {P1, P4} -&gt; {P2, P3, P5, P6} Rule 6: {P1, P3} -&gt; {P2} Rule 7: {P3, P6} -&gt; {P4, P5} 9.5.4 Fuzzy version The fuzzy version of the data appears in Table 2 in the mentioned paper. 9.5.4.1 Computing Implications and Concepts As before, we build the formal context object and compute all implications: [1] 12 LHS RHS 1.541667 1.916667 The extracted ruleset is: Implication set with 12 implications. Rule 1: {P6 [0.5]} -&gt; {P1 [0.5], P2 [0.5], P6} Rule 2: {P5 [0.5]} -&gt; {P4 [0.5]} Rule 3: {P3 [0.5], P4 [0.5], P5 [0.5]} -&gt; {P2, P5} Rule 4: {P3 [0.5], P4} -&gt; {P3} Rule 5: {P2 [0.5], P4 [0.5]} -&gt; {P2, P3 [0.5], P5} Rule 6: {P2 [0.5], P3 [0.5]} -&gt; {P2} Rule 7: {P2, P3, P4 [0.5], P5} -&gt; {P4} Rule 8: {P1 [0.5], P4 [0.5]} -&gt; {P1, P2, P3, P4, P5, P6} Rule 9: {P1 [0.5], P3 [0.5]} -&gt; {P1, P2, P3} Rule 10: {P1 [0.5], P2} -&gt; {P1} Rule 11: {P1, P2 [0.5]} -&gt; {P2} Rule 12: {P1, P2, P3, P6} -&gt; {P4, P5} 9.5.4.2 Visualization Let us plot the concept lattice and the formal context. 9.5.4.3 Redudancy Removal Let us apply some functions to remove redudancies in the set of implications: [1] 12 LHS RHS 1.541667 1.916667 [1] 12 LHS RHS 1.458333 1.916667 [1] 12 LHS RHS 1.458333 1.916667 The reduced ruleset is: Implication set with 12 implications. Rule 1: {P6 [0.5]} -&gt; {P1 [0.5], P2 [0.5], P6} Rule 2: {P5 [0.5]} -&gt; {P4 [0.5]} Rule 3: {P3 [0.5], P5 [0.5]} -&gt; {P2, P5} Rule 4: {P3 [0.5], P4} -&gt; {P3} Rule 5: {P2 [0.5], P4 [0.5]} -&gt; {P2, P3 [0.5], P5} Rule 6: {P2 [0.5], P3 [0.5]} -&gt; {P2} Rule 7: {P2, P3, P5} -&gt; {P4} Rule 8: {P1 [0.5], P4 [0.5]} -&gt; {P1, P2, P3, P4, P5, P6} Rule 9: {P1 [0.5], P3 [0.5]} -&gt; {P1, P2, P3} Rule 10: {P1 [0.5], P2} -&gt; {P1} Rule 11: {P1, P2 [0.5]} -&gt; {P2} Rule 12: {P1, P2, P3, P6} -&gt; {P4, P5} 9.5.4.4 Closures Let us show how to compute the closure of a set S with respect to the implication set. {P2 [0.5], P3 [0.5]} $closure {P2, P3 [0.5]} 9.6 Exercise From an implication extracted from a formal cotext return the string representig the implication: &gt; cadena &lt;- impl.as.character(Implication) &gt; cadena &gt; &quot;a,b -&gt; c,d&quot; Use the function: impl.as.character &lt;- function(Implication ){ xxxx return{cadena} } From a string add the implication represented in the string to an implicational set: cadena &lt;- &quot;a,b -&gt; c,d&quot; implicationsNew &lt;- add_implication(cadena,Implications) Use the function: add_implication &lt;- function(stringImplication,ImplicationSet){ xxxx return{ImplicationSetNew} } 9.7 Simplification Logic for Mushroom Dataset” This is a simple example of some of the functionalities introduced in package fcaR: Import from/export to arules format. Removal of redundancies in implications. Computation of closures. 9.7.1 Data In this example, we’ll use the well-known Mushroom dataset, from the arules package. We’ll use the a priori algorithm to extract a large number of implications from the dataset. Apriori Parameter specification: confidence minval smax arem aval originalSupport maxtime support minlen 1 0.1 1 none FALSE TRUE 5 0.1 1 maxlen target ext 10 rules TRUE Algorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUE Absolute minimum support count: 812 set item appearances …[0 item(s)] done [0.00s]. set transactions …[114 item(s), 8124 transaction(s)] done [0.01s]. sorting and recoding items … [53 item(s)] done [0.00s]. creating transaction tree … done [0.00s]. checking subsets of size 1 2 3 4 5 6 7 8 9 10 done [1.41s]. writing … [1799427 rule(s)] done [0.11s]. creating S4 object … done [0.58s]. The number of implications extracted by the algorithm (with this configuration) is length(mush) = 1799427. 9.7.2 Preprocessing Next step is to remove the redundant rules in the implications set. To this end, let us use the is.redundant() function in arules. user system elapsed 4.223 0.176 4.413 After this, the number of implications is length(mush_clean) = 2002, that is, just a 0.111 percent of the original ruleset. 9.7.3 Importing in fcaR To use the functionalities in this package, one must import all objects (the formal context and implications above) into our data model. This is accomplished by just typing: We can check some of the properties of the ruleset: [1] 2002 LHS RHS 3.038462 1.000000 9.7.4 Applying Rules We can improve the redudancy removal performed by arules, using some simplification rules. Currently, the following rules are implemented: Composition. Generalization. Simplification. R-Simplification We can apply them one by one, just to see their effect, or we could use them sequentially. [1] 961 LHS RHS 3.122789 2.083247 [1] 961 LHS RHS 3.122789 1.072841 Also, we can compute the support of each implication: [1] 1.0000000 0.1024126 0.1280158 0.1290005 0.1319547 0.1378631 9.7.5 Computing Closure Given a fuzzy set represented by a sparse matrix, we can compute its closure with respect to the implications in the ruleset. $closure {CapColor=white, GillAttached=free, ColorAboveRing=white, ColorBelowRing=white, VeilType=partial, VeilColor=white} 9.7.6 Re-Exporting to arules After this phase of redudancy removal, we can export the obtained ruleset to arules format. set of 961 rules [1] “rules” attr(,“package”) [1] “arules” "],["social-network-analysis.html", "Capítulo 10 Social Network Analysis 10.1 R for SNA 10.2 igraph 10.3 Exportar grafos 10.4 De Twitter 10.5 Medidas de bondad, calidad 10.6 Proyecto SNA - USairports 10.7 Proyecto - películas + actores", " Capítulo 10 Social Network Analysis Disciplina con base sólida de Matemática Aplicada: Teoría de Grafos y Matemática Discreta. Unida con Álgebra Lineal: las bases de Pagerank (algoritmo de Google) The Mathematics of Google Search. Describir las relaciones entre los elementos de una red y extraer conocimiento acerca de las estructuras sociales que existen en esa red. Tópico de enorme interés para extraer conocimiento de redes sociales en cualquier área. en una red los actores no intervienen aislados decribir todos los actores intervinientes en las redes redes de alta complejidad Existen muy destacadas aplicaciones para SNA: Gephi https://gephi.org https://gephi.org/tutorials/gephi-tutorial-quick_start.pdf Exploratory Data Analysis: intuition-oriented analysis by networks manipulations in real time. Link Analysis: revealing the underlying structures of associations between objects. Social Network Analysis: easy creation of social data connectors to map community organizations and small-world networks. Biological Network analysis: representing patterns of biological data. Cytoscape https://cytoscape.org Cytoscape is an open source software platform for visualizing molecular interaction networks and biological pathways and integrating these networks with annotations, gene expression profiles and other state data. Library for visualization 10.1 R for SNA Usaremos el paquete igraph. Nos servirá para analizar más adelante datos extraído de Twiter. Ventajas de usar R: Reproducible research no es posible con las aplicaciones GUI. Herramientas sólidas para manipular los datos. Cada vez más paquetes diseñados para hacer de R una herramienta completa de análisis de redes. Paquetes statnet y igraph. Thomas Lin Pedersen ha publicado los paquetes tidygraph y ggraph, que aprovechan la potencia de igraph de forma coherente con el flujo de trabajo de tidyverse. Crear gráficos de red interactivos con el marco htmlwidgets que traduce el código de R a JavaScript. 10.1.1 Elementos de una red nodos o vértices de grafo (nodes, vertices) [1] “Tom Hanks” “Gary Sinise” “Bill Paxton” “Kevin Bacon” “Ed Harris” [6] “Sean Connery” “Robin Wright” “Nicolas Cage” arcos o enlaces (edges, links) [,1] [,2] [1,] “Tom Hanks” “Gary Sinise” [2,] “Tom Hanks” “Robin Wright” [3,] “Gary Sinise” “Robin Wright” [4,] “Tom Hanks” “Gary Sinise” [5,] “Tom Hanks” “Bill Paxton” [6,] “Tom Hanks” “Kevin Bacon” [7,] “Tom Hanks” “Ed Harris” [8,] “Gary Sinise” “Bill Paxton” [9,] “Gary Sinise” “Kevin Bacon” [10,] “Gary Sinise” “Ed Harris” [11,] “Bill Paxton” “Kevin Bacon” [12,] “Bill Paxton” “Ed Harris” [13,] “Kevin Bacon” “Ed Harris” [14,] “Ed Harris” “Sean Connery” [15,] “Ed Harris” “Nicolas Cage” [16,] “Sean Connery” “Nicolas Cage” Nodos y arcos pueden contender atributos adicionales con importante información: [1] “Forest Gump” “Forest Gump” “Forest Gump” “Apollo 13” “Apollo 13” [6] “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” [11] “Apollo 13” “Apollo 13” “Apollo 13” “The Rock” “The Rock” [16] “The Rock” 10.1.2 Representación de redes 10.1.2.1 Grafos como listas de arcos data.frame o matriz (si los datos del mismo tipo) que contiene dos columnas: primera columna: nodos que son el origen de una conexión segunda columna: nodos que son el destino de la conexión Si el sentido es importante, la red se denomina dirigida, en otro caso, no dirigida. alumnos1 &lt;- c(&quot;Luis&quot;, &quot;Ana&quot;, &quot;Fran&quot;, &quot;Pedro&quot;, &quot;Laura&quot;, &quot;Susana&quot;) alumnos2 &lt;- c(&quot;Juan&quot;, &quot;Jose&quot;, &quot;Amalia&quot;, &quot;Lucía&quot;, &quot;Maite&quot;, &quot;Eduardo&quot;) grupos &lt;- data.frame(integrante1 = alumnos1, integrante2 = alumnos2, stringsAsFactors = F) print(grupos) integrante1 integrante2 1 Luis Juan 2 Ana Jose 3 Fran Amalia 4 Pedro Lucía 5 Laura Maite 6 Susana Eduardo str(grupos) ‘data.frame’: 6 obs. of 2 variables: $ integrante1: chr “Luis” “Ana” “Fran” “Pedro” … $ integrante2: chr “Juan” “Jose” “Amalia” “Lucía” … 10.1.2.2 Grafos como matrices # Se pueden usar matrices &#39;sparse&#39; A &lt;- rbind(c(0,1,0), c(1,0,1), c(1,0,0)) nodeNames &lt;- c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;) dimnames(A) &lt;- list(nodeNames, nodeNames) A A B C A 0 1 0 B 1 0 1 C 1 0 0 str(A) num [1:3, 1:3] 0 1 1 1 0 0 0 1 0 - attr(*, “dimnames”)=List of 2 ..$ : chr [1:3] “A” “B” “C” ..$ : chr [1:3] “A” “B” “C” Caminos de longitud dos, tres, etc: # Multiplicación matricial A2 &lt;- A %*% A A2 A B C A 1 0 1 B 1 1 0 C 0 1 0 A3 &lt;- A %*% A %*% A A3 A B C A 1 1 0 B 1 1 1 C 1 0 1 Representado arcos: Arcos &lt;- rbind(c(&quot;A&quot;,&quot;B&quot;), c(&quot;B&quot;,&quot;A&quot;), c(&quot;B&quot;,&quot;C&quot;), c(&quot;C&quot;,&quot;A&quot;)) Arcos [,1] [,2] [1,] “A” “B” [2,] “B” “A” [3,] “B” “C” [4,] “C” “A” 10.2 igraph # Instalar la primera vez - descomentar #install.packages(&quot;igraph&quot;) #install.packages(&quot;igraphdata&quot;) library(igraph) library(igraphdata) # Importar la red de datasets ya establecidos: igraphdata # Limpia la memoria - Cuidado - borra todas las variables rm(list=ls()) #Lista de datasets de redes de nodos en igraph # data(package=&quot;igraphdata&quot;) # El paquete tiene un conjunto de datasets # Carga data set y vemos que contiene #Red social entre miembros de club de karate de universidad data(karate,package=&quot;igraphdata&quot;) plot(karate) 10.2.1 Acceder a elementos de grado IGRAPH 6f42903 D-W- 81 817 – + attr: Type (g/c), Date (g/c), Citation (g/c), Author (g/c), Group | (v/n), weight (e/n) + edges from 6f42903: [1] 57-&gt;52 76-&gt;42 12-&gt;69 43-&gt;34 28-&gt;47 58-&gt;51 7-&gt;29 40-&gt;71 5-&gt;37 48-&gt;55 [11] 6-&gt;58 21-&gt; 8 28-&gt;69 43-&gt;21 67-&gt;58 65-&gt;42 5-&gt;67 52-&gt;75 37-&gt;64 4-&gt;36 [21] 12-&gt;49 19-&gt;46 37-&gt; 9 74-&gt;36 62-&gt; 1 15-&gt; 2 72-&gt;49 46-&gt;62 2-&gt;29 40-&gt;12 [31] 22-&gt;29 71-&gt;69 4-&gt; 3 37-&gt;69 5-&gt; 6 77-&gt;13 23-&gt;49 52-&gt;35 20-&gt;14 62-&gt;70 [41] 34-&gt;35 76-&gt;72 7-&gt;42 37-&gt;42 51-&gt;80 38-&gt;45 62-&gt;64 36-&gt;53 62-&gt;77 17-&gt;61 [51] 7-&gt;68 46-&gt;29 44-&gt;53 18-&gt;58 12-&gt;16 72-&gt;42 52-&gt;32 58-&gt;21 38-&gt;17 15-&gt;51 [61] 22-&gt; 7 22-&gt;69 5-&gt;13 29-&gt; 2 77-&gt;12 37-&gt;35 18-&gt;46 10-&gt;71 22-&gt;47 20-&gt;19 + … omitted several edges + 81/81 vertices, from 6f42903: [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 [76] 76 77 78 79 80 81 + 817/817 edges from 6f42903: [1] 57-&gt;52 76-&gt;42 12-&gt;69 43-&gt;34 28-&gt;47 58-&gt;51 7-&gt;29 40-&gt;71 5-&gt;37 48-&gt;55 [11] 6-&gt;58 21-&gt; 8 28-&gt;69 43-&gt;21 67-&gt;58 65-&gt;42 5-&gt;67 52-&gt;75 37-&gt;64 4-&gt;36 [21] 12-&gt;49 19-&gt;46 37-&gt; 9 74-&gt;36 62-&gt; 1 15-&gt; 2 72-&gt;49 46-&gt;62 2-&gt;29 40-&gt;12 [31] 22-&gt;29 71-&gt;69 4-&gt; 3 37-&gt;69 5-&gt; 6 77-&gt;13 23-&gt;49 52-&gt;35 20-&gt;14 62-&gt;70 [41] 34-&gt;35 76-&gt;72 7-&gt;42 37-&gt;42 51-&gt;80 38-&gt;45 62-&gt;64 36-&gt;53 62-&gt;77 17-&gt;61 [51] 7-&gt;68 46-&gt;29 44-&gt;53 18-&gt;58 12-&gt;16 72-&gt;42 52-&gt;32 58-&gt;21 38-&gt;17 15-&gt;51 [61] 22-&gt; 7 22-&gt;69 5-&gt;13 29-&gt; 2 77-&gt;12 37-&gt;35 18-&gt;46 10-&gt;71 22-&gt;47 20-&gt;19 [71] 19-&gt;31 68-&gt;13 49-&gt;69 30-&gt;63 5-&gt;49 53-&gt;75 62-&gt;57 73-&gt;81 29-&gt;69 71-&gt;40 [81] 19-&gt;58 49-&gt;42 37-&gt; 5 18-&gt; 2 20-&gt;80 75-&gt;53 15-&gt;54 76-&gt;58 40-&gt;23 5-&gt;12 [91] 20-&gt;54 6-&gt;47 51-&gt;14 78-&gt; 4 52-&gt;49 29-&gt;55 27-&gt;35 66-&gt; 6 21-&gt;29 4-&gt;61 + … omitted several edges + 81/81 vertices, from 6f42903: [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 [51] 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 [76] 76 77 78 79 80 81 Class ‘igraph’ hidden list of 10 $ : num 81 $ : logi TRUE $ : num [1:817] 56 75 11 42 27 57 6 39 4 47 … $ : num [1:817] 51 41 68 33 46 50 28 70 36 54 … $ : num [1:817] 580 411 719 376 569 215 533 620 527 592 … $ : num [1:817] 241 433 238 352 258 274 115 24 263 25 … $ : num [1:82] 0 6 23 27 37 65 74 91 93 101 … $ : num [1:82] 0 9 28 32 40 50 58 76 82 87 … $ :List of 4 ..$ : num [1:3] 1 0 1 ..$ :List of 4 .. ..$ Type : chr “TSPE” .. ..$ Date : chr “Mon Mar 19 21:56:02 2007” .. ..$ Citation: chr “Nepusz T., Petroczi A., Negyessy L., Bazso F.: Fuzzy communities and the concept of bridgeness in complex netwo”| truncated .. ..$ Author : chr “Nepusz T., Petroczi A., Negyessy L., Bazso F.” ..$ :List of 1 .. ..$ Group: num [1:81] 3 1 3 3 2 2 2 1 3 2 … ..$ :List of 1 .. ..$ weight: num [1:817] 4 14 4 4 10 2 6 2 4 4 … $ :&lt;environment: 0x10c99be08&gt; [1] 4 14 4 4 10 2 [1] 3 1 3 3 2 2 10.2.2 Construir/Modificar un grafo Añadir arcos a un grafo vacío: # Un grafo dirigido vacío g &lt;- make_empty_graph(n = 0, directed = TRUE) g IGRAPH c24d318 D— 0 0 – + edges from c24d318: g &lt;- g + vertices(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;)) g IGRAPH 24e76cf DN– 3 0 – + attr: name (v/c) + edges from 24e76cf (vertex names): # Arcos: A to C , B to C g &lt;- g + edges(c(&quot;A&quot;,&quot;C&quot;, &quot;B&quot;,&quot;C&quot;)) g IGRAPH 2ec374e DN– 3 2 – + attr: name (v/c) + edges from 2ec374e (vertex names): [1] A-&gt;C B-&gt;C # Eliminar arco A g &lt;- g - V(g)[&quot;A&quot;] g IGRAPH 0fea690 DN– 2 1 – + attr: name (v/c) + edge from 0fea690 (vertex names): [1] B-&gt;C # Eliminará todos los arcos conectados con A Lista de arcos: graph() and get.edgelist(): # Un grafo dirigido vacío # graph() id desde 1. g1 &lt;- graph( c(1,2, 1,3, 2,3, 3,4 ));g1 IGRAPH 61b9198 D— 4 4 – + edges from 61b9198: [1] 1-&gt;2 1-&gt;3 2-&gt;3 3-&gt;4 summary(g1) IGRAPH 61b9198 D— 4 4 – plot(g1) # El parámetro &quot;directed&quot; a FALSE para # grafos no dirigidos. g2 &lt;- graph( c(1,2, 1,3, 2,3, 3,4 , 3, 5, 1, 3), directed=FALSE); g2 IGRAPH 6889701 U— 5 6 – + edges from 6889701: [1] 1–2 1–3 2–3 3–4 3–5 1–3 summary(g2) IGRAPH 6889701 U— 5 6 – plot(g2) #Obtener la lista de arcos a partir de un grafo edgelist&lt;-get.edgelist(g2) ; edgelist [,1] [,2] [1,] 1 2 [2,] 1 3 [3,] 2 3 [4,] 3 4 [5,] 3 5 [6,] 1 3 edgelist &lt;- as_edgelist(g2) ; edgelist [,1] [,2] [1,] 1 2 [2,] 1 3 [3,] 2 3 [4,] 3 4 [5,] 3 5 [6,] 1 3 # Obtener el grafo a partir de la lista de arcos g3&lt;-graph( t(edgelist)); g3; plot(g3) IGRAPH c59ec1f D— 5 6 – + edges from c59ec1f: [1] 1-&gt;2 1-&gt;3 2-&gt;3 3-&gt;4 3-&gt;5 1-&gt;3 g3&lt;-graph( edgelist); g3; plot(g3) IGRAPH 7da7d2a D— 5 6 – + edges from 7da7d2a: [1] 1-&gt;1 2-&gt;3 3-&gt;1 2-&gt;3 3-&gt;4 5-&gt;3 # algunos parámetros de plot plot(g3, vertex.color=&quot;green&quot;, edge.arrow.size=0.5, vertex.size=25, edge.curved=0.5, layout_as_star=TRUE) Matrices de adyacencia: graph.adjacency(), get.adjacency() adjm_u&lt;-matrix( c(0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0), nrow=6, ncol=6, byrow = TRUE) #grafo a partir de matriz de adyacencia g_adj_u &lt;- graph.adjacency(adjm_u, mode=&quot;undirected&quot;) plot(g_adj_u) # Matriz de adyacencia a partir de grafo A &lt;- get.adjacency(g_adj_u); A 6 x 6 sparse Matrix of class “dgCMatrix” [1,] . 1 . . 1 . [2,] 1 . 1 . 1 . [3,] . 1 . 1 . . [4,] . . 1 . 1 1 [5,] 1 1 . 1 . . [6,] . . . 1 . . A &lt;- as_adjacency_matrix(g_adj_u, sparse = FALSE) A [,1] [,2] [,3] [,4] [,5] [,6] [1,] 0 1 0 0 1 0 [2,] 1 0 1 0 1 0 [3,] 0 1 0 1 0 0 [4,] 0 0 1 0 1 1 [5,] 1 1 0 1 0 0 [6,] 0 0 0 1 0 0 Grafo a partir de data frame # Primero, crear el data frame node1 = c(&quot;Ella&quot;, &quot;Tu&quot;, &quot;El&quot;); node2 = c(&quot;El&quot;, &quot;Ella&quot;, &quot;Tu&quot;) weight = c(10, -2, 3) df = data.frame(node1, node2, weight); df node1 node2 weight 1 Ella El 10 2 Tu Ella -2 3 El Tu 3 # Crear el grafo g &lt;- graph.data.frame(df, directed=FALSE); g IGRAPH bdbcd42 UNW- 3 3 – + attr: name (v/c), weight (e/n) + edges from bdbcd42 (vertex names): [1] Ella–El Ella–Tu Tu –El plot(g) # Si se conocen los vértices # g &lt;- graph.data.frame(df, vertices=listvertices, directed=FALSE);g # Obtener los nombres de los nodos V(g)$name [1] “Ella” “Tu” “El” # Obtener los pesos de los arcos E(g)$weight [1] 10 -2 3 Grafo a partir de literales #?graph_from_literal g &lt;- graph_from_literal(A--C, A-+D, C-+A, , D-+C) g IGRAPH 4be95cd DN– 4 3 – + attr: name (v/c) + edges from 4be95cd (vertex names): [1] A-&gt;D C-&gt;A D-&gt;C plot(g) #IGRAPH DN-- 4 4 -- #+ attr: name (v/c) #+ edges (vertex names): #[1] A-&gt;D D-&gt;C D-&gt;B B-&gt;A G3 &lt;-graph_from_literal(A-B, B -+C) plot(G3) G3 &lt;-graph_from_literal(A-B, B -C) plot(G3) grafo aleatorio 10.2.3 Visualización Buscar ayuda de los comandos plot.igraph, igraph.plotting. A continuación dibujamos algunos grafos interesantes: #library(igraph) # Trees g &lt;- make_tree(27, children=3) g; plot(g) IGRAPH 8b5b1d7 D— 27 26 – Tree + attr: name (g/c), children (g/n), mode (g/c) + edges from 8b5b1d7: [1] 1-&gt; 2 1-&gt; 3 1-&gt; 4 2-&gt; 5 2-&gt; 6 2-&gt; 7 3-&gt; 8 3-&gt; 9 3-&gt;10 4-&gt;11 4-&gt;12 4-&gt;13 [13] 5-&gt;14 5-&gt;15 5-&gt;16 6-&gt;17 6-&gt;18 6-&gt;19 7-&gt;20 7-&gt;21 7-&gt;22 8-&gt;23 8-&gt;24 8-&gt;25 [25] 9-&gt;26 9-&gt;27 # Cliques g &lt;- make_full_graph(n=6) g; plot(g) IGRAPH c3c5c9d U— 6 15 – Full graph + attr: name (g/c), loops (g/l) + edges from c3c5c9d: [1] 1–2 1–3 1–4 1–5 1–6 2–3 2–4 2–5 2–6 3–4 3–5 3–6 4–5 4–6 5–6 # Lattices g &lt;- make_lattice(dimvector = c(5,5), circular = FALSE) V(g)$label &lt;- NA g; plot(g) IGRAPH de17e93 U— 25 40 – Lattice graph + attr: name (g/c), dimvector (g/n), nei (g/n), mutual (g/l), circular | (g/l), label (v/l) + edges from de17e93: [1] 1– 2 1– 6 2– 3 2– 7 3– 4 3– 8 4– 5 4– 9 5–10 6– 7 [11] 6–11 7– 8 7–12 8– 9 8–13 9–10 9–14 10–15 11–12 11–16 [21] 12–13 12–17 13–14 13–18 14–15 14–19 15–20 16–17 16–21 17–18 [31] 17–22 18–19 18–23 19–20 19–24 20–25 21–22 22–23 23–24 24–25 #Stars g &lt;- make_star(n=10,mode = &quot;undirected&quot;) g; plot(g) IGRAPH d783e8e U— 10 9 – Star + attr: name (g/c), mode (g/c), center (g/n) + edges from d783e8e: [1] 1– 2 1– 3 1– 4 1– 5 1– 6 1– 7 1– 8 1– 9 1–10 Anillo con conexiones cruzadas: g &lt;- make_ring(10, directed=TRUE, mutual=TRUE) V(g)$name &lt;- LETTERS[1:10] g &lt;- g + edges(9,5, 7,1, 1,5) plot(g) 10.2.4 Layout Un layout es un conjunto de coordenadas x,y preestablecidas. Se pueden especificar manualmente o usando layout_functions Determina la posición de los nodos en la red. Hay layouts ya diseñados o puedes diseñarlo desde 0. Intentar minimizar cruces de arcos. Algoritmos que lo consiguen: por ejemplo - Kamada Kawai algorithm, the Fruchterman Reingold algorithm, etc. Lykamada &lt;- layout.kamada.kawai(g) plot(g, layout=Lykamada) Lyfruchtermant &lt;- layout.fruchterman.reingold(g) plot(g, layout=Lyfruchtermant) lo &lt;- layout_in_circle(g) head(lo, n=4) [,1] [,2] [1,] 1.000000 0.0000000 [2,] 0.809017 0.5877853 [3,] 0.309017 0.9510565 [4,] -0.309017 0.9510565 # lo es una matriz de coordenadas lo [,1] [,2] [1,] 1.000000 0.000000e+00 [2,] 0.809017 5.877853e-01 [3,] 0.309017 9.510565e-01 [4,] -0.309017 9.510565e-01 [5,] -0.809017 5.877853e-01 [6,] -1.000000 1.224647e-16 [7,] -0.809017 -5.877853e-01 [8,] -0.309017 -9.510565e-01 [9,] 0.309017 -9.510565e-01 [10,] 0.809017 -5.877853e-01 plot(g, layout=lo) # See ?layout_ for a full list # Para redes tipo árbol: layout_as_tree gTree &lt;- make_tree(15) plot(gTree, layout=layout_as_tree(gTree, root = 1)) # layout como un grid plot(g, layout=layout_on_grid(g)) Mallas: 10.2.5 Dibujar grafos ponderados 10.2.6 Cambiar aspecto y propiedades de un grafo V(g)$shape V(g)$size V(g)$color vertex.shape vertex.color vertex.size set_edge_attr set_vertex_attr set_graph_attr Note: colores en R http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf 10.2.7 Ejemplos - plantillas Plot vocales como rectángulos g &lt;- make_ring(10, directed=TRUE, mutual=TRUE) V(g)$name &lt;- LETTERS[1:10] g &lt;- g + edges(9,5, 7,1, 1,5) plot(g) vowel &lt;- V(g)$name %in% c(&quot;A&quot;,&quot;E&quot;,&quot;I&quot;,&quot;O&quot;,&quot;U&quot;) + 1 # gives 1 or 2 plot(g, layout=lo, vertex.shape=c(&quot;circle&quot;, &quot;square&quot;)[vowel]) #colores plot(g, layout=lo, vertex.color=c(&quot;tomato2&quot;, &quot;royalblue&quot;)[vowel]) #tamaño plot(g, layout=lo, vertex.size=c(15,30)[vowel]) #Propiedades usando atributos V(g)$shape &lt;- &quot;circle&quot; # Aplicado a todos los vértices V(g)$size &lt;- 15 V(g)$color &lt;- &quot;orange&quot; isVowel &lt;- V(g)$name %in% c(&quot;A&quot;,&quot;E&quot;,&quot;I&quot;,&quot;O&quot;,&quot;U&quot;) # Sobreescribir los nodos vocales V(g)[isVowel]$shape &lt;- &quot;square&quot; V(g)[isVowel]$color &lt;- &quot;royalblue&quot; V(g)[isVowel]$size &lt;- 25 plot(g, layout=lo) Propiedades de los arcos E(g)$width &lt;- 1 v1 &lt;-V(g)[isVowel] v1 3/10 vertices, named, from 5b82f2b: [1] A E I E(g)[v1 %--% v1]$width &lt;- 4 # Ver http://igraph.org/r/doc/igraph-es-indexing.html plot(g, layout=lo) #Arcos curvados plot(g, layout=lo, edge.curved=0.3*which_mutual(g)) Agrupaciones por índices: groupList &lt;- list(vowelGroup = which(isVowel), constGroup1 = c(2,3,4), constGroup2 = c(6,7,8)) groupColours &lt;- c(rgb(0,0.3,1,0.5), rgb(0.8,0.4,0.1,0.5), rgb(0.8,0.4,0.1,0.5)) plot(g, layout=layout_with_fr, # Fruchterman?Reingold layout mark.groups=groupList, # Mark the groups mark.col= groupColours, # Eliminar el borde mark.border = NA, edge.curved=0.1*which_mutual(g)) text(0.45,0.1,&quot;Vocales&quot;, cex=1.5) text(0.5,0.9,&quot;Grupo consonantes 1&quot;, cex=1.5) text(-0.8,-1,&quot;Grupo consonantes 2&quot;, cex=1.5) 10.3 Exportar grafos igraph permite importar y exportar de/desde un considerable número de formatos. Se usan los comandos read_graph y write_graph. Un formato abierto (open) es graphml. write_graph(g, &quot;gr1.graphml&quot;, format=&quot;graphml&quot;) Otros formatos: edgelist: Fichero de texto con arcos en cada línea. pajek: Pajek es un programa popular en Windows para análisis de redes. gml: Graph Modelling Language es uno de los formatos abiertos más populares. graphml: Graph Markup Language es un formato abierto basado en XML. dot: Formato usado por GraphViz. ** Gephi: Para exportar al formato nativo GEXF de Gephise usa el paquete rgexf al que puede convertirse desde un objeto igraph ** Referencias: https://www.r-project.org/nosvn/conferences/useR-2010/slides/Zhang.pdf https://programminghistorian.org/en/lessons/temporal-network-analysis-with-r 10.4 De Twitter El primer paso será extraer los términos usando las técnicas de text mining y crear una matriz de términos (DTM - Document Term Matrix:). Los documentos serían los tweets y los términos serían las palabras o grupos de palabras destacadas en los datos extraidos. Objetivo: Construir una red de términos (personas) basada en sus co-ocurrencias en los mismos tweets (pertenencia a los mismos grupos). 10.5 Medidas de bondad, calidad Analysis of the Networks to extract knowledge. &gt;&gt; Goal of a SNA proyect \\[[http://snap.stanford.edu/class/cs224w-2015/slides/06-applicationsI.pdf\\](http://snap.stanford.edu/class/cs224w-2015/slides/06-applicationsI.pdf)](http://snap.stanford.edu/class/cs224w-2015/slides/06-applicationsI.pdf](http://snap.stanford.edu/class/cs224w-2015/slides/06-applicationsI.pdf)) * Locate people in the network for… higher compensation positive performance evaluations more promotions more good ideas &gt; Ego network is a special type of network consisting of one central node and all other nodes directly connected to it. The central node is known as ego, while the other surrounding nodes directly connected to it are known as alters. * \\[[https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f\\](https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f)](https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f](https://medium.com/applied-data-science/the-google-vs-trick-618c8fd5359f)) * \\[[http://olizardo.bol.ucla.edu/classes/soc-111/lessons-winter-2022/5-lesson-egonet-metrics.html\\](http://olizardo.bol.ucla.edu/classes/soc-111/lessons-winter-2022/5-lesson-egonet-metrics.html)](http://olizardo.bol.ucla.edu/classes/soc-111/lessons-winter-2022/5-lesson-egonet-metrics.html](http://olizardo.bol.ucla.edu/classes/soc-111/lessons-winter-2022/5-lesson-egonet-metrics.html)) Vamos a usar este grafo como ejemplo de las medidas de bondad del grafo. ```{r} library(igraph) g1 &lt;- graph( c(1,2, 1,3, 2,3, 3,4, 3,5, 1,5, 4,2, 3,6, 4,8, 8,1, 9,1, 10,2, 7,6, 5,10)) g1 summary(g1) plot(g1) ``` **Vértices, arcos**. ```{r} class(g1) V(g1) V(g1)\\[1\\] E(g1) E(g1)\\[1\\] class(V(g1)) class(E(g1)) ``` 10.5.1 Centrality Importancia de los nodos en un grafo. - Número de arcos de entrada-salida de los nodos. - Redes con alta centralidad tienen pocos nodos con muchas conexiones. - Redes con baja centralidad tienen muchos nodos con similar o menos conexiones. - Ver https://en.wikipedia.org/wiki/Centrality#PageRank_centrality 10.5.2 Degree Número de arcos conectados a un vértice. Señala la importancia de un vértice o el nivel de actividad del vértice en la red. - Cómo de central es un nodo en la red - Cuántos arcos de entrada-salida tiene o con cuántos nodos se conecta directamente via un arco. &gt; &gt; `centr_degree`, `igraph::degree` ```{r} g1 plot(g1) igraph::degree(g1) igraph::degree(g1, mode=“in”) igraph::degree(g1, mode=“out”) deg &lt;- centr_degree(g1) deg ``` 10.5.3 Betweenness Mide el grado en el que la información fluye a través de un vértice particular y su importancia relativa como un intermediario en la red. Describe nodos que son conexiones clave o puentes entre grupos de nodos. - El número de caminos más cortos que pasan por un nodo dado (medida relativa) - la suma de las longitudes de los caminos más cortos entre otros nodos pasando por el nodo, dividida por las longitudes de camino más cortas (no necesariamente a través del nodo) entre los otros nodos. &gt; &gt; `igraph::betweenness` ```{r} igraph::betweenness(g1) ``` - Por el vértice 6 no pasa ningún *camino más corto* entre dos vértices. - Por el vértice 3 pasan 25 *caminos más cortos* entre dos vértices. - … 10.5.4 Edge_betweenness Similar al anterior pero teniendo en cuenta cada arco. &gt; &gt; `igraph::edge_betweenness` ```{r} g1 &lt;- set.edge.attribute(g1, “weight”, value= 1) bg &lt;- edge_betweenness(g1) plot(g1, edge.label = bg) ``` 10.5.5 Closeness Distancia a otros nodos. Un nodo con valor alto de este estimador es más central y puede difundir la información a muchos otros nodos. - Se obtiene como 1 divido por la suma de las distancias geodésicas desde un vértice al resto. Alcanzará su valor máximo cuando un vértice esté conectado a todos los demás. Longitud media de los caminos más cortos (geodésicos). - Mide cuantos pasos se requieren desde un vértice para alcanzar el resto de vértices de la red. - Caminos cortos entre vértices señalan que estos están cercanos unos a otros. &gt; &gt; `centr_clo`, `igraph::closeness` ```{r} igraph::closeness(g1) ``` 10.5.6 Eigenvector No todas las conexiones tienen la misma importancia - medida de la importancia de un nodo. - La medida *Eigenvector Centrality* se calcula como el autovalor de mayor módulo de la matriz de adyacencia que contiene los pesos. - *a high score to vertices that either have a lot of connections, or are connected to someone with a lot of connections* &gt; &gt; Eigenvector Centrality: `eigen_centrality` ```{r} eigen_centrality(g1) ``` 10.5.7 Pagerank Algoritmo de Google para realizar un ranking con la importancia de los resultados de la búsqueda. Nodos son más importantes si tienen muchos enlaces de entrada. &gt; &gt; `page.rank` ```{r} page.rank(g1) ``` # Paths Caminos de un vértice a otro o de grupos de vértices a otros. 10.5.8 Diameter El máximo camino más corto entre cualquier par de nodos. En grafos muy grandes indica la posibilidad de que la información se difunda más o menos fácilmente. El algoritmo tiene costo $O(n^3)$. En Twitter hay cientos de millones de usuarios,… &gt; &gt; `get_diameter` &gt; &gt; `diameter`- el camino más largo entre dos nodos. ```{r} plot(g1) diameter(g1) ``` 10.5.9 Caminos y distancias **Distancia geodésica**: El menor número de arcos a atravesar para conectar dos nodos. ```{r} sp &lt;- shortest_paths(g1, from=“1”, to=“10”) sp$vpath sp1 &lt;- shortest_paths(g1, from=“1”, to=“9”) sp1$vpath distances(g1) ``` Relacionado con distancias: &gt; &gt; `distance_table, mean_distance` # Clustering *Whether your friends are likely to be friends*. **Grupos**: Subconjunto de vértices que comparten características en común. - Una primera forma es buscar los triángulos en el grafo. \\*La medida de clustering (transitividad) es la frecuencia relativa de triángulos cerrados. $$C=\\frac{3*\\mbox{ número de triángulos }}{\\mbox{número de triples conectados}}$$ 10.5.10 Transitivity *friends of friends to be friends and enemies of enemies to be enemies* Probabilidad de que vértices adyacentes de un vértice estén conectados - se denomina también coeficiente de agrupación o *clustering*. &gt; &gt; `transitivity`, `shortest_paths` 10.5.10.1 Global clustering: ```{r} transitivity(g1, type = “global”) ``` 10.5.10.2 Local clustering Fracción de triples conectados a través de cada vértice que son cerrados. ```{r} transitivity(g1, type = “local”) ``` # Otras medidas y definiciones - Densidad: Número de conexiones respecto al total de conexiones posibles. Un grafo completo tiene una densidad igual a 1 - `edge_density`. - Popularidad: nodos que son centrales tienden a ser más populares. - Cliques: todos con todos - `clique_num(g, min=k)` encuenta cliques con un mínimo de k vértices. - Componentes: Una componente es el conjunto de vértices de la que tienen conexiones entre ellos. Una red puede tener varias componentes - `components` - Nodos a distancia k - `random_walk`. - Hub, Authorities - `hub_score, authority.score` un nodo se denomina hub tiene muchos enlaces de salida y se denomina authorities si tiene muchos de entrada. - detección de comunidades: `cluster_edge_betweenness` 10.6 Proyecto SNA - USairports Nota: Buscar información en https://igraph.org/r/ Cargamos la librería igraphdata y el dataset USairports. Cambia el nombre al grafo (nuevo nombre US) usando:graph_attr. Visualizar el grafo. Usar función V para acceder a los vértices del grafo. Visualizar los cinco primeros vértices. Usar función E para acceder a los arcos del grafo. Visualizar los cinco primeros arcos. Mira las propiedades asociadas a los vértices vertex_attr_names, vertex_attr. Usando estas propiedades extrae los nombres de los 5 primeros aeropuertos y de sus ciudades. 6.Examina las propiedades asociadas a los arcos edge_attr_names, `edge_attr```. Usando estas propiedades extrae los nombres de los 5 primeros compañías aéreas (carrier), de cuantas salidas tiene cada una de estas compañías. Mira las conexiones entre los cinco primeros aeropuertos. Usa vcount para saber el tamaño del grafo. Usa V() y las funciones de manejo de listas para añadirle un nuevo atributo a los vértices: Group que va a tener como valor para cada aeropuerto los valores “A” o “B” aleatoriamente. Analizar qué hacen las siguientes órdenes: Analizar qué hacen las siguientes órdenes: Eliminar del grafo el aeropuerto último de la lista de aeropuertos. Eliminar la conexión entre BJC y MIA. Encontrar las conexiones directas desde BOS. Encontrar las conexiones desde BOS. Caminos y distancias entre dos aeropuertos. Explica cómo calcularlas. ¿Qué aeropuertos están a más de 1000 km? ¿Cuantas conexiones de entrada y salida tiene el primer aeropuerto? ¿Cuales son las ciudades vecinas de Bangor (conectadas por vuelos directamente) Usa neighbors. Analizar qué hacen las siguientes órdenes: Pasar el grafo a un dataframe. Pasar el grafo a una matriz y encontrar los aeropuertos conectados por tres escalas. Guardar en un grafo y visualizarlo. Aplicar a este grafo, las medidas de calidad de grafos: tamaño, diámetro, clustering, transitividad, etc. Buscar información en https://igraph.org/r/ Eliminar ciclos en el grafo. Buscar información de simplify. 10.7 Proyecto - películas + actores En el ejercicio verás funciones aplicadas al grado y sus resultados como resumen, repaso y explicación de nuevas funcionalidades. Aparecen resultados sin el código correspondiente. Esos son los apartados a resolver (los resultados que aquí aparecen son orientativos - no tiene porqué salir exáctamente lo mismo). 10.7.1 crear grafo Crea un grafo dirigido usando igraph con los dos data.frames leidos anteriormente Visualiza el grafo IGRAPH c0e24c7 UN– 8 16 – + attr: name (v/c), Gender (v/c), BestActorActress (v/c), Movie (e/c) + edges from c0e24c7 (vertex names): [1] Tom Hanks –Gary Sinise Tom Hanks –Robin Wright [3] Gary Sinise –Robin Wright Tom Hanks –Gary Sinise [5] Tom Hanks –Bill Paxton Tom Hanks –Kevin Bacon [7] Tom Hanks –Ed Harris Gary Sinise –Bill Paxton [9] Gary Sinise –Kevin Bacon Gary Sinise –Ed Harris [11] Bill Paxton –Kevin Bacon Bill Paxton –Ed Harris [13] Kevin Bacon –Ed Harris Ed Harris –Sean Connery [15] Ed Harris –Nicolas Cage Sean Connery–Nicolas Cage 10.7.2 Análisis del grafo ¿Nodos en el grafo? [1] 8 [1] 8 ¿Arcos en el grafo? [1] 16 Nodo 3 del grafo 1/8 vertex, named, from c0e24c7: [1] Robin Wright Arco 1 del grafo 1/16 edge from c0e24c7 (vertex names): [1] Tom Hanks–Gary Sinise Cambia algunas propiedades del grafo: Cambia el layout del grafo (he usado el de estrella pero puedes usar el que quieras. Agrupar los nodos de protagonistas que han ganado un oscar 10.7.3 Cambiar atributos del grafo Para acceder a los atributos del grafo usamos: IGRAPH c0e24c7 UN– 8 16 – + attr: name (v/c), Gender (v/c), BestActorActress (v/c), Movie (e/c) + edges from c0e24c7 (vertex names): [1] Tom Hanks –Gary Sinise Tom Hanks –Robin Wright [3] Gary Sinise –Robin Wright Tom Hanks –Gary Sinise [5] Tom Hanks –Bill Paxton Tom Hanks –Kevin Bacon [7] Tom Hanks –Ed Harris Gary Sinise –Bill Paxton [9] Gary Sinise –Kevin Bacon Gary Sinise –Ed Harris [11] Bill Paxton –Kevin Bacon Bill Paxton –Ed Harris [13] Kevin Bacon –Ed Harris Ed Harris –Sean Connery [15] Ed Harris –Nicolas Cage Sean Connery–Nicolas Cage $name [1] “Tom Hanks” “Gary Sinise” “Robin Wright” “Bill Paxton” “Kevin Bacon” [6] “Ed Harris” “Sean Connery” “Nicolas Cage” $Gender [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” $BestActorActress [1] “Winner” “None” “None” “None” “None” “Nominated” [7] “None” “Winner” $Movie [1] “Forest Gump” “Forest Gump” “Forest Gump” “Apollo 13” “Apollo 13” [6] “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” [11] “Apollo 13” “Apollo 13” “Apollo 13” “The Rock” “The Rock” [16] “The Rock” 8/8 vertices, named, from c0e24c7: [1] Tom Hanks Gary Sinise Robin Wright Bill Paxton Kevin Bacon [6] Ed Harris Sean Connery Nicolas Cage [1] “Tom Hanks” “Gary Sinise” “Robin Wright” “Bill Paxton” “Kevin Bacon” [6] “Ed Harris” “Sean Connery” “Nicolas Cage” [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” [1] “Winner” “None” “None” “None” “None” “Nominated” [7] “None” “Winner” 1/16 edge from c0e24c7 (vertex names): [1] Tom Hanks–Gary Sinise [1] “Forest Gump” “Forest Gump” “Forest Gump” “Apollo 13” “Apollo 13” [6] “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” [11] “Apollo 13” “Apollo 13” “Apollo 13” “The Rock” “The Rock” [16] “The Rock” $name [1] “Tom Hanks” “Gary Sinise” “Robin Wright” “Bill Paxton” “Kevin Bacon” [6] “Ed Harris” “Sean Connery” “Nicolas Cage” $Gender [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” $BestActorActress [1] “Winner” “None” “None” “None” “None” “Nominated” [7] “None” “Winner” $color [1] “green” “green” “green” “green” “green” “green” “green” “green” 8 x 8 sparse Matrix of class “dgCMatrix” Tom Hanks Gary Sinise Robin Wright Bill Paxton Kevin Bacon Tom Hanks . 2 1 1 1 Gary Sinise 2 . 1 1 1 Robin Wright 1 1 . . . Bill Paxton 1 1 . . 1 Kevin Bacon 1 1 . 1 . Ed Harris 1 1 . 1 1 Sean Connery . . . . . Nicolas Cage . . . . . Ed Harris Sean Connery Nicolas Cage Tom Hanks 1 . . Gary Sinise 1 . . Robin Wright . . . Bill Paxton 1 . . Kevin Bacon 1 . . Ed Harris . 1 1 Sean Connery 1 . 1 Nicolas Cage 1 1 . Recordar usar funciones vectoriales: $name [1] “Tom Hanks” “Gary Sinise” “Robin Wright” “Bill Paxton” “Kevin Bacon” [6] “Ed Harris” “Sean Connery” “Nicolas Cage” $Gender [1] “Male” “Male” “Female” “Male” “Male” “Male” “Male” “Male” $BestActorActress [1] “Winner” “None” “None” “None” “None” “Nominated” [7] “None” “Winner” $color [1] “green” “green” “green” “green” “green” “green” “green” “green” $Movie [1] “Forest Gump” “Forest Gump” “Forest Gump” “Apollo 13” “Apollo 13” [6] “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” “Apollo 13” [11] “Apollo 13” “Apollo 13” “Apollo 13” “The Rock” “The Rock” [16] “The Rock” $color [1] “red” “red” “red” “blue” “blue” “blue” “blue” “blue” “blue” “blue” [11] “blue” “blue” “blue” “red” “red” “red” Más información de los grafos, vértices y arcos: color, size, shape, etc. https://igraph.org/r/doc/ [[https://igraph.org/r/doc/plot.common.html](https://igraph.org/r/doc/plot.common.html](https://igraph.org/r/doc/plot.common.html Calcula los índices de los vértices del grafo correspondientes a los protagonistas que han ganado un oscar (no mirando en el dataset). Usando estos índices, destaca en amarillo los actores que han ganado un oscar. 2/8 vertices, named, from c0e24c7: [1] Tom Hanks Nicolas Cage Destaca con un arco rojo qué actores han trabajado juntos en Apollo13 o en The Rock. 13/16 edges from c0e24c7 (vertex names): [1] Tom Hanks –Gary Sinise Tom Hanks –Bill Paxton [3] Tom Hanks –Kevin Bacon Tom Hanks –Ed Harris [5] Gary Sinise –Bill Paxton Gary Sinise –Kevin Bacon [7] Gary Sinise –Ed Harris Bill Paxton –Kevin Bacon [9] Bill Paxton –Ed Harris Kevin Bacon –Ed Harris [11] Ed Harris –Sean Connery Ed Harris –Nicolas Cage [13] Sean Connery–Nicolas Cage Para los ganadores de oscar, hacemos que el tamaño del nodo se corresponda con la importancia de ese actor en el dataset y además que la etiqueta sea más grande : Definimos la importancia como el número de películas multiplicado por 15. Ayuda: ver funciones strength(grafo) Eliminar el nombre (la etiqueta - atributo label) de los nodos con protagonistas con importancia menor que 4. 10.7.4 Modificar el grafo Para practicar todo lo visto: - Modifica los atributos del grafo para que se muestre parecido al siguiente (función legend para las leyendas): 13/16 edges from c0e24c7 (vertex names): [1] Tom Hanks –Gary Sinise Tom Hanks –Bill Paxton [3] Tom Hanks –Kevin Bacon Tom Hanks –Ed Harris [5] Gary Sinise –Bill Paxton Gary Sinise –Kevin Bacon [7] Gary Sinise –Ed Harris Bill Paxton –Kevin Bacon [9] Bill Paxton –Ed Harris Kevin Bacon –Ed Harris [11] Ed Harris –Sean Connery Ed Harris –Nicolas Cage [13] Sean Connery–Nicolas Cage "],["pagerank-1.html", "Capítulo 11 pagerank", " Capítulo 11 pagerank "],["análisis-de-componentes-principales.html", "Capítulo 12 Análisis de Componentes Principales", " Capítulo 12 Análisis de Componentes Principales "],["text-imining.html", "Capítulo 13 Text Imining", " Capítulo 13 Text Imining "],["references-1.html", "References", " References "]]
