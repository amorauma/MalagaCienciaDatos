#  Series Temporales

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(knitr)
library(ggplot2)
library(latex2exp)
knitr::opts_chunk$set(fig.align = "center",
                      fig.width = 5,
                      fig.height = 4,
                      collapse = TRUE)
```

## COVID predictions 
 
 
- [P A G T A G N A: Philippine COVID-19 Case Forecasting Web Application](https://jamalrogersapp.shinyapps.io/tsforecast/) 

- [Andre Calero](https://andrecalerovaldez.shinyapps.io/CovidTimeSeriesTest/)

- [covid19forecast](http://covid19forecast.science.unimelb.edu.au/?lang=es)


 
 
Note: Install  **TTR**, **forecast** packages

```{r}
library(TTR)
library(forecast)
```

## Introduction
Forecasting is one of the principal applications in data science. 

See [Forecasting: Principles and Practice](https://otexts.org/fpp2/)

> **Tell us what the future holds, so we may know that you are gods.
(Isaiah 41:23)**

**Forecasting is the set of techniques modelling how to predict the future as accurately as possible, given all of the information available, including historical data and  any future data. These methods try to  extrapolate trend and seasonal patterns**

__Applications of Forecasting__

- Medicine, epidemiology,
- planning for the economy features of a country, financial institutions, pocily organizations, 
- weather, global temperature changes
- scheduling in bussiness,
- marketing, 
- predicting sales in a ship, 
- finance and risk management,
- prediction of population in a country, in a region,
- seismic recordings
- planning th stock of product in a online shop, 
- deciding whether to build another power generation plant,
- scheduling personal  in a call centre depending of the number of calls,  
- deciding capital investments,

See [The statistical forecasting perspective](https://otexts.org/fpp2/perspective.html)

## Time series analysis

The study of adjacent points using conventional statistical methods have problems. 

- Conventional statistical methods are dependent on the   assumption that these adjacent
observations are independent and identically distributed. 
- The temporal data appears temporal correlations and ad-hoc techniques have been developed to treat these data.


__Basic steps__

- Problem study
- Data collection
- Data analysis
- Model selection and fitting
- Model validation
- Forecasting model deployment
- Monitoring forecasting model performance

## Time Series in `R`

### Importing time series
 
To analyse your time series data we need to read it into R, and to plot the
time series. 

You can read data into R using   **scan()**, which assumes that your data for successive time
points is in a simple text file with one column.
 
<!-- The file *kings.dat* contains data on the age of death of successive -->
<!-- kings of England, starting with William the Conqueror (original source: Hipel and Mcleod, 1994).  -->

The file *ts_k1.dat* contains data about average business sales of a product.  
 
```{r, echo=FALSE,eval=FALSE}
# Read from an URL
# Skip three lines - comments
kings <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
head(kings)
``` 
 
```{r}
# Read from an URL
# Skip 1 line - comment
data.ts.k1 <- scan("data/ts_k1.dat", skip = 1)
head(data.ts.k1)
```
 
 
The next step is to store the data in a time series object in R.

**A time series is a list of numbers  with  temporal information  stored as a ts object in R**

There is a lot of packages and functions to deal with time series. Firstly, we will use **ts** function, which is used to create time-series objects. 

The sintax of **ts** function is: 

```{r, eval=FALSE}
serie_1<-  ts(data, start, end, frequency)

# data - the  values in the time series

# start -  the first forecast observations in a time series

# end -  the last observation  in a time series

# frequency -  periods  (month, quarter, annual)

#     frequency=1 (by default) -  no periods or season in your data
#     frequency=12 - period is monthly
#     frequency=4  - period is quarterly
#Example

# data <- c(2, 8, 6, 10,
#           5, 14, 7, 14,
#           9, 23, 11, 29, 12)
# serie1 <- ts(data,start = c(2023,1),frequency = 12)

# plot(serie1)
# autoplot(serie1) - ggplot of the ts object
```


See [ts-objects](https://otexts.org/fpp2/ts-objects.html).


 
```{r}
ts.k1 <- ts(data.ts.k1)
ts.k1
```
 
If  the data set  has values for periods less than
one year, for example, monthly or quarterly, one can specify the number of times that data was
collected per year by using the ‘frequency’ parameter in the **ts** function. 

<!-- For example, data set of the number of births per month in New York city, from January 1946 to December -->
<!-- 1959 (originally collected by Newton). -->

Dataset of unemployment in a city:

```{r, echo=FALSE,eval=FALSE}
births <- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")
birthstimeseries <- ts(births, frequency=12, start=c(1946,1))
birthstimeseries

# ahora la descripción aparece: unemployment rates in a city
```

```{r}
tsn1 <- scan("data/ts_n1.dat", skip = 1)
ts.n1 <- ts(tsn1, frequency = 12, start = c(1980, 1))
ts.n1
```

<!-- For instance, *fancy.dat* contains monthly sales for a souvenir shop at a -->
<!-- beach resort town in Queensland, Australia, for January 1987-December 1993 (original data from Wheelwright -->
<!-- and Hyndman, 1998).   -->

**ts_f1.dat** dataset  contains monthly unemployment rates in a city.


```{r,echo=FALSE,eval=FALSE}
souvenir <- scan("http://robjhyndman.com/tsdldata/data/fancy.dat")
souvenirtimeseries <- ts(souvenir, frequency=12, start=c(1987,1))
souvenirtimeseries

#ahora es fancy.dat (souvenirs) --> ts_f1.dat  
```

```{r}
tsf1 <- scan("data/ts_f1.dat")
ts.f1 <- ts(tsf1, frequency = 12, start = c(1987, 1))
ts.f1
```




### Displaying time series

Let us make a basic plot of the time series data (for the two first datasets):

```{r}
plot.ts(ts.k1)
plot.ts(ts.n1)
```


In the second one, there seems to be seasonal variation in the number of unemployees per month: There is a peak every summer, and a trough every winter.
 
The plot for the third dataset is the following: 
```{r}
plot.ts(ts.f1)
```

 
 
The size of
the seasonal fluctuations and random fluctuations seem to increase with the level of the time series. In these cases it is reasonable to transform the time series by calculating the natural logarithm of the original data.

```{r}
logts.f1 <- log(ts.f1)
plot.ts(logts.f1)
```
 
See  [time plots](https://otexts.org/fpp2/time-plots.html).
 
## Time-dependent regression 
 
With one of the previously imported series (tsf1), we are fitting the model: 

$$y=a+bx+cx^2$$

```{r}
library(ggplot2)
library(forecast)
str(ts.f1)
mydata <- data.frame(x=1:length(ts.f1), y=tsf1)
autoplot(ts.f1)
model <- lm( y ~ x , data=mydata)
summary(model)
prediction_model <- forecast::forecast(model, newdata = data.frame(x=86:100))
plot(prediction_model)
```

 
 
##  Time Series Components

A time series has  usually a trend component and an irregular component and, if it turns out to be a seasonal time series, a seasonal component as well.

See [Time series patterns: Trend, Seasonal, Cyclic](https://otexts.org/fpp2/tspatterns.html).


###  Seasonal plot


```{r}
# https://www.rdocumentation.org/packages/forecast/versions/8.4/topics/ggseasonplot
library("forecast")
data("AirPassengers")
ggseasonplot(AirPassengers, col = rainbow(12), year.labels = TRUE)
ggseasonplot(AirPassengers, year.labels = TRUE, continuous = TRUE)
seasonplot(AirPassengers, col = rainbow(12), year.labels = TRUE)
```

### Subseries plot

```{r}
library(ggplot2)
ggsubseriesplot(AirPassengers) +
  ylab("N. Passengers") +
  ggtitle("Seasonal subseries plot: Airpassengers")
```

The mean is indicated by horizontal lines. 


See [Time series graphics: Scatterplots, lag plots, etc. ](https://otexts.org/fpp2/graphics.html). 

## Other packages
- base R: times series class named ts. too restrictive, functiones associated limited

- zoo, xts packages: to represent time series
  
    * special structure for time series
    
    * many functions interestings

<div class="warning" style='padding:1em; background-color:#CCCCCC; color:#404040'>
<h3>**Exercise**</h3>

- Make an .rmd with a mini-tutorial on the use of the zoo and xts packages, applying them to a time series (dataset with data from Spain) that you search on the web about electricity consumption, product consumption, etc.

- Hand in the homework of the time series topic. 

</div>

## Basic forecasting methods

We denote $\hat{y}_{T+h\mid T}$ the value of the variable $y$  in the time $T+h$ based on $y_1, \ldots, y_T$.

The residual will be computed by: 

$$e_T=y_T-\hat{y}_{T}$$
The forecast error is: 
$$e_t(T)=y_t-\hat{y}_{t}(t-T)$$

### Average method

$$\hat{y}_{T+h\mid T}=\frac{(y_1, \ldots, y_T)}{T}$$

```{r}
data.ts.k1 <- scan("data/ts_k1.dat", skip = 1)
mdata.ts.k1 <- meanf(data.ts.k1, 10)
plot(mdata.ts.k1)

```

### Naïve method

The prediction will  be the  last $Y_T$.

$$\hat{y}_{T+h\mid T}= y_T$$

```{r}
data.ts.k1 <- scan("data/ts_k1.dat", skip = 1)
h <- 10
mdata.ts.k1 <- naive(data.ts.k1, h)
plot(mdata.ts.k1)
```

### Seasonal naïve method

The prediction will  be the  same that the value of the variable in the last year in the same time.

$$\hat{y}_{T+h\mid T}= y_{T+h-m(k+1)}$$
(where $m$ is the season in the year and $k$ is the truncated value of $(h-1)/m$)

```{r}
tsn1 <- scan("data/ts_n1.dat", skip = 1)
ts.n1 <- ts(tsn1, frequency = 12, start = c(1980, 1))
h <- 10
mdata.ts.k1 <- snaive(ts.n1, h)
plot(mdata.ts.k1)
```


### Drift method

It is based on naïve method where the amount of change in time (the drift) is set as the average change  in the historical data:

$$\hat{y}_{_{T+h\mid T}}=y_{_{T}}+\frac{h}{T-1}\displaystyle\sum_{t=2}^T(y_t-t_{t-1})=y_{_{T}}+h\frac{y_{_T}-y_{_{1}}}{T-1}$$

```{r}
h <- 20
mdata.ts.k1 <- rwf(ts.n1, h, drift = TRUE)
plot(mdata.ts.k1)
```

See all the methods in the same picture -  [Forecasts for quarterly beer production](https://otexts.org/fpp2/simple-methods.html#fig:beerf). 

Note: It is necessary the package **fpp2** to execute the code in this page. 

## Example: Temperature forecast in Malaga - Basic forecasting methods

To analyse the evolution of minimum temperatures since 2001 in Malaga. 

First of all we obtain a time-series object with the function __ts__, as our data are divided into months, we set the parameter _frecuency_ to 12 and we indicate that we start in January 2001. After this, we draw the object obtained:
```{r}
library('ggplot2')
library('TTR')
library('forecast')
library('tseries')
```

```{r}
library(readr)
temperaturas <- read_csv("data/temp.csv")
ts.p1 <- ts(temperaturas$tm_min, start = c(2001, 1), frequency = 12)
plot.ts(ts.p1, xlab = "años", ylab = "temperatura mínima", main = "Temperaturas mínimas Málaga")
```

We decompose the data: _Seasonal component_, _Trend component_, _Cycle component_ y _residual_.

```{r}
# obtenemos una serie temporal con frecuencia 12 meses
obj.ts <- ts(na.omit(temperaturas$tm_max), frequency = 12, start = c(2001, 1))

# descompose
decomp <- stl(obj.ts, s.window = "periodic") # uses additive model by default
# decomposing the series and removing the seasonality can be done with seasadj() within the forecast package
deseasonal_cnt <- seasadj(decomp)
plot(decomp)
```

Let's see if it is seasonal, let's look at the data by year:
```{r}
ggseasonplot(ts.p1, col = rainbow(12), year.labels = TRUE)
```

As we can see, all the years are identical, which indicates that the temperatures repeat annual and monthly cycles.

With these graphs we can see that our series is seasonal, at the beginning of the year it is down, during the year it rises and from August onwards it starts to fall. This cycle is repeated every year.

```{r}
ggsubseriesplot(ts.p1) +
  ylab("Temperatura") +
  ggtitle("Seasonal subseries plot: Temperaturas mínimas")
```

With this last graph we see the subseries by months. From it we see how each month moves in the same range. With this we confirm that it is a seasonal series.



### Average method

With this method we see that the prediction is basically an average. 

```{r}
mdata.ts.p1 <- meanf(ts.p1,10)
plot(mdata.ts.p1)
```



### Naïve method

This method uses the last value of $y_t$.

```{r}
mdata.ts.p1 <- naive(ts.p1, 10)
plot(mdata.ts.p1)
```


### Seasonal naïve method

This method uses the last year to predict. As we will see in the graph, it generates what has happened in the last year.

```{r}
mdata.ts.p1 <- snaive(ts.p1, 12)
plot(mdata.ts.p1)
```

### Drift method

This method is based on the previous one and that the amount of change over time is set as the average change in the historical data.

```{r}
mdata.ts.p1 <- rwf(ts.p1, 20, drift = TRUE)
plot(mdata.ts.p1)
```


### Conclusion

My data series as I have mentioned is for minimum temperatures, so these methods are too simple to predict well what will happen with the temperature in the following years. The only model that could be used is the _Seasonal naïve method_ which uses the last year to see what will happen next year. But we still need to use better methods.

### Decomposing Time Series

Decomposing a time series means separating it into its constituent components:   trend, cycles,  irregular, a seasonal component  and a remainder component.


If $S_t$ is the seasonal component, $T_t$ is the trend-cycle component, and $R_t$ is the remainder component, we can consider an **additive decomposition** or a **multiplicative decomposition** as follows:
  

Additive decomposition:
$$y_t=S_t+T_t+R_t$$
Multiplicative decomposition:
$$y_t=S_t\times T_t\times R_t$$




- Addditve decomposition is adequate when the  seasonal part and the trend-cycle does not vary too much. 

- Multiplicative decomposition is adequate when the amplitude of the seasonal part increases or decreases with the average level of the time series.


Multiplicative decomposition can be transformed into an additive one just by applying logarithms:
$$\log y_t= \log S_t + \log T_t +\log R_t$$
 
 
 
### Additive decomposition
We describe how decompose a time series considering additive decomposition.

**Note: **Multiplicative decomposition is similar, except that the subtractions are replaced by divisions.


### Non-Seasonal Data

- A non-seasonal time series consists of a trend component and an irregular component.

- To estimate the trend component of a non-seasonal time series that can be described using an _additive model_, it is
common to use a smoothing method, such as calculating the Simple Moving Average (SMA) of the time series.

The **SMA()** function in the ***TTR***  package is used to smooth time series data using a simple moving average:

$$\hat{y_{_t}}=\frac{1}{m}\displaystyle\sum_{j=-k}^k y_{_{t+j}}$$

To compute the average eliminates randomness in the data. 


To specify the order  of the simple moving average,   the parameter $n$ will be used. For example, to calculate a simple moving
average of order 2, we set $n=2$ in the SMA() function.


```{r}
library("TTR")
# simple moving average of order 2
# applied in the first dataset
data.ts.k1 <- scan("data/ts_k1.dat", skip = 1)
ts.k1 <- ts(data.ts.k1)

# Applying the MA of order 2,5,9
ts.k1.sma2 <- SMA(ts.k1, n = 2)
ts.k1.sma5 <- SMA(ts.k1, n = 5)
ts.k1.sma9 <- SMA(ts.k1, n = 9)

# Draw all the pictures together
par(mfrow = c(2, 2))
plot.ts(ts.k1)
plot.ts(ts.k1.sma2)
plot.ts(ts.k1.sma5)
plot.ts(ts.k1.sma9)
```

There still appears to be quite a lot of random fluctuations in the time series smoothed using a simple moving
average of order 2.


Thus, to estimate the trend component more accurately, we might want to try smoothing the
data with a simple moving average of a higher order. This takes a little bit of trial-and-error, to find the right
amount of smoothing.

With MA of order 5, and order 9 the trend starts to be clearly identified in the plots. 
 
### Seasonal Data

A seasonal time series consists of a trend component, a seasonal component and an irregular component.


We can use the **decompose** function in R which estimates:

- the trend-cycle component using MA $\hat{y_t}$, 
- the detrended serie $y_t-\hat{y_t}$
- the seasonal component $S_t$, computing the average of the detrended values for that season,   
- the random components $R_t=y_t-\hat{y_t}-S_t$.


To compute  the detrended serie in additive decomposition: $y_t-T_t =S_t+R_t$




Given the time series  **ts.n1**, we will decompose the time serie using the following:

<!-- Given the time series  of the number of births per month in New York. -->




```{r}
tsn1 <- scan("data/ts_n1.dat", skip = 1)
ts.n1 <- ts(tsn1, frequency = 12, start = c(1980, 1))
ts.n1.comp <- decompose(ts.n1)
```
The estimated seasonal factors are given for the months January-December, and are the same for each year.



```{r}
plot(ts.n1.comp)
```
<!--  We see that the -->
<!-- estimated trend component shows a small decrease from about 24 in 1947 to about 22 in 1948, followed by a -->
<!-- steady increase from then on to about 27 in 1959. -->

<!--  We see that the -->
<!-- estimated trend component shows a small decrease from about 24 in 1980 to about 22 in 1948, followed by a -->
<!-- steady increase from then on to about 27 in 1959. -->

The different parts of the decomposed series can be managed as usual:

```{r,eval=FALSE}
ts.n1.comp$trend
ts.n1.comp$seasonal
ts.n1.comp$random
```




### Seasonal Adjusting
As we have explained, if you have a seasonal time series that can be described using an _additive model_, you can seasonally adjust the
time series by estimating the seasonal component, and subtracting the estimated seasonal component from the
original time series.


```{r}
ts.n1.comp <- decompose(ts.n1)
ts.n1.comp.aj <- ts.n1 - ts.n1.comp$seasonal

ts.n1.comp.aj
plot(ts.n1.comp.aj)
```

The seasonal variation has been removed from the seasonally adjusted time series. The seasonally
adjusted time series now just contains the trend component and an irregular component.

## Example: Temperature forecast in Malaga - Decomposing Time Series

We have seen that the time series we use is seasonal so we are going to use the method of
_Additive decomposition_.

### Additive decomposition

A seasonal time series consists of a trend component, a seasonal component and an irregular component. To decompose it we are going to use __decompose__.

```{r}
# we decompose the time series and draw it
ts.p1.comp <- decompose(ts.p1)
plot(ts.p1.comp)
```

### Seasonal Adjusting

If we have a seasonal time series that can be described using an additive model, we can seasonally adjust the time series by estimating the seasonal component and subtracting the estimated seasonal component from the original time series.


```{r}
# we subtract the stationary component
ts.p1.comp.aj <- ts.p1 - ts.p1.comp$seasonal
ts.p1.comp.aj
```

```{r}
# we draw the adjusted values
plot(ts.p1.comp.aj)
```

### Forecasts using Exponential Smoothing

Exponential Smoothing considers weigths to the values of the variables:
weighted averages of past observations. The weights decrease
exponentially for distant values.

[Taxonomy](https://otexts.org/fpp2/taxonomy.html)

### Simple Exponential Smoothing

The idea is to consider a weighted average between the value of the
variable and the previous one:

$$\hat{y}_{T+1\mid T}=\alpha y_T + (1-\alpha)\hat{y}_{T\mid T-1}$$

If we have into account more past values of the variable, after some
computations, the forecast will be:

$$\hat{y}_{T+1\mid T}=\alpha y_T + \alpha(1-\alpha){y}_{T-1}+\alpha(1-\alpha)^2{y}_{T-2}+\ldots $$

The parameter $\alpha$ controls the rates at which the weights decrease.
The weights decrease exponentially and this is the reason of the name of
the method.

-   $\alpha$ has values between 0 and 1.

-   $\alpha$ close to 0, more important are the longer observations -
     slow learning (past observations have a large influence on
    forecasts).

-   $\alpha$ close to 1, more important the more recent observation s-
    fast learning (that is, only the most recent values influence the
    forecasts).

-   With $\alpha=0$ or $\alpha=1$ we have the extreme cases.

If you have a time series that can be described using an additive model
with constant level (no clear trend) and no seasonality, you can use
**simple exponential smoothing** to make short-term forecasts.

-   The simple exponential smoothing method provides a way of estimating
    the level at the current time point.

The file **ts_p1.dat** contains total annual snowfall for a city.\
<!-- For instance, the file  **precip1.dat** contains total annual rainfall in inches for -->
<!-- London, from 1813-1912 (original data from Hipel and McLeod, 1994).   -->

```{r}
tsp1 <- scan("data/ts_p1.dat", skip = 1)
ts.p1 <- ts(tsp1, start = c(1900))
plot.ts(ts.p1)
```

```{r,echo=FALSE,eval=FALSE}
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat", skip = 1)
rainseries <- ts(rain, start = c(1813))
plot.ts(rainseries)

# ahora es Average snowfall in a city in northern Europe
```

-   The mean stays constant at about 25 - roughly constant level.
-   The random fluctuations seem to be roughly constant.

Thus, we can make forecasts using simple exponential smoothing.

In R, we can fit a simple exponential smoothing predictive model using
the **HoltWinters** function.

[Main
parameters](https://docs.tibco.com/pub/enterprise-runtime-for-R/4.0.0/doc/html/Language_Reference/stats/HoltWinters.html):

-   alpha: Smoothing is controlled by this parameter; the value of alpha
    lies between 0 and 1 (close to 0 mean that little weight is placed
    on the most recent observations when making forecasts of future
    values).
-   beta=FALSE: the function will do exponential smoothing.
-   gamma=FALSE: a non-seasonal model is fitted.
-   seasonal: **additive by default** or multiplicative seasonal model.

```{r}
ts.p1.forecasts <- HoltWinters(ts.p1, beta = FALSE, gamma = FALSE)
ts.p1.forecasts
```

The output of `HoltWinters()` tells us that the estimated value of the
alpha parameter is about 0.024. This is very close to zero, telling us
that past observations have more influence on forecasts.

```{r}
plot(ts.p1.forecasts)
```

The plot shows the original time series in black, and the forecasts of
these values as a red line. The time series of forecasts is much
smoother than the time series of the original data here.

```{r}
plot(forecast(ts.p1.forecasts))
```

The function `forecast` predicts the value as the previous plot shows.

As a measure of the accuracy of the forecasts, we can calculate the sum
of squared errors for the in-sample forecast errors, that is, the
forecast errors for the time period covered by our original time series.
The sum-of-squared- errors is stored in a named element of the list
variable `ts.p1.forecasts` called "SSE", so we can get its value by
typing:

```{r}
ts.p1.forecasts$SSE
```

Finally, we try with different values of $\alpha$ and the errors are
computed:

```{r}
ts.p2.forecasts <- HoltWinters(ts.p1, alpha = 0.5, beta = FALSE, gamma = FALSE)
ts.p2.forecasts
plot(forecast(ts.p2.forecasts))
ts.p2.forecasts$SSE
ts.p3.forecasts <- HoltWinters(ts.p1, alpha = 0.9, beta = FALSE, gamma = FALSE)
ts.p3.forecasts
plot(forecast(ts.p3.forecasts))
ts.p3.forecasts$SSE
```

Note: The forecasts made by `HoltWinters` are stored in the list called
fitted.

```{r}
ts.p3.forecasts$fitted
```

## Forecasting using forecast package

We can make forecasts for further time points by using the
`forecast.HoltWinters` function in the R `forecast` package.

```{r}
require("forecast")
library("forecast")
```

When using the `forecast.HoltWinters` function, as its first argument
(input), you pass it the predictive model that you have already fitted
using the `HoltWinters` function.

You specify how many further time points you want to make forecasts for
by using the "h" parameter in `forecast.HoltWinters()`.

The output for the next 20 years is:

```{r}
# ts.p1.forecasts2 <- forecast.HoltWinters(ts.p1.forecasts, h=20)

ts.p1.forecasts2 <- forecast:::forecast.HoltWinters(ts.p1.forecasts, h = 20)
ts.p1.forecasts2
```

The forecast for a year, a 80% prediction interval for the forecast, and
a 95% prediction interval for the forecast.

<!-- Columns: -->

<!-- - Point: the yearThe forecasted snowfall for 2008 is about 24.68, -->

<!-- with a 95% prediction interval of (16.24, 33.11).  -->

Plotting the predictions:

```{r}
forecast:::plot.forecast(ts.p1.forecasts2)

```

The forecasts for 2000-2020 are plotted as a blue line, the 80%
prediction interval as a dark blue shaded area, and the 95% prediction
interval as a grey shaded area.

__Forecast errors__

Forecast errors are stored in the element *residuals*.

-   If the predictive model cannot be improved upon, there should be no
    correlations between forecast errors for successive predictions.
-   If there are correlations between forecast errors for successive
    predictions, it is likely that the simple exponential smoothing
    forecasts could be improved upon by another forecasting technique.

To figure out whether this is the case, we can obtain a correlogram of
the in-sample forecast errors. We can calculate a correlogram of the
forecast errors using the `acf` function in R.

`acf` computes (and plots, by default) the estimation of the
autocovariance or AutoCorrelation Function (ACF).

```{r}
acf(ts.p1.forecasts2$residuals,na.action = na.pass, lag.max = 20)
```

The autocorrelation at lag 3 is just touching the significance bounds
(significant evidence for non-zero correlations).

To test whether there is significant evidence for non-zero correlations
at lags, we can carry out a Ljung- Box test. This can be done in R using
the **Box.test** function.

```{r}
Box.test(ts.p1.forecasts2$residuals, lag = 20, type = "Ljung-Box")
```

The p-value is 0.6, so there is little evidence of non-zero
autocorrelations in the in-sample forecast errors at lags 1-20.

<!-- To check whether the forecast errors have -->

<!-- constant variance, we can make a time plot of the in-sample forecast errors: -->

<!-- ```{r} -->

<!-- plot.ts(ts.p1.forecasts2$residuals) -->

<!-- ``` -->

<!-- # others  -->

<!--  We will use the R **stats** package for  manipulating and plotting time series, and for calculating the -->

<!-- autocorrelation function. -->

<!-- - The function **stl()** decomposes a times series into a trend and seasonal components, etc. -->

<!-- - The functions **ar()** (for “autoregressive” models) and associated functions, and **arima0()** ( “autoregressive integrated moving average models”) fit standard types of time domain short memory models.  -->

<!-- - The function **spectrum()** and related functions are designed for frequency domain or “spectral” analysis. -->

<!-- -  The function **acf()** to plot the autocorrelation function -->

<!-- The function **gls()** in the -->

<!-- **nlme** package, which can fit relatively complex models that may have autoregressive, arima and various other -->

<!-- types of dependence structure. -->

## Example: Temperature forecast in Malaga - Exponential Smoothing

Analyse the evolution of minimum temperatures since 2001 in Malaga **with the methods seen in this document. Malaga **with the methods seen in this document**.

We obtain a time-series object with the `ts` function. data are divided into months, the parameter *frequency* is set to 12 and we indicate that we start in January 2001. After this, we draw the object:

```{r}
library('ggplot2')
library('TTR')
library('forecast')
library('tseries')
```

```{r}
library(readr)
temperaturas <- read_csv("data/temp.csv")
ts.p1 <- ts(temperaturas$tm_min, start = c(2001, 1), frequency = 12)
plot.ts(ts.p1, xlab = "años", ylab = "temperatura mínima", main = "Temperaturas mínimas Málaga")
```

__Forecasts using Exponential Smoothing__

*Exponential Smoothing* considers weights on variable values: weighted averages of past observations. The weights decrease exponentially for distant values.

### Simple Exponential Smoothing

-   alpha parameter of Holt-Winters Filter.
-   beta parameter of Holt-Winters Filter. If set to FALSE, the function
    will do exponential smoothing.
-   gamma parameter used for the seasonal component. If set to FALSE, an
    non-seasonal model is fitted.

Looking at these parameters we have to assemble a model with *beta=FALSE* as we want exponential smoothing.

```{r}
# we obtain the model
ts.p1.forecasts <- HoltWinters(ts.p1, beta = FALSE)
ts.p1.forecasts
```

```{r}
# we draw the model
plot(ts.p1.forecasts)
```

The graph shows the original time series in black, and forecasts of these values as a red line. The forecast time series is very close to the original data.

We now draw the forecast using `forecast`.

```{r}
plot(forecast(ts.p1.forecasts))
```

As we can see it fits quite well with what is happening in recent years.

Let's try a higher *alpha* value:

```{r}
ts.p2.forecasts <- HoltWinters(ts.p1, alpha = 0.5, beta = FALSE)
ts.p2.forecasts
```

```{r}
plot(forecast(ts.p2.forecasts))
```

```{r}
ts.p3.forecasts <- HoltWinters(ts.p1, alpha = 0.9, beta = FALSE)
ts.p3.forecasts
```

```{r}
plot(forecast(ts.p3.forecasts))
```

As we can see there is not much difference between the first model and the other 2 created later, so the *alpha* value generated by the first model is the optimal one. We can see that the higher the *alpha* value, the narrower the fixed values, and the *gamma* values increase along with *alpha*.

### Forecasting using forecast package

When we use the *`forecast.HoltWinters`* function, as the first argument, it is passed the predictive model that we have already set using the `HoltWinters` function.

To specify how many additional time points we want to predict, we use the parameter *"h"*.

The prediction for the next 2 years is:

```{r}
# we indicate the parameter h=24 for 2 years, as we have the time series by months
ts.p1.forecasts2 <- forecast:::forecast.HoltWinters(ts.p1.forecasts, h = 24)
ts.p1.forecasts2
```

```{r}
forecast:::plot.forecast(ts.p1.forecasts2)
```

The forecasts for the next 2 years are represented as a blue line, the 80% prediction interval as a dark grey shaded area and the 95% prediction interval as a grey shaded area.

__Forecast errors__

Errors are stored in `residuals`.

- If the model cannot be improved, there should be no correlations between forecast errors for successive predictions.
- If there are correlations between forecast errors for successive predictions, it is likely that the predictions can be improved with another technique.

To find out which is the case, we can obtain a correlation of the errors. We can calculate it using the `acf` function. `acf` calculates the estimate of the autocovariance or autocorrelation function *(ACF)*.

Let's see what happens, we use `acf` with residuals and lag max 24.

```{r}
# na.action, to take or not to take into account the NA values
acf(ts.p1.forecasts2$residuals, na.action = na.pass, lag.max = 24)
```

We performed a Ljung Box text to see if there is significant evidence of non-zero correlations in the **lag**.

```{r}
Box.test(ts.p1.forecasts2$residuals, lag = 24, type = "Ljung-Box")
```

As we can see, the p-value is 0.01, which tells us that there is evidence of autocorrelations other than 0.

Note: The ACF (AutoCorrelation Function) chart is used to identify non-stationary time series.  Stationary: if the ACF is rapidly declining


## ARIMA methods

The difference between exponential smoothing models and ARIMA models is that the former are based on a description of the trend and seasonality of the data, while **ARIMA models study the autocorrelations of the data.** 

As we have already seen, the name ARIMA comes from *AutoRegressive Integrated Moving Average*.

- ***AR***: the study variable is analysed using the above values (*lag*).

- **MA**: the regression error is a linear combination of current and past values of the signal.

- ***I***: replaces the value of the variable by the difference between the values and the previous values (one or more times).

ARIMA models can be obtained following the **Box-Jenkins** method.

### Stationarity

A time series is said to be stationary when it does not depend on the instant of observation (white noise series).

The autocovariance function is defined as
$$Cov(y_t,y_{t+k}) = \gamma(k), \forall k$$

That is, the covariance does not depend on time, only on the parameter $k$ (lag).

- If a series has a trend or stationarity it is not stationary.

- A time series that has a cyclical component is stationary.

## Autoregressive models - **AR(p)**

The current value of the $x_t$ series can be explained as a function of $p$ past values, $x_{t-1},x_{t-2},x_{t-p}$, where $p$ determines the number of past steps needed for the prediction.

$$
y_t=c+\phi_1y_{t-1}+\phi_2y_{t-2}+\cdots+\phi_py_{t-p}+\epsilon_t
$$

- $\epsilon_t$: Gaussian white noise with mean 0 and variance $sigma^2_epsilon$.
- If the mean is non-zero, $c$ takes the value $c = \mu(1-\phi_1-\cdots-\phi_p)$.

This is an autoregressive model of order $p$ that applies to stationary models.

## Moving average models

As an alternative to the autoregressive representation where $x_t$ (on the left-hand side of the equation) are assumed to be linear combinations using moving averages of order $q$, $MA(q)$. We assume white noise $epsilon_t$. Model MA($q$):

$$
y_t=c+\epsilon_t+\theta_1\epsilon_{t-1}+\theta_2\epsilon_{t-2}+\cdots+\theta_q\epsilon_{t-q}
$$ 

As with autoregressive models, changing the parameters affects the time series patterns and changing the error term changes the scale.

### Differentiation

To convert a non-stationary time series into a stationary time series, the differences between consecutive signal values are calculated (trend and seasonality are removed or reduced).

The following equation allows to calculate the differences between consecutive signal values:

$$
y'_t=y_t-y_{t-1}
$$

If the differentiated series is white noise:

$$
y_t-y_{t-1}=\epsilon_t
$$ ($\epsilon_t$ denotes white noise)

A closely related model allows the difference to have a non-zero mean. 
$$
y_t-y_{t-1}=c+\epsilon_t
$$ 
where $c$ is the average of the changes between consecutive observations.

### Second order differencing

The difference of the data may have to be recalculated a second time to obtain a stationary series (the change in changes).

## Seasonal differencing

A seasonal difference is the difference between one observation and the previous observation: 
$$
y'_{_t}=y_{_t}-y_{_{t-m}}
$$
where $m$ is the number of seasons.

```{r, echo = TRUE, eval=TRUE}
lynx <- read.csv(file = "data/annual-number-of-lynx-trapped-ma.csv", header = TRUE, quote = '\"')
lynx.ts <- ts(lynx$Lynx, frequency = 1, start = c(1821))
electricity <- read.csv(file = "data/monthly-electricity-production-i.csv", header = TRUE, quote = '\"')
electricity.ts <- ts(electricity$Million.kilowatt.hours, frequency = 12, start = c(1956, 1))

```

```{r, echo= TRUE, eval=TRUE, fig.width=5, fig.asp=0.62, fig.cap="\\label{fig:cyclic}Although there are cycles, they are not fixed in time."}

plot.ts(lynx.ts, xlab = "Year", ylab = "Lynx Trapped")
```

```{r, echo= TRUE, eval=TRUE, fig.width=5, fig.asp=0.62, fig.cap="\\label{fig:trendy}Clear trend indicates this is not a stationary series."}

plot.ts(electricity.ts, xlab = "Year", ylab = "kW/h")
```

```{r, echo= TRUE, eval=TRUE, fig.width=7, fig.asp=0.5, fig.cap="\\label{fig:acf}Lynx trapped ACF plot (left) and kilowatts per hour in Australia ACF plot (right)."}
par(mfrow = c(1, 2))
acf(lynx.ts)
acf(electricity.ts)
```

### Parameters in ARIMA methods.

Therefore, in an ARIMA model we will use three parameters: (*p*, *d*, *q*).

- **p**: refers to the use of past values of the series. It specifies the number of lags used in the model. For example, AR(2) or, ARIMA(2,0,0) specifies the use of two lags of the series.
- **d**: degree of differentiation. Differencing your current and previous values *d* times.
- **q**: model error as a combination of q previous error terms.

ARIMA works best for long and stable series.

## Non-seasonal ARIMA models

If we combine differencing and autoregression we obtain a non-seasonal ARIMA model: 
$$
y'_t = c + \phi_1y'_{t-1}+\cdots+\phi_py'_{t-p}+\theta_1\epsilon_{t-1}+\cdots+\theta_q\epsilon_{t-q}+\epsilon_t
$$ 
where $y'_t$ is the series to which differencing has been applied. These models are represented as ARIMA($p,d,q$), where $d$ represents the degree of differencing.

## Auto-ARIMA model

In R we have functions that automatically calculate the parameter values of an ARIMA model.

The *`auto.arima()`* function is a modification of the **Hyndman-Khandakar** algorithm.

R's `auto.arima()` function combines tests, and minimisations to obtain an ARIMA model. By default, it can use initial approximations to speed up the model search (`approximation=TRUE/FALSE`).

If this does not allow to find a good model (minimum AICc) a larger set of models can be searched with the argument `stepwise=FALSE`. See [@forecasting] for more on this topic.

To choose a custom model for a non-seasonal series, we will use the `Arima()` function as follows:

1. Draw the dataset and identify outliers.

2. Transform the data to stabilise the variance.

3. If the data is not stationary use differencing until it is.

4.  Analyse ACF/PACF for an ARIMA($p,d,0$) or ARIMA($0,d,q$) model.

5.  Try to pick your model and use AICc to find the best one.

6.  Check that the residuals behave like white noise:

- if this is not satisfied look for another model
- if this is true, we can use the model to predict

## Example: Temperature forecast in Malaga - Modelos ARIMA

Analyse the evolution of the minimum temperatures since 2001 in Malaga **with the methods seen in this document**.

We obtain a time-series object with the `ts` function, as our data are divided into months, we set the parameter *frequency* to 12 and we indicate that we start in January 2001. Then we draw the object obtained:

```{r}
library('ggplot2')
library('TTR')
library('forecast')
library('tseries')
```

```{r}
library(readr)
temperaturas <- read_csv("data/temp.csv")
ts.p1 <- ts(temperaturas$tm_min, start = c(2001, 1), frequency = 12)
plot.ts(ts.p1, xlab = "años", ylab = "temperatura mínima", main = "Temperaturas mínimas Málaga")
```

___ARIMA models___

We decompose the data: *Seasonal component*, *Trend component*, *Cycle component* y *residual*.

```{r}
# we obtain a time series with a frequency of 12 months
obj.ts <- ts(na.omit(temperaturas$tm_min), frequency = 12, start = c(2001, 1))

# descompose
decomp <- stl(obj.ts, s.window = "periodic") # uses additive model by default
# decomposing the series and removing the seasonality can be done with seasadj() within the forecast package
deseasonal_cnt <- seasadj(decomp)
plot(decomp)
```

Previously we have seen that the series is stationary, but to fit an ARIMA model requires the series to be stationary.

```{r}
# with ADF test we can know if it is stationary or not.
adf.test(obj.ts, alternative = "stationary")
```

We see that it is stationary.

We fix an ARIMA model using auto.arima:

```{r}
fit <- auto.arima(deseasonal_cnt, seasonal = FALSE)
fit
```

```{r}
tsdisplay(residuals(fit), main = "(1,1,1) Model Residuals")
```

As we can see, even lag=23 is equal, so p or q is 23.

```{r}
fit2 <- arima(deseasonal_cnt, order = c(1, 1, 23))

fit2

tsdisplay(residuals(fit2), main = "Seasonal Model Residuals")
```

We forecast for the next 30 months:

```{r}
fcast <- forecast(fit2, h = 30)
plot(fcast)
```




## References
Information and images obtained from:

[Forecasting: Principles and Practice. Rob J Hyndman and George Athanasopoulos. Monash University, Australia](https://otexts.org/fpp2/)

[Introducion to Forecasting with ARIMA in R](https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials)

[AEMET](Los datos han sido obtenidos de [aemet](http://www.aemet.es/es/portada))
 

 
 



